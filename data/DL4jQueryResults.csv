Id,Body,Tags,CreationDate,DeletionDate,Score,OwnerUserId,LastActivityDate,AnswerCount,CommentCount,FavoriteCount,ClosedDate,UserId,Reputation,UpVotes,DownVotes
"38274058","<p>I am looking through the example of deeplearning 4j for classifying movie reviews according to their sentiment. 
<a href=""https://github.com/deeplearning4j/dl4j-0.4-examples/blob/master/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/SentimentExampleIterator.java)"" rel=""nofollow"">ReviewExample</a> </p>

<p>At line 124-142 the N-dimensional arrays are created and I am kind of unsure what is happening at these lines:</p>

<p>Line 132: </p>

<pre><code>features.put(new INDArrayIndex[]{NDArrayIndex.point(i),
NDArrayIndex.all(), NDArrayIndex.point(j)}, vector);
</code></pre>

<p>I can image that <code>.point(x)</code> and <code>.point(j)</code> address the cell in the array, but what exactly does the <code>NDArrayIndex.all()</code> call do here? </p>

<p>While building the feature array is more or less ok what is happening there I get totally confused by the label mask and this <code>lastIdx</code> variable</p>

<p>Line 138 - 142</p>

<pre><code>            int idx = (positive[i] ? 0 : 1);
            int lastIdx = Math.min(tokens.size(),maxLength);
            labels.putScalar(new int[]{i,idx,lastIdx-1},1.0);   //Set label: [0,1] for negative, [1,0] for positive
            labelsMask.putScalar(new int[]{i,lastIdx-1},1.0);   //Specify that an output exists at the final time step for this example
</code></pre>

<p>The label array itself is addressed by <code>i, idx</code> e.g. column/row that is set to 1.0 - but I don't really get how this time-step information fits in? Is this conventional that the last parameter has to mark the last entry?</p>

<p>Then why does the labelsMask use only <code>i</code> and not <code>i, idx</code> ? </p>

<p>Thanks for explanations or pointer that help to clarify some of my questions</p>
","<java><deeplearning4j><nd4j>","2016-07-08 19:27:11","","1","1430550","2017-12-12 03:32:40","2","0","","","1430550","815","30","0"
"46879409","<p>I'm in the process of trying to learn the Deeplearning4j library. I'm trying to implement a simple 3-layer neural network using sigmoid activation functions to solve XOR. What configurations or hyper-parameters am I missing? I've managed to get accurate outputs using RELU activations with a softmax output from some of the MLP examples I found online, however with sigmoid activations it doesn't seem to want to fit accurately. Can anyone share why my network isn't producing the correct outputs?</p>

<pre><code>    DenseLayer inputLayer = new DenseLayer.Builder()
            .nIn(2)
            .nOut(3)
            .name(""Input"")
            .weightInit(WeightInit.ZERO)
            .build();

    DenseLayer hiddenLayer = new DenseLayer.Builder()
            .nIn(3)
            .nOut(3)
            .name(""Hidden"")
            .activation(Activation.SIGMOID)
            .weightInit(WeightInit.ZERO)
            .build();

    OutputLayer outputLayer = new OutputLayer.Builder()
            .nIn(3)
            .nOut(1)
            .name(""Output"")
            .activation(Activation.SIGMOID)
            .weightInit(WeightInit.ZERO)
            .lossFunction(LossFunction.MEAN_SQUARED_LOGARITHMIC_ERROR)
            .build();

    NeuralNetConfiguration.Builder nncBuilder = new NeuralNetConfiguration.Builder();
    nncBuilder.iterations(10000);
    nncBuilder.learningRate(0.01);
    nncBuilder.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);

    NeuralNetConfiguration.ListBuilder listBuilder = nncBuilder.list();
    listBuilder.layer(0, inputLayer);
    listBuilder.layer(1, hiddenLayer);
    listBuilder.layer(2, outputLayer);

    listBuilder.backprop(true);

    MultiLayerNetwork myNetwork = new MultiLayerNetwork(listBuilder.build());
    myNetwork.init();

    INDArray trainingInputs = Nd4j.zeros(4, inputLayer.getNIn());
    INDArray trainingOutputs = Nd4j.zeros(4, outputLayer.getNOut());

    // If 0,0 show 0
    trainingInputs.putScalar(new int[]{0,0}, 0);
    trainingInputs.putScalar(new int[]{0,1}, 0);
    trainingOutputs.putScalar(new int[]{0,0}, 0);

    // If 0,1 show 1
    trainingInputs.putScalar(new int[]{1,0}, 0);
    trainingInputs.putScalar(new int[]{1,1}, 1);
    trainingOutputs.putScalar(new int[]{1,0}, 1);

    // If 1,0 show 1
    trainingInputs.putScalar(new int[]{2,0}, 1);
    trainingInputs.putScalar(new int[]{2,1}, 0);
    trainingOutputs.putScalar(new int[]{2,0}, 1);

    // If 1,1 show 0
    trainingInputs.putScalar(new int[]{3,0}, 1);
    trainingInputs.putScalar(new int[]{3,1}, 1);
    trainingOutputs.putScalar(new int[]{3,0}, 0);

    DataSet myData = new DataSet(trainingInputs, trainingOutputs);
    myNetwork.fit(myData);


    INDArray actualInput = Nd4j.zeros(1,2);
    actualInput.putScalar(new int[]{0,0}, 0);
    actualInput.putScalar(new int[]{0,1}, 0);

    INDArray actualOutput = myNetwork.output(actualInput);
    System.out.println(""myNetwork Output "" + actualOutput);
    //Output is producing 1.00. Should be 0.0
</code></pre>
","<java><machine-learning><data-science><deeplearning4j>","2017-10-22 21:39:43","","0","3488161","2017-10-23 00:06:14","1","1","","","3488161","25","0","0"
"47991897","<p>I have two questions on deeplearning4j that are somewhat related.</p>

<ol>
<li>When I execute “<em>INDArray predicted = model.output(features,false);</em>” to generate a prediction, I get the label predicted by the model; it is either 0 or 1. I tried to search for a way to have a probability (value between 0 and 1) instead of strictly 0 or 1. This is useful when you need to set a threshold for what your model should consider as a 0 and what it should consider as a 1. For example, you may want your model to output '1' for any prediction that is higher than or equal to 0.9 and output '0' otherwise.</li>
<li>My second question is that I am not sure why the output is represented as a two-dimensional array (shown after the code below) even though there are only two possibilities, so it would be better to represent it with one value - especially if we want it as a probability (question #1) which is one value.
<br>PS: in case relevant to the question, in the Schema the output column is defined using ""<em>.addColumnInteger</em>"". Below are snippets of the code used.</li>
</ol>

<p><strong>Part of the code:</strong></p>

<pre><code>MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(seed)
            .iterations(1)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .learningRate(learningRate)
            .updater(org.deeplearning4j.nn.conf.Updater.NESTEROVS).momentum(0.9)
            .list()
            .layer(0, new DenseLayer.Builder()
                    .nIn(numInputs)
                    .nOut(numHiddenNodes)
                    .weightInit(WeightInit.XAVIER)
                    .activation(""relu"")
                    .build())
            .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                    .weightInit(WeightInit.XAVIER)
                    .activation(""softmax"")
                    .weightInit(WeightInit.XAVIER)
                    .nIn(numHiddenNodes)
                    .nOut(numOutputs)
                    .build()
            )
    .pretrain(false).backprop(true).build();

    MultiLayerNetwork model = new MultiLayerNetwork(conf);
    model.init();
    model.setListeners(new ScoreIterationListener(10));

    for (int n=0; n&lt;nEpochs; n++) {
        model.fit(trainIter);
    }

    Evaluation eval = new Evaluation(numOutputs);
    while (testIter.hasNext()){
        DataSet t = testIter.next();
        INDArray features = t.getFeatureMatrix();
        System.out.println(""Input features: "" + features);
        INDArray labels = t.getLabels();
        INDArray predicted = model.output(features,false);
        System.out.println(""Predicted output: ""+ predicted);
        System.out.println(""Desired output: ""+ labels);
        eval.eval(labels, predicted);
        System.out.println();
    }
    System.out.println(eval.stats());
</code></pre>

<p><strong>Output from running the code above:</strong></p>

<blockquote>
  <p>Input features: [0.10,  0.34,  1.00,  0.00,  1.00]<br>
  Predicted output: <strong>[1.00,  0.00]</strong><br>
  Desired output: <strong>[1.00,  0.00]</strong><br></p>
</blockquote>

<p>*What I want the output to look like (i.e. a one-value probability):**</p>

<blockquote>
  <p>Input features: [0.10,  0.34,  1.00,  0.00,  1.00]<br>
  Predicted output: <strong>0.14</strong><br>
  Desired output: <strong>0.0</strong><br></p>
</blockquote>
","<deeplearning4j>","2017-12-27 12:05:13","","2","9144781","2017-12-27 13:31:11","1","0","","","9144781","31","2","0"
"47017582","<p>I am using a <code>MultiLayerNetwork</code>. This network is trained and the results are as expected.
As part of an effort to implement <code>Federated Learning</code> I need to upgrade the weights of this network with a new set of gradients coming from an external component.</p>

<p>My question is, in DL4J, once you execute <code>model.update(newGradients)</code>, are the weights updated too or do I need to execute something else?</p>

<p>Thanks</p>
","<machine-learning><deeplearning4j>","2017-10-30 14:21:47","","0","813751","2017-11-01 19:15:14","1","0","","","813751","184","2","0"
"48708666","<p>I am using a Convolutional Neural Network and I am saving it and loading it via the model serializer class.</p>

<p>What I want to do is to be able to come back at a later time and continue training the model on new data provided to it.</p>

<p>What I am doing is I load it using
ComputationGraph net = ModelSerializer.restoreComputationGraph(modelFileName);</p>

<p>and then I give it the data like before with
net.train(dataSetIterator);</p>

<p>This seems to work, but it makes my accuracy really bad. It was about 89% before I did this, and, using the same data, it gets to be around 50% accurate after a few iterations (using the same data it just trained itself on, so if anything it should be getting stupidly more accurate right?).</p>

<p>Am I missing a step?</p>
","<conv-neural-network><deeplearning4j><pre-trained-model>","2018-02-09 15:05:47","","0","6080613","2018-02-26 15:14:56","1","0","","","6080613","13","0","0"
"39558642","<p>Using Word2vec and Doc2vec methods provided by Gensim, they have a distributed version which uses BLAS, ATLAS, etc to speedup (details <a href=""http://rare-technologies.com/word2vec-in-python-part-two-optimizing/"" rel=""noreferrer"">here</a>). However, is it supporting GPU mode? Is it possible to get GPU working if using Gensim?</p>
","<optimization><gpu><gensim><deeplearning4j>","2016-09-18 14:20:48","","8","4250174","2016-09-29 04:45:33","1","1","2","","4250174","81","1","0"
"46726599","<p>I am using dl4j examples in my system. It's a maven project with various modules. There are several pom files in it. In the parent pom file I noticed I got an error stating: </p>

<blockquote>
  <p>Cannot resolve symbol 'session.executionRootDirectory'</p>
</blockquote>

<p>.</p>

<p>I don't remember changing this line (or this pom file either). The line producing the problem is this one:</p>

<pre><code>&lt;configFile&gt;${session.executionRootDirectory}/contrib/formatter.xml&lt;/configFile&gt;
</code></pre>

<p>in file <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/pom.xml"" rel=""nofollow noreferrer"">parent pom file</a> line 90.</p>

<p>My question is since <code>${session.executionRootDirectory}</code> seems as a variable which my system fails to identify is it something of an environment variable I should set? Should this be set by another way?
Any idea of what <code>${session.executionRootDirectory}</code> is and how to set it welcome.</p>

<p>My system is win7 and I am using Intellij Idea.</p>

<p>The problem seem to be solved by replacing the variable with an actual value of the path to the file it points to (that is <em>formatter.xml</em>) but I would like to know why the solution with the variable does not work.</p>
","<java><maven><deeplearning4j>","2017-10-13 09:23:45","","0","3584765","2017-10-17 07:11:59","1","5","","","3584765","2406","230","10"
"37909556","<p>I trained the word2vec model from <a href=""http://deeplearning4j.org/word2vec"" rel=""nofollow"">http://deeplearning4j.org/word2vec</a> 
successfully and now get this exception when trying to apply the 
wordsNearest:</p>

<pre><code>Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 99
at 
org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.loadTxt(WordVectorSerializer.java:1107)
at 
org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.loadTxtVectors(WordVectorSerializer.java:1033)
at 
org.deeplearning4j.examples.nlp.word2vec.NearestWords.main(NearestWords.java:13)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
</code></pre>

<p>This is my code:</p>

<pre><code>package org.deeplearning4j.examples.nlp.word2vec;
import java.io.File;
import java.util.Collection;
import org.deeplearning4j.models.embeddings.loader.WordVectorSerializer;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
public class NearestWords {
    public static void main(String[] args) throws Exception{
        File file = new File(""pathToWriteto.txt"");
        WordVectors vec = WordVectorSerializer.loadTxtVectors(file);
        Collection&lt;String&gt; similar = vec.wordsNearest(""day"", 10);
        System.out.println(similar);
    }
}
</code></pre>
","<java><deep-learning><word2vec><deeplearning4j>","2016-06-19 16:45:34","","0","5809613","2016-06-29 01:32:55","1","1","","","5809613","75","3","0"
"37494727","<p>I'm trying to create an AI for the risk game Lux Delux that will make use of a neural net trained in Deeplearning4j.</p>

<p>Naturally, when I compile I get a bunch of package does not exist errors. How would one go about adding such dependencies to an ant build? Do I need to use Ivy? Build file is as follows:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project name=""SillysoftSDK"" default=""compile"" basedir="".""&gt;
    &lt;!-- Edit these properties to fit your build environment. 
         Then you will be able to run the 'ant dist' command to re-compile 
         and deploy your file to where Lux will find it. --&gt;
    &lt;property name=""built_file"" location=""build/com/sillysoft/lux/agent/YourClassName.class"" /&gt;
    &lt;property name=""lux_agent_folder"" location=""${user.home}/Library/Application Support/Lux/Agents/"" /&gt;
    &lt;property name=""lux_mapgen_folder"" location=""${user.home}/Library/Application Support/Lux/MapGenerators/"" /&gt;


    &lt;!-- Move your agent class into Lux's agent folder (switch it to lux_mapgen_folder if needed) --&gt;
    &lt;target name=""dist"" depends=""compile""&gt;
        &lt;copy file=""${built_file}"" todir=""${lux_agent_folder}""/&gt;
    &lt;/target&gt;


    &lt;!-- Clean all build products --&gt;
    &lt;target name=""clean""&gt;
        &lt;delete dir=""build""/&gt;
    &lt;/target&gt;

    &lt;!-- Compile the java files into their .class files --&gt;
    &lt;target name=""compile""&gt;
        &lt;mkdir dir=""build""/&gt;
        &lt;javac srcdir=""src"" destdir=""build""
            debug=""true"" 
            debuglevel=""lines,vars,source"" 
            includeantruntime=""false""
            target=""1.7""
            source=""1.7"" &gt;
        &lt;compilerarg value=""-XDignore.symbol.file""/&gt;
        &lt;/javac&gt;
    &lt;/target&gt;

&lt;/project&gt;
</code></pre>
","<java><ant><deeplearning4j>","2016-05-28 02:55:31","","1","5810383","2016-07-25 12:13:01","1","2","","","5810383","106","2","0"
"46953999","<p>I am trying to run <code>CnnSentenceClassification</code> from deeplearning4j example. I moved this file to my Gradle project. When I run the class from the eclipse it works fine. However when I run it from <code>./gradlew run</code> I get following error:</p>

<pre><code>Exception in thread ""main"" java.lang.ExceptionInInitializerError
at 
main.CnnSentenceClassification.main(CnnSentenceClassification.java:75)
Caused by: java.lang.RuntimeException: 
org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: 
Please ensure that you have an nd4j backend on your classpath. Please 
see: http://nd4j.org/getstarted.html
at org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:6089)
at org.nd4j.linalg.factory.Nd4j.&lt;clinit&gt;(Nd4j.java:201)
... 1 more
Caused by: 
org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: 
Please ensure that you have an nd4j backend on your classpath. Please 
see: http://nd4j.org/getstarted.html
at org.nd4j.linalg.factory.Nd4jBackend.load(Nd4jBackend.java:258)
at org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:6086)
... 2 more
</code></pre>

<p>I checked and <code>nd4j-api-0.9.1.jar</code> is in my classpath. This is my <code>build.gradle</code>:</p>

<pre><code>apply plugin: 'java'
apply plugin: 'eclipse'
apply plugin: 'application'

repositories {
    jcenter()
}

mainClassName=""main.CnnSentenceClassification""

dependencies {
    compile group: 'org.deeplearning4j', name: 'deeplearning4j-core', version: '0.9.1'
    compile group: 'org.deeplearning4j', name: 'deeplearning4j-nlp', version: '0.9.1'       

    testCompile group: 'org.nd4j', name: 'nd4j-native-platform', version: '0.9.1'
    compile group: 'org.nd4j', name: 'nd4j-api', version: '0.9.1'

    compile ""org.slf4j:slf4j-simple:1.7.25""
    compile ""org.slf4j:slf4j-api:1.7.25""
}
</code></pre>
","<java><eclipse><gradle><deeplearning4j><nd4j>","2017-10-26 12:10:48","","1","4869675","2017-10-26 13:45:11","1","0","","","4869675","23","32","0"
"46459801","<p>Given a trained system, a network can be run backward with output values and partial inputs to find the value of a missing input value. Is there a name for this operation? </p>

<p>In example with a trained XOR network with 2 input neurons (with values 1 and X) and an output layer neuron (with value 1). If someone wanted to find what the value of the second input neuron was, they could feed the information backwards can calculate that it would be close to 0. What exactly is this operation called?</p>
","<machine-learning><neural-network><dataset><deep-learning><deeplearning4j>","2017-09-28 01:59:03","","3","1972245","2017-09-28 08:05:19","2","0","0","","1972245","311","29","1"
"48363951","<p>fooling around with deeplearning4j in eclipse oxygen on windows 8.1</p>

<p>usually after running the <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/feedforward/regression/RegressionMathFunctions.java"" rel=""nofollow noreferrer"">LR sample program</a> a few times and perhaps doing a gradle build, my d: drive (containing the source code) goes off-line. comes back after a reboot.</p>

<p>has anyone else experience this?</p>

<p>thanks</p>

<p>edit. this happened again when i tried to do a build with gradle from eclipse.</p>

<p>Launching Gradle tasks failed due to an error connecting to the Gradle build.
Could not execute build using Gradle distribution '<a href=""https://services.gradle.org/distributions/gradle-2.14.1-bin.zip"" rel=""nofollow noreferrer"">https://services.gradle.org/distributions/gradle-2.14.1-bin.zip</a>'.</p>

<p>Failed to release lock on task history cache (D:\ray\dev\ml\trygradleanddl4j.gradle\2.14.1\taskArtifacts)
The device is not ready
org.gradle.tooling.GradleConnectionException: Could not execute build using Gradle distribution '<a href=""https://services.gradle.org/distributions/gradle-2.14.1-bin.zip"" rel=""nofollow noreferrer"">https://services.gradle.org/distributions/gradle-2.14.1-bin.zip</a>'.
    at org.gradle.tooling.internal.consumer.ExceptionTransformer.transform(ExceptionTransformer.java:55)
...
org.gradle.cache.internal.filelock.LockFileAccess.clearLockInfo(LockFileAccess.java:73)
    at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock$2.stop(DefaultFileLockManager.java:228)
    ... 74 more</p>
","<java><windows><gradle><deep-learning><deeplearning4j>","2018-01-21 05:25:05","","0","51292","2018-01-24 06:15:47","1","0","","","51292","7386","62","1"
"37410308","<p>I'm trying to implement the code described in this blogpost with deeplearning4j.</p>

<p><a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"" rel=""nofollow"">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a></p>

<p>I'm having problems implementing multiple kernels on the same channel and concatenating the resulting tensors. Is there even a concat function in deeplearning4j? And is it even possible to have different kernels on the same channel?</p>

<p>Tank you in advance.</p>
","<machine-learning><tensorflow><deeplearning4j>","2016-05-24 10:01:44","","0","2509422","2016-10-25 22:53:16","1","1","","","2509422","85","2","0"
"37855760","<p>I'm new to the deeplearning4j library, but I've got some experience with neural networks in general.<br>
I'm trying to train a recurrent neural network (a LSTM in particular) which is supposed to detect beats in music in realtime. All examples for using recurrent neural nets with deeplearning4j that I've found so far use a reader which reads the training data from a file. As I want to record music in realtime via a microphone, I can't read some pregenerated file, so the data which is fed into the neural network is generated in realtime by my application.  </p>

<p>This is the code that I'm using to generate my network:  </p>

<pre><code>    NeuralNetConfiguration.ListBuilder builder = new NeuralNetConfiguration.Builder()
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)
            .learningRate(0.1)
            .rmsDecay(0.95)
            .regularization(true)
            .l2(0.001)
            .weightInit(WeightInit.XAVIER)
            .updater(Updater.RMSPROP)
            .list();

    int nextIn = hiddenLayers.length &gt; 0 ? hiddenLayers[0] : numOutputs;
    builder = builder.layer(0, new GravesLSTM.Builder().nIn(numInputs).nOut(nextIn).activation(""softsign"").build());

    for(int i = 0; i &lt; hiddenLayers.length - 1; i++){
        nextIn = hiddenLayers[i + 1];
        builder = builder.layer(i + 1, new GravesLSTM.Builder().nIn(hiddenLayers[i]).nOut(nextIn).activation(""softsign"").build());
    }

    builder = builder.layer(hiddenLayers.length, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(nextIn).nOut(numOutputs).activation(""softsign"").build());

    MultiLayerConfiguration conf = builder.backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(DEFAULT_RECURRENCE_DEPTH).tBPTTBackwardLength(DEFAULT_RECURRENCE_DEPTH)
            .pretrain(false).backprop(true)
            .build();

    net = new MultiLayerNetwork(conf);
    net.init();  
</code></pre>

<p>In this case I'm using about 700 inputs (which is mostly FFT-data of the recorded audio), 1 output (which is supposed to output a number between 0 [no beat] and 1 [beat]) and my hiddenLayers array consists of the ints {50, 25, 10}.  </p>

<p>For getting the output of the network I'm using this code:  </p>

<pre><code>    double[] output = new double[]{net.rnnTimeStep(Nd4j.create(netInputData)).getDouble(0)};
</code></pre>

<p>where netInputData is the data I want to input into the network as a one-dimensional double array.<br>
I'm relatively sure that this code is working fine, since I get some output for an untrained network which looks <a href=""http://i.imgur.com/2JVe542.png"" rel=""nofollow"">something like this</a> when I plot it.<br>
However, once I try to train a network (even if I train it just for a short time, which should alter the weights of the network just a little bit, so that the output should be very similar to the untrained network), I get an output which <a href=""http://i.imgur.com/YrJLnFY.png"" rel=""nofollow"">looks like a constant</a>.  </p>

<p>This is the code which I'm using to train the network:</p>

<pre><code>    for(int timestep = 0; timestep &lt; trainingData.length - DEFAULT_RECURRENCE_DEPTH; timestep++){
        INDArray inputDataArray = Nd4j.create(new int[]{1, numInputs, DEFAULT_RECURRENCE_DEPTH},'f');
        for(int inputPos = 0; inputPos &lt; trainingData[timestep].length; inputPos++)
            for(int inputTimeWindowPos = 0; inputTimeWindowPos &lt; DEFAULT_RECURRENCE_DEPTH; inputTimeWindowPos++)
                inputDataArray.putScalar(new int[]{0, inputPos, inputTimeWindowPos}, trainingData[timestep + inputTimeWindowPos][inputPos]);

        INDArray desiredOutputDataArray = Nd4j.create(new int[]{1, numOutputs, DEFAULT_RECURRENCE_DEPTH},'f');
        for(int outputPos = 0; outputPos &lt; desiredOutputData[timestep].length; outputPos++)
            for(int inputTimeWindowPos = 0; inputTimeWindowPos &lt; DEFAULT_RECURRENCE_DEPTH; inputTimeWindowPos++)
                desiredOutputDataArray.putScalar(new int[]{0, outputPos, inputTimeWindowPos}, desiredOutputData[timestep + inputTimeWindowPos][outputPos]);

        net.fit(new DataSet(inputDataArray, desiredOutputDataArray));
    }  
</code></pre>

<p>Once again, I've got my data for the input and for the desired output as a double array. This time the two arrays are two-dimensional. The first index represents the time (where index 0 is the first audio data of the recorded audio) and the second index represents the input (or respectively the desired output) for this time step.<br>
Given the shown output after training a network, I tend to think that there must be something wrong with my code used for creating the INDArrays from my data. Am I missing some important step for initializing these arrays or did I mess up the order I need to put my data into these arrays?  </p>

<p>Thank you for any help in advance.</p>
","<java><deep-learning><lstm><recurrent-neural-network><deeplearning4j>","2016-06-16 09:53:17","","2","6473700","2016-12-05 20:44:54","1","1","","","6473700","96","3","0"
"41529464","<p>I am confused about changing the parameters for training a <code>net</code> in caffe based on our own data. </p>

<ul>
<li>Which layers of net we should pay attention more to train the net on
our own data? For example, the number of outputs based on the number
of classes.</li>
</ul>

<p>I tried to train FCN32 for semantic segmentation. I changed the number of outputs in <code>Deconvolution layer (i.e.,upscore_sign)</code> to the number of classes in my data, but it is giving an error.  </p>

<ul>
<li>We have different outputs in different convolutional layers. How can
I detect different outputs from each other and which one should I
change?</li>
<li>The next question is what is the difference between <code>deploy.prototxt</code>
and <code>train_val.prototxt</code>.</li>
<li>And what is the application of deploy.prototxt? Should I change the
layers in <code>deploy.prototxt</code> based on the train_val.prototxt?</li>
</ul>

<p>I really appreciate if someone knows, please share your knowledge.</p>

<p>Thanks </p>
","<caffe><pycaffe><deeplearning4j><matcaffe>","2017-01-08 04:30:05","","2","6494707","2017-01-08 11:55:55","1","0","","","6494707","715","63","1"
"36214924","<p>I created a RNN program which creates an RNN model, trains it and predicts using the model using deeplearning4j. It runs with a good speed(30 minutes)but when it is added to the spark pipeline, it takes about 10 hours to run.Can you give me reason for that?</p>
","<performance><machine-learning><apache-spark-sql><deep-learning><deeplearning4j>","2016-03-25 06:08:49","","0","5950143","2016-03-25 06:08:49","0","4","","","5950143","131","40","0"
"48956485","<p>I have a deeplearning for java project which is producing huge amounts of logger output on STDO. I want to disable that but I cant seem to figure out how to do it.</p>

<p>I have a <code>log4j.properties</code> file in my src/main/resources folder which looks like this:</p>

<pre><code>log4j.rootLogger=ERROR, Console
log4j.logger.play=WARN
log4j.appender.Console=org.apache.log4j.ConsoleAppender
log4j.appender.Console.layout=org.apache.log4j.PatternLayout
log4j.appender.Console.layout.ConversionPattern=%d{ABSOLUTE} %-5p ~ %m%n

log4j.appender.org.springframework=WARN
log4j.appender.org.nd4j=WARN
log4j.appender.org.canova=WARN
log4j.appender.org.datavec=WARN
log4j.appender.org.deeplearning4j=WARN
log4j.appender.opennlp.uima=OFF
log4j.appender.org.apache.uima=OFF
log4j.appender.org.cleartk=OFF

log4j.logger.org.springframework=WARN
log4j.logger.org.nd4j=WARN
log4j.logger.org.canova=WARN
log4j.logger.org.datavec=WARN
log4j.logger.org.deeplearning4j=WARN
log4j.logger.opennlp.uima.util=OFF
log4j.logger.org.apache.uima=OFF
log4j.logger.org.cleartk=OFF
log4j.logger.org.deeplearning4j.optimize.solvers.BaseOptimizer=OFF
slf4j.logger.org.deeplearning4j.optimize.solvers.BaseOptimizer=OFF
</code></pre>

<p>The specific output that is far too much is: </p>

<p><code>21:26:34.860 [main] DEBUG o.d.optimize.solvers.BaseOptimizer - Hit termination condition on iteration 0: score=1.2894165074915344E19, oldScore=1.2894191699433697E19, condition=org.deeplearning4j.optimize.terminations.EpsTermination@55f111f3</code></p>

<p>which happens multiple times a second while training.</p>
","<java><logging><log4j><slf4j><deeplearning4j>","2018-02-23 21:29:56","","0","2329773","2018-02-23 22:18:54","1","2","","","2329773","753","54","21"
"41611130","<p>I am trying to run FCN-8s. I did the following steps:
1. downloaded <a href=""https://github.com/developmentseed/caffe-fcn"" rel=""nofollow noreferrer"">this repository</a> 
2. converting my data to LMDB and changing the paths in train_val.prototxt
3. downloading the fcn8s-heavy-pascal <code>caffemodel</code>
4. changing the <code>number_of_output</code> in <code>train_val.prototxt</code> and <code>deploy.prototxt</code> from <strong>60</strong> to <strong>5</strong> (the number of classes in my data) in the last following layers:</p>

<pre><code>layer {
  name: ""score59""
  type: ""Convolution""
  bottom: ""fc7""
  top: ""score59""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 5 #60
    kernel_size: 1
    engine: CAFFE
  }
}
layer {
  name: ""upscore2""
  type: ""Deconvolution""
  bottom: ""score59""
  top: ""upscore2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 5 #60
    bias_term: false
    kernel_size: 4
    stride: 2
  }
}
layer {
  name: ""score-pool4""
  type: ""Convolution""
  bottom: ""pool4""
  top: ""score-pool4""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 5 #60
    kernel_size: 1
    engine: CAFFE
  }
}
layer { type: 'Crop' name: 'crop' bottom: 'score-pool4' bottom: 'upscore2'
  top: 'score-pool4c' }
layer {
  name: ""fuse""
  type: ""Eltwise""
  bottom: ""upscore2""
  bottom: ""score-pool4c""
  top: ""score-fused""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""upsample-fused-16""
  type: ""Deconvolution""
  bottom: ""score-fused""
  top: ""score4""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 5 #60
    bias_term: false
    kernel_size: 4
    stride: 2
  }
}
layer {
  name: ""score-pool3""
  type: ""Convolution""
  bottom: ""pool3""
  top: ""score-pool3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 5 #60
    kernel_size: 1
    engine: CAFFE
  }
}
layer { type: 'Crop' name: 'crop' bottom: 'score-pool3' bottom: 'score4'
  top: 'score-pool3c' }
layer {
  name: ""fuse""
  type: ""Eltwise""
  bottom: ""score4""
  bottom: ""score-pool3c""
  top: ""score-final""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""upsample""
  type: ""Deconvolution""
  bottom: ""score-final""
  top: ""bigscore""
  param {
    lr_mult: 0
  }
  convolution_param {
    num_output: 5 #60
    bias_term: false
    kernel_size: 16
    stride: 8
  }
}
layer { type: 'Crop' name: 'crop' bottom: 'bigscore' bottom: 'data' top: 'score' }
layer {
  name: ""loss""
  type: ""SoftmaxWithLoss""
  bottom: ""score""
  bottom: ""label""
  top: ""loss""
  loss_param {
    normalize: false
  }
}
</code></pre>

<p>I started the training with the weights of pre-trained model for pascal dataset. But the the loss remains constant (loss = 105476) over the time.</p>

<pre><code>0112 18:25:07.198588  5878 sgd_solver.cpp:106] Iteration 150, lr = 1e-14
I0112 18:26:07.614239  5878 solver.cpp:228] Iteration 200, loss = 105476
I0112 18:26:07.614459  5878 solver.cpp:244]     Train net output #0: loss = 105476 (* 1 = 105476 loss)
I0112 18:26:07.614490  5878 sgd_solver.cpp:106] Iteration 200, lr = 1e-14
I0112 18:27:06.198556  5878 solver.cpp:228] Iteration 250, loss = 105476
I0112 18:27:06.198801  5878 solver.cpp:244]     Train net output #0: loss = 105476 (* 1 = 105476 loss)
I0112 18:27:06.198834  5878 sgd_solver.cpp:106] Iteration 250, lr = 1e-14
I0112 18:28:05.056469  5878 solver.cpp:228] Iteration 300, loss = 105476
I0112 18:28:05.056715  5878 solver.cpp:244]     Train net output #0: loss = 105476 (* 1 = 105476 loss)
I0112 18:28:05.056751  5878 sgd_solver.cpp:106] Iteration 300, lr = 1e-14
I0112 18:29:04.537042  5878 solver.cpp:228] Iteration 350, loss = 105476
I0112 18:29:04.537261  5878 solver.cpp:244]     Train net output #0: loss = 105476 (* 1 = 105476 loss)
I0112 18:29:04.537293  5878 sgd_solver.cpp:106] Iteration 350, lr = 1e-14
I0112 18:30:05.320504  5878 solver.cpp:228] Iteration 400, loss = 105476
I0112 18:30:05.320751  5878 solver.cpp:244]     Train net output #0: loss = 105476 (* 1 = 105476 loss)
I0112 18:30:05.320796  5878 sgd_solver.cpp:106] Iteration 400, lr = 1e-14
I0112 18:31:06.690937  5878 solver.cpp:228] Iteration 450, loss = 105476
I0112 18:31:06.691177  5878 solver.cpp:244]     Train net output #0: loss = 105476 (* 1 = 105476 loss)
I0112 18:31:06.691207  5878 sgd_solver.cpp:106] Iteration 450, lr = 1e-14
I0112 18:32:06.593940  5878 solver.cpp:228] Iteration 500, loss = 105476
I0112 18:32:06.596643  5878 solver.cpp:244]     Train net output #0: loss = 105476 (* 1 = 105476 loss)
I0112 18:32:06.596701  5878 sgd_solver.cpp:106] Iteration 500, lr = 1e-14
</code></pre>

<p>I do not know which part I am doing wrong. I really appreciate your help to solve this issue.</p>
","<deep-learning><caffe><pycaffe><deeplearning4j><matconvnet>","2017-01-12 10:46:15","","1","6494707","2017-04-27 14:00:12","1","2","","","6494707","715","63","1"
"50945820","<p>I am using deeplearning4j java library to build paragraph vector model (doc2vec) of dimension 100. I am using a text file. It has around 17 million lines, and size of the file is 330 MB. 
I can train the model and calculate paragraph vector which gives reasonably good results.</p>

<p>The problem is that when I try to save  the model (by writing to disk) with WordVectorSerializer.writeParagraphVectors (dl4j method) it takes around 20 GB of space.  And around 30GB when I use native java serializer. </p>

<p>I'm thinking may be the model is size is too big for that much data. Is the model size 20GB reasonable for the text data of 300 MB?  </p>

<p>Comments are also welcome from people who have used doc2vec/paragraph vector in other library/language. </p>

<p>Thank you!</p>
","<nlp><gensim><word-embedding><doc2vec><deeplearning4j>","2018-06-20 10:17:47","","3","1478061","2018-06-21 00:14:16","1","0","1","","1478061","321","167","12"
"37134408","<p>When I run the sample program <code>Word2VecRawTextExample.java</code></p>

<p>url: 
[ <a href=""https://raw.githubusercontent.com/deeplearning4j/dl4j-0.4-examples/master/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java"" rel=""nofollow"">https://raw.githubusercontent.com/deeplearning4j/dl4j-0.4-examples/master/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java</a> ]</p>

<p>to generate word2Vec vectors for a given file of sample sentences, I get different word vector weightings each time I run the process.</p>

<p>I would have thought that given the process is seeded, the result should be the same each time?</p>
","<java><word2vec><deeplearning4j>","2016-05-10 09:28:49","","3","2945735","2016-05-18 06:02:14","1","0","1","","2945735","21","0","0"
"55753493","<p>I trained a little model using keras+tensorflow. At this model, I try to identify some images categories. When I load this model using python+keras it's working OK and make the prediction in a satisfactory way.  </p>

<p>The problem starts when I try to open this model in a java application using deeplearning4j lib</p>

<p>Below are the source code used to create the keras model and export it to the .h5 file, and the  java import .h5 file</p>

<p><b>Here my model structure defined in python</b></p>

<pre class=""lang-py prettyprint-override""><code>
img_width, img_height = 150, 150
batch_size = 32
samples_per_epoch = 1000
validation_steps = 300
nb_filters1 = 32
nb_filters2 = 64
conv1_size = 3
conv2_size = 2
pool_size = 2
classes_num = 3
lr = 0.0004

model = Sequential()
model.add(Convolution2D(nb_filters1, conv1_size, conv1_size, border_mode =""same"", input_shape=(img_width, img_height, 3)))
model.add(Activation(""relu""))
model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))

model.add(Convolution2D(nb_filters2, conv2_size, conv2_size, border_mode =""same""))
model.add(Activation(""relu""))
model.add(MaxPooling2D(pool_size=(pool_size, pool_size), dim_ordering='th'))


model.add(Flatten())
model.add(Dense(256))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(classes_num, activation='softmax'))


model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=lr),
              metrics=['accuracy'])
...
...
...

model.save('./testesAndre/Exemplos_Keras/model/Docs_model.h5')
</code></pre>

<p><b> Here the java code to open the keras model</b></p>

<pre class=""lang-java prettyprint-override""><code>import org.deeplearning4j.nn.modelimport.keras.KerasModelImport;

...

String simpleMlp = new ClassPathResource(sFilePath).getFile().getPath();
this.kerasH5Model = KerasModelImport.importKerasSequentialModelAndWeights(simpleMlp,true);

</code></pre>

<p><b>Error when i try import a model in java using the deeplearning4j lib</b></p>

<pre><code>org.deeplearning4j.nn.modelimport.keras.exceptions.InvalidKerasConfigurationException: Cannot assign arrays: arrays must both be scalars, both vectors, or shapes must be equal other than size 1 dimensions. Attempting to do x.assign(y) with x.shape=[87616, 256] and y.shape=[88800, 256]
Tried to set weights for layer with name dense_1, of class org.deeplearning4j.nn.conf.layers.DenseLayer.
Failed to set weights for parameter W
Expected shape for this parameter: Rank: 2,Offset: 0
 Order: f Shape: [87616,256],  stride: [1,87616], 
got: Rank: 2,Offset: 0
 Order: c Shape: [88800,256],  stride: [256,1]. For more information, see http://deeplearning4j.org/model-import-keras.
    at org.deeplearning4j.nn.modelimport.keras.KerasLayer.copyWeightsToLayer(KerasLayer.java:334)
    at org.deeplearning4j.nn.modelimport.keras.utils.KerasModelUtils.copyWeightsToModel(KerasModelUtils.java:76)
    at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:248)
    at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork(KerasSequentialModel.java:235)
    at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:181)
</code></pre>

<p>Sorry if I'm committing some primary error because this is the first time I use python and the deeplearnig4l lib</p>

<p>Thank you all for your help</p>
","<java><tensorflow><keras><deeplearning4j>","2019-04-18 20:43:36","","0","11380847","2019-04-19 11:39:06","0","0","","","11380847","1","0","0"
"32858693","<p>I have a bunch of sensors and I really just want to reconstruct the input. </p>

<p>So what I want is this: </p>

<ol>
<li>after I have trained my model I will pass in my feature matrix </li>
<li>get the reconstructed feature matrix back</li>
<li>I want to investigate which sensor values are completely different from the reconstructed value</li>
</ol>

<p>Therefore I thought a RBM will be the right choice and since I am used to Java, I have tried to use deeplearning4j. But I got stuck very early. If you run the following code, I am facing 2 problems. </p>

<ol>
<li><p>The result is far away from a correct prediction, most of them are simply [1.00,1.00,1.00].</p></li>
<li><p>I would expect to get back 4 values (which is the number of inputs expected to be reconstructed)</p></li>
</ol>

<p>So what do I have to tune to get a) a better result and b) get the reconstructed inputs back?</p>

<pre><code>public static void main(String[] args) {
    // Customizing params
    Nd4j.MAX_SLICES_TO_PRINT = -1;
    Nd4j.MAX_ELEMENTS_PER_SLICE = -1;
    Nd4j.ENFORCE_NUMERICAL_STABILITY = true;
    final int numRows = 4;
    final int numColumns = 1;
    int outputNum = 3;
    int numSamples = 150;
    int batchSize = 150;
    int iterations = 100;
    int seed = 123;
    int listenerFreq = iterations/5;

    DataSetIterator iter = new IrisDataSetIterator(batchSize, numSamples);

    // Loads data into generator and format consumable for NN
    DataSet iris = iter.next();
    iris.normalize();
    //iris.scale();
    System.out.println(iris.getFeatureMatrix());

    NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
            // Gaussian for visible; Rectified for hidden
            // Set contrastive divergence to 1
            .layer(new RBM.Builder()
                    .nIn(numRows * numColumns) // Input nodes
                    .nOut(outputNum) // Output nodes
                    .activation(""tanh"") // Activation function type
                    .weightInit(WeightInit.XAVIER) // Weight initialization
                    .lossFunction(LossFunctions.LossFunction.XENT)
                    .updater(Updater.NESTEROVS)
                    .build())
            .seed(seed) // Locks in weight initialization for tuning
            .iterations(iterations)
            .learningRate(1e-1f) // Backprop step size
            .momentum(0.5) // Speed of modifying learning rate
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // ^^ Calculates gradients
            .build();

    Layer model = LayerFactories.getFactory(conf.getLayer()).create(conf);
    model.setListeners(Arrays.asList((IterationListener) new ScoreIterationListener(listenerFreq)));

    model.fit(iris.getFeatureMatrix());
    System.out.println(model.activate(iris.getFeatureMatrix(), false));
}
</code></pre>
","<java><deep-learning><rbm><deeplearning4j>","2015-09-30 05:49:29","","1","1298461","2017-03-08 09:11:00","1","1","","","1298461","2951","209","2"
"35860597","<p>I am trying to add sentiment analysis program to Spark pipeline. When doing it, I have class which extends <code>org.apache.spark.ml.PredictionModel</code>. When extending this <code>PredictionModel</code> class, I have to override <code>predict()</code> method which predicts the label for given feature. But, I get either 0 or 1 all the time when I execute this code.For example, if there are 10 movie reviews, five are negative reviews and other five are negative, it classifies all reviews as negative. I have attached the code below.</p>

<pre><code>import org.apache.spark.ml.PredictionModel;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.mllib.linalg.DenseVector;
import org.apache.spark.mllib.linalg.Vector;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.nd4j.linalg.api.buffer.DataBuffer;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import java.io.*;

//Model produced by a ProbabilisticClassifier
public class MovieReviewClassifierModel extends PredictionModel&lt;Object, MovieReviewClassifierModel&gt; implements  Serializable{


    private static final long serialVersionUID = 1L;
    private MultiLayerNetwork net;

    MovieReviewClassifierModel (MultiLayerNetwork net) throws Exception {
        this.net=net;
 }

    @Override
    public MovieReviewClassifierModel copy(ParamMap args0) {
        return null;
    }

    @Override
    public String uid() {
        return ""MovieReviewClassifierModel"";
    }


    public double raw2prediction(Vector rawPrediction) {//Given a vector of raw predictions, select the predicted label
        return rawPrediction.toArray()[0];
    }

    @Override
    public double predict(Object o) {

        int prediction=0;
        DenseVector v=(DenseVector)o;
        double[] a=v.toArray();
        INDArray arr=Nd4j.create(a);
        INDArray array= net.output(arr,false);
        DataBuffer ob = array.data();
        double[] d=ob.asDouble();
        double zeroProbability=d[0];
        double oneProbability=d[1];
        if (zeroProbability &gt; oneProbability) {
            prediction=0;
        }
        else{
            prediction=1;

        }


        return prediction;
    }


}
</code></pre>

<p>Can you give me reasons for the wrong predictions?</p>
","<java><apache-spark><neural-network><deep-learning><deeplearning4j>","2016-03-08 06:34:22","","0","5950143","2016-03-08 08:03:29","1","0","","","5950143","131","40","0"
"51456905","<p>Is there anyway we could find line number of ""Un-terminated quoted field at end of CSV line""? I'm trying to read CSV file in java, but getting error most probably because - there is a quote is opened but isn't closed. 
I'm using CSVRecordReader of deeplearning4j - datavec. </p>

<p>The idea is that if I find that line number, I can remove/correct it. But problem is, I can not find the line number when error is thrown. </p>

<p>If you have any suggestion - in Java or other, please let me know. Thanks!</p>
","<java><csv><import><deeplearning4j>","2018-07-21 14:18:16","","1","1478061","2018-07-22 06:54:41","4","5","","","1478061","321","167","12"
"51531433","<p>I have made a program that trains a ComputationGraph Convolutional Neural Network using a Word2Vec object to help. These are saved to files and can then be loaded by the program being run again later.</p>

<p>I'm trying to make a jar that uses resources inside of it. The ComputationGraph is able to be loaded from inside the jar by using a method in ModelSerializer that takes an argument of type InputStream ( that method is restoreComputationGraph(InputStream is) ).</p>

<p>Is there a similar method to restore a Word2Vec object in WordVectorSerializer or elsewhere in the DL4j project? I am unable to find it.</p>

<p>Failing that, is there some other way of loading a Word2Vec from inside of the jar file?</p>

<p>Thanks</p>
","<java><word2vec><deeplearning4j><dl4j>","2018-07-26 05:14:05","","0","6080613","2018-07-26 05:14:05","0","1","","","6080613","13","0","0"
"46221305","<p>I am trying to train Doc2Vec model using the following code:</p>

<pre><code>String modelPath = ""input_data.csv"";
File file = new File(modelPath);
SentenceIterator iter = new BasicLineIterator(file);

AbstractCache&lt;VocabWord&gt; cache = new AbstractCache&lt;&gt;();

TokenizerFactory t = new DefaultTokenizerFactory();
t.setTokenPreProcessor(new CommonPreprocessor());

LabelsSource source = new LabelsSource(""DOC_"");

ParagraphVectors vec = new ParagraphVectors.Builder()
    .minWordFrequency(1)
    .iterations(5)
    .epochs(1)
    .layerSize(100)
    .learningRate(0.025)
    .labelsSource(source)
    .windowSize(5)
    .iterate(iter)
    .trainWordVectors(false)
    .vocabCache(cache)
    .tokenizerFactory(t)
    .sampling(0)
    .workers(4)
    .build();

vec.fit();
File tempFile = new File(""trained_model.zip"");
WordVectorSerializer.writeParagraphVectors(vec, tempFile);
</code></pre>

<ul>
<li>This code works for small input file</li>
<li><p>When I try to execute this code on large file (18GB), I am getting the following error</p>

<pre><code>.........
o.d.m.s.SequenceVectors - Time spent on training: 5667912 ms
Exception in thread ""main"" java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: Stream Closed
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.writeParagraphVectors(WordVectorSerializer.java:477)
at org.deeplearning4j.examples.nlp.paragraphvectors.ParagraphVectorsTextExample.main(ParagraphVectorsTextExample.java:73)
Caused by: java.lang.RuntimeException: java.io.IOException: Stream Closed
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.writeWordVectors(WordVectorSerializer.java:393)
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.writeParagraphVectors(WordVectorSerializer.java:687)
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.writeParagraphVectors(WordVectorSerializer.java:475)
... 1 more
Caused by: java.io.IOException: Stream Closed
at java.io.FileOutputStream.writeBytes(Native Method)
at java.io.FileOutputStream.write(FileOutputStream.java:326)
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
at java.io.FilterOutputStream.close(FilterOutputStream.java:158)
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.writeWordVectors(WordVectorSerializer.java:392)
... 3 more
</code></pre></li>
</ul>

<p>I am not sure what I am doing wrong. Is there any way around this?</p>
","<java><training-data><deeplearning4j><doc2vec><bigdata>","2017-09-14 14:07:04","","1","2183228","2017-09-15 02:32:52","0","5","","","2183228","26","0","0"
"36989291","<p><code>Word2vec</code> is a great tool is deeplearning4j. I managed to create a vector for a corpus following this <a href=""http://deeplearning4j.org/word2vec"" rel=""nofollow"">tutorial</a>. </p>

<p>The question now is how to update the model with new sentences without having to rebuild it again from scratch.</p>

<p>Some thoughts on this, would this method helps?</p>

<pre><code>public void trainSentence(List&lt;VocabWord&gt; sentence){}
</code></pre>

<p>Would that update the model? If yes, how to prepare the sentence to be sent to this method?</p>
","<deeplearning4j>","2016-05-02 18:37:16","","1","1019952","2016-05-25 15:17:50","1","0","","","1019952","6404","445","16"
"51106170","<p>I'm trying to do image classification. I'm using Scala, the Akka actor system, and deeplearning4j. The thing is that I have to detect always on the same spots or crop on the image. I was thinking of creating a new actor for each crop of the image, on each frame. The thing is that, from what I understand, instantiating a new model for each actor creation is not viable, but having an instance of the model, and passing to each actor isn't either. Should I have a pool of instances? I'm a bit stuck with this problem, since it is the first time I'm trying deeplearning4j. Previously, I would use a python REST api, but I think that this solution should perform better. </p>

<p>Thank you in advance.</p>
","<java><scala><akka><deeplearning4j>","2018-06-29 16:40:47","","1","4889611","2018-07-14 02:47:30","3","0","","","4889611","302","186","2"
"37212134","<p>We are working on a project in Java using neural networks. We want to test different network structures on our datasets. Now we evaluate which of the Java Neural Networks is the best in terms of performance. We are evaluating Encog, Neuroph and DL4J. Can you please tell us some good resources or your own experiences about this?
Thanks</p>
","<java><machine-learning><neural-network><encog><deeplearning4j>","2016-05-13 14:06:02","","3","3162981","2018-02-07 21:06:23","3","0","","","3162981","121","8","0"
"48426457","<p>I am using the LibSVM record reader to load sparse data into neural networks.</p>

<p>This worked fine when using a MLP model, but when I tried to load data into one of the example CNNs given in one of the problems:</p>

<pre><code>ComputationGraphConfiguration config = new NeuralNetConfiguration.Builder()
            .trainingWorkspaceMode(WorkspaceMode.SINGLE).inferenceWorkspaceMode(WorkspaceMode.SINGLE)
            //.trainingWorkspaceMode(WorkspaceMode.SEPARATE).inferenceWorkspaceMode(WorkspaceMode.SEPARATE)
            .weightInit(WeightInit.RELU)
            .activation(Activation.LEAKYRELU)
            .updater(Updater.ADAM)
            .convolutionMode(ConvolutionMode.Same)
            .regularization(true).l2(0.0001)
            .learningRate(0.01)
            .graphBuilder()
            .addInputs(""input"")
            .addLayer(""cnn3"", new ConvolutionLayer.Builder()
                .kernelSize(3, vectorSize)
                .stride(1, vectorSize)
                .nIn(1)
                .nOut(cnnLayerFeatureMaps)
                .build(), ""input"")
            .addLayer(""cnn4"", new ConvolutionLayer.Builder()
                .kernelSize(4, vectorSize)
                .stride(1, vectorSize)
                .nIn(1)
                .nOut(cnnLayerFeatureMaps)
                .build(), ""input"")
            .addLayer(""cnn5"", new ConvolutionLayer.Builder()
                    .kernelSize(5, vectorSize)
                    .stride(1, vectorSize)
                    .nIn(1)
                    .nOut(cnnLayerFeatureMaps)
                    .build(), ""input"")
            .addVertex(""merger"", new MergeVertex(), ""cnn3"", ""cnn4"", ""cnn5"")
            .addLayer(""globalPool"", new GlobalPoolingLayer.Builder()
                    .poolingType(globalPoolingType)
                    .dropOut(0.5)
                    .build(), ""merger"")
            .addLayer(""out"", new OutputLayer.Builder()
                    .lossFunction(LossFunctions.LossFunction.MCXENT)
                    .activation(Activation.SOFTMAX)
                    .nIn(3*cnnLayerFeatureMaps)
                    .nOut(classes.length)
                    .build(), ""globalPool"")
            .setOutputs(""out"")
            .setInputTypes(InputType.convolutionalFlat(32,45623,1))
            .build();
</code></pre>

<p>I got an error that seems to be saying that it was getting 2-dimensional data, but it needs 3-dimensional data (with the third dimension being a trivial one).</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: Invalid input: expect output columns must be equal to rows 32 x columns 45623 x channels 1 but was instead [32, 45623]
</code></pre>

<p>How do I give it the 1 channel dimension?</p>

<p>Failing that, how do I get the CNN to recognize channel-less data, or how do I give a CNN sparse data?</p>

<p>Thank you</p>
","<java><deep-learning><sparse-matrix><libsvm><deeplearning4j>","2018-01-24 15:48:59","","1","6080613","2018-01-25 01:24:50","1","2","","","6080613","13","0","0"
"41898017","<p>I'm new to deep learning, (particularly deeplearning4j) and am trying out the examples. Particularly, I want to know which type neural-network is used in the following CSV Example. Is this a deep learning neural network or just ""regular neural network"". I do understand that difference between normal neural network and deeplearning neural network is that DL algorithms tackle ""vanishing gradient"" problem, whereas normal neural network don't. I'm bit confused here. What I feel is that following is regular neural network, but I want to confirm. </p>

<pre><code>    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
        .seed(seed)
        .iterations(iterations)
        .activation(Activation.TANH)
        .weightInit(WeightInit.XAVIER)
        .learningRate(0.1)
        .regularization(true).l2(1e-4)
        .list()
        .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3)
            .build())
        .layer(1, new DenseLayer.Builder().nIn(3).nOut(3)
            .build())
        .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
            .activation(Activation.SOFTMAX)
            .nIn(3).nOut(outputNum).build())
        .backprop(true).pretrain(false)
        .build();

    //run the model
    MultiLayerNetwork model = new MultiLayerNetwork(conf);
    model.init();
    model.setListeners(new ScoreIterationListener(100));
</code></pre>

<p>model.fit(trainingData);</p>

<p>The code - <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/dataExamples/CSVExample.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/dataExamples/CSVExample.java</a></p>
","<deep-learning><deeplearning4j>","2017-01-27 15:56:20","","0","1478061","2017-01-27 16:07:50","1","0","","","1478061","321","167","12"
"40548031","<p>I would like to print the labels of traindata / testdata used in classification. Here is the definition of both inputs (using deep4j).</p>

<pre><code>    InputSplit[] inputSplit = fileSplit.sample(pathFilter, splitTrainTest, 1 - splitTrainTest);
    InputSplit trainData = inputSplit[0];
    InputSplit testData = inputSplit[1];
</code></pre>

<p>that are then transformed in DataSetIterator like this : </p>

<pre><code>    ImageRecordReader recordReader = new ImageRecordReader(height, width, channels, labelMaker);
    recordReader.initialize(trainData, null);
    trainIter = new RecordReaderDataSetIterator(recordReader, batchSize, 1, numLabels);
</code></pre>

<p>Then I want to print how many examples per labels where found in each iterator in this function : </p>

<pre><code>public void print(DataSetIterator iter){

    HashMap&lt;String, Integer&gt; hash = new HashMap&lt;String, Integer&gt;();

    while(iter.hasNext()){
        DataSet example = iter.next();
        for(int i = 0 ; i&lt;numLabels ; i++){
            if(example.getLabels().getDouble(i)==1.){
                String label = example.getLabelName(i);
                if(hash.containsKey(label))
                    hash.put(label, hash.get(label)+1);
                else
                    hash.put(label, 1);
            }
        }
    }

    for (String label: hash.keySet()){
        System.out.println(""   label : "" + label.toString() + "", "" + hash.get(label) + "" examples"");
    }
}
</code></pre>

<p>The issue is that it displays only one example per label, whereas there should much more... And when I don't split my dataset using <code>fileSplit.sample()</code> the function displays the right number of examples.
Any suggestion ?</p>
","<java><deep-learning><deeplearning4j>","2016-11-11 12:27:50","","0","4560470","2016-11-11 14:26:54","1","0","","","4560470","1054","285","2"
"50081322","<p>I try to create the easiest of a NeuralNetwork and training it with some data:
Therefore I created a test.csv with a the following pattern:</p>

<blockquote>
  <p>number,number+1;</p>
  
  <p>number2,number2+1</p>
</blockquote>

<p>...</p>

<p>I try to make a linear regression with the network...</p>

<p>But I do not find a way to acquire the data, DataSetIterator does not work.</p>

<p>How to fit the Data, how to test the Data?</p>
","<neural-network><deeplearning4j><data-acquisition>","2018-04-28 21:40:47","","0","8215588","2018-05-15 05:46:35","1","1","","","8215588","1","0","0"
"49137905","<pre><code>mean , variance = tf.nn.moments(X_train, axes = 1, keep_dims = True)
</code></pre>

<p>I am trying to get the mean and variance using <code>tf.nn.moments()</code> as shown above. However, I am encountering the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-43-fc383f99b15b&gt; in &lt;module&gt;()
     33 Y_train = Y_train.reshape(1,355)
     34 X_mean = tf.reduce_mean(X_train, axis = 1, keepdims = True)
---&gt; 35 mean , variance = tf.nn.moments(X_train, axes = 1, keep_dims = True)
     36 X_train = tf.divide(tf.subtract(X_train,mean),tf.sqrt(variance))
     37 #Y_train = Y_train/(Y_train.max(axis = 1, keepdims = True))

/Users/abhinandanchiney/anaconda2/lib/python2.7/site-      packages/tensorflow/python/ops/nn_impl.pyc in moments(x, axes, shift, name, keep_dims)
    664     # sufficient statistics. As a workaround we simply perform the operations
    665     # on 32-bit floats before converting the mean and variance back to fp16
--&gt; 666     y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x
    667     # Compute true mean while keeping the dims for proper broadcasting.
    668     mean = math_ops.reduce_mean(y, axes, keepdims=True, name=""mean"")

 TypeError: data type not understood
</code></pre>

<p>Kindly help where I am going wrong.</p>
","<python-2.7><tensorflow><neural-network><deeplearning4j>","2018-03-06 18:44:41","","2","9452932","2018-03-06 22:41:35","1","2","","","9452932","13","0","0"
"48602143","<p>I'm trying to do some simple time series prediction in Deeplearning4j, using an LSTM, but I'm having a hard time getting it working.
I have a simple textfile with a list of numbers like below and would like the network to learn to predict the next number.
Is there any example code for this? The Java examples I find all seem to be about image processing and classification.</p>

<pre><code>112
118
132
129
121
135
148
...
</code></pre>
","<lstm><deeplearning4j>","2018-02-03 21:16:36","","0","9309807","2018-02-04 05:21:42","1","0","","","9309807","1","0","0"
"39234402","<p>I'm trying to train a neural network using deeplearning4j. But I get this error message which I cannot explain:</p>

<pre><code>java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:294)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Unable to get linear index &gt;= 1
    at org.nd4j.linalg.api.ndarray.BaseNDArray.getDouble(BaseNDArray.java:3275)
    at org.deeplearning4j.eval.Evaluation.eval(Evaluation.java:197)
    at mypackage.myclass.main(Learn.java:77)
</code></pre>

<p>My data is in a csv file, it's 64 numbers (values 0,1,2,3) and a label of value -1000 to 1000 (floats).</p>

<p>for example:</p>

<pre><code> 2,3,2,2,1,1,2,3,0,1,1,2,3,1,1,0,0,0,2,2,0,0,3,1,0,1,3,1,1,1,2,2,2,2,2,2,3, 2,2,2,2,3,3,1,2,2,1,3,0,0,2,3,2,3,2,0,0,3,0,1,1,3,3,2,-228.0
</code></pre>

<p>I use this code to load the csv file and train the network:</p>

<pre><code>RecordReader recordReader = new CSVRecordReader(0, "","");
recordReader.initialize(new FileSplit(new File(""data.csv"")));

DataSetIterator iterator = new RecordReaderDataSetIterator(recordReader, new DoubleWritableCo    nverter(), 600000, 64, 64, true);

     DataSet allData = iterator.next();
     SplitTestAndTrain testAndTrain = allData.splitTestAndTrain(0.9);

     DataSet trainingData = testAndTrain.getTrain();
     DataSet testData = testAndTrain.getTest();

     //We need to normalize our data. We'll use NormalizeStandardize (which gives us mean 0, unit     variance):
     DataNormalization normalizer = new NormalizerStandardize();
     normalizer.fit(trainingData);           //Collect the statistics (mean/stdev) from the traini    ng data. This does not modify the input data
     normalizer.transform(trainingData);     //Apply normalization to the training data
     normalizer.transform(testData);         //Apply normalization to the test data. This is using     statistics calculated from the *training* set
     long seed = 123;
     int inputNum = 64;
     int hiddenNum = 64;
     int outputNum = 1;
     int iterations = 1;

     MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
         .seed(seed)
         .activation(""tanh"")
         .iterations(iterations)
         .weightInit(WeightInit.XAVIER)
         .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
         .learningRate(0.1)
         .regularization(true).l2(1e-4)
         .list()
         .layer(0, new DenseLayer.Builder().nIn(inputNum).nOut(hiddenNum).build())
         .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.MSE)
             .activation(""identity"")
             .nIn(hiddenNum).nOut(outputNum).build())
         .backprop(true).pretrain(false)
         .build();
     MultiLayerNetwork model = new MultiLayerNetwork(conf);
     model.init();
     model.setListeners(new ScoreIterationListener(100));

     model.fit(trainingData); 

     //evaluate the model on the test set
     Evaluation eval = new Evaluation(2);
     INDArray output = model.output(testData.getFeatureMatrix());
     eval.eval(testData.getLabels(), output); &lt;---- this is line 77, where the error occurs
     System.out.println(eval.stats());
     recordReader.close();
</code></pre>

<p>What does this error mean and how can I fix this?</p>
","<java><java-8><deeplearning4j>","2016-08-30 18:13:00","","0","988324","2016-08-30 23:30:17","1","3","","","988324","1213","141","29"
"37245079","<p>I am trying to understand LSTM on Deeplearning4j. I am examining source code for the example, but I can't understand this.</p>

<pre><code>        //Allocate space:
    //Note the order here:
    // dimension 0 = number of examples in minibatch
    // dimension 1 = size of each vector (i.e., number of characters)
    // dimension 2 = length of each time series/example
    INDArray input = Nd4j.zeros(currMinibatchSize,validCharacters.length,exampleLength);
    INDArray labels = Nd4j.zeros(currMinibatchSize,validCharacters.length,exampleLength);
</code></pre>

<p>Why do we store 3D array, and what does it mean? </p>
","<neural-network><deep-learning><lstm><deeplearning4j>","2016-05-16 00:19:40","","1","6338590","2016-05-16 14:34:11","1","4","","","6338590","11","0","0"
"37654540","<p>I am using digits for image classification task.</p>

<p>I wanted to score the test db and get the predicted prob in csv file. Can anyone tell me where should i make changes in the digits files for that?</p>
","<caffe><deeplearning4j><nvidia-digits>","2016-06-06 10:00:01","","1","6333660","2016-06-06 10:00:01","0","0","","","6333660","41","0","0"
"50374063","<p>Here is my code for CSV data extraction and transformation:</p>

<pre><code>Schema schema = new Schema.Builder()
            .addColumnsString(""RowNumber"")
            .addColumnInteger(""CustomerId"")
            .addColumnString(""Surname"")
            .addColumnInteger(""CreditScore"")
            .addColumnCategorical(""Geography"",Arrays.asList(""France"",""Spain"",""Germany""))
            .addColumnCategorical(""Gender"",Arrays.asList(""Male"",""Female""))
            .addColumnsInteger(""Age"",""Tenure"",""Balance"",""NumOfProducts"",""HasCrCard"",""IsActiveMember"",""EstimatedSalary"",""Exited"").build();
    TransformProcess transformProcess = new TransformProcess.Builder(schema)
                                            .removeColumns(""RowNumber"",""Surname"",""CustomerId"")
                                            .categoricalToInteger(""Gender"")
                                            .categoricalToOneHot(""Geography"").build();
    RecordReader reader = new CSVRecordReader(1,',');
    reader.initialize(new FileSplit(new ClassPathResource(""Churn_Modelling.csv"").getFile()));
    TransformProcessRecordReader transformProcessRecordReader = new TransformProcessRecordReader(reader,transformProcess);
    System.out.println(""args = "" + transformProcessRecordReader.next() + """");
</code></pre>

<p>I just tried printing the first record:</p>

<blockquote>
  <p>args = [619, 1, 0, 0, 1, 42, 2, 0, 1, 1, 1, 101348.88, 1]</p>
</blockquote>

<p>For example, the three values followed by 619 -> 1, 0, 0
I would like to keep 619 followed by 0, 0.</p>

<p>Basically I would like to keep the first category as base category and others are predicted from the base category to avoid any multi-collinear relationship (dummy variable trap)</p>

<p>How do I do that? Can anyone advice on this? </p>
","<deep-learning><deeplearning4j><dl4j>","2018-05-16 14:51:51","","0","1973779","2018-05-16 20:50:08","1","0","","","1973779","1367","27","14"
"47914953","<p>please I have a university project called (automatic generation of OCL</p>

<p>constraints) and my supervisor asked me to choose a tool from this list of tools </p>

<p>for natural language processing:</p>

<p>Apache OpenNLP, Deeplearning4j, ChatScript, DELPH-IN, DKPro Core,
general architecture text engineering GATE, Gensim, LinguaStream, 
Mallet (software project), Modular Audio Recognition Framework, MontyLingua,
Natural Language Toolkit, SpaCy, UIMA.</p>

<p>what would be the easiest to implement and which one would be most suitable for my future work?</p>

<p>else any propositions!</p>
","<nltk><opennlp><deeplearning4j><ocl><dkpro-core>","2017-12-20 21:59:54","","-3","9110584","2017-12-21 07:21:26","1","0","1","","9110584","8","0","0"
"47530596","<p>I work with deeplearning4j and created INDArray of quite big size. Into that array I write some values. If I try to see those values in debugger, initially I see zeros, and only for data at FloatBuffer I see entered values. See the screenshot. </p>

<p><a href=""https://i.stack.imgur.com/CIuRs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CIuRs.png"" alt=""INDArray with missing values""></a></p>

<p>If to debug the code of XorExample in deeplearning4j such behavior I didn't notice:
<a href=""https://i.stack.imgur.com/x9I2i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x9I2i.png"" alt=""INDArray with displayed values""></a></p>

<p>Is there any way to always show or always hide values that sit inside of INDArray without shoving zeros? Or it is some kind of bug inside of idea?</p>
","<intellij-idea><deeplearning4j>","2017-11-28 11:36:17","","0","677824","2017-11-30 09:06:10","1","0","","","677824","3462","1186","15"
"47174015","<p>I have customer purchase history and some independent variables associated with each product that customer bought. I am trying to predict the next best predict for customers.</p>

<p>T  Customer Input1 Input2 ...... Input10 PrdouctBought
 1    cust1
 2    cust1
 3    cust1
 4    cust1
 1    cust2
 2    cust2
 3    cust2
 4    cust2</p>

<p>I have restricted each customer purchase history to 4 so that we can fix batch_size=4 and we get away with the problem of model taking in account of last product of previous customer.
What I am not able to do is how to incorporate these input variables to make RNN predict the next best product for that customer.</p>
","<python-3.x><tensorflow><lstm><rnn><deeplearning4j>","2017-11-08 07:35:56","","2","8904873","2017-11-08 07:35:56","0","1","","","8904873","21","1","0"
"51841061","<p>Importing the Keras MobileNet V2 HDF5 file (obtained <a href=""https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases"" rel=""nofollow noreferrer"">here</a>) with DL4J in Java throws the following error:</p>

<p><code>java.lang.RuntimeException: org.deeplearning4j.nn.modelimport.keras.exceptions.InvalidKerasConfigurationException: Model configuration attribute missing from myfile.h5 archive.. For more information, see http://deeplearning4j.org/model-import-keras.</code></p>

<p>I figure, I only downloaded the weights and not the model's structure. </p>

<p>I wish I could just download the needed file (some json I suppose), however, as I have searched a lot for it, guess it isn't out there. Additionally, I have no experience in creating such definitions myself. Also, Python is new to me.</p>

<p>How would you suggest I proceed to get any MobileNet version running for DL4J? </p>

<p>Thanks!</p>
","<java><keras><deeplearning4j>","2018-08-14 12:08:11","","2","2227015","2018-08-14 12:08:11","0","4","","","2227015","477","32","0"
"41560773","<p>I going to do Anomaly detection on my own images by using the example on deeplearning4j platform. And I change the code like this:</p>

<pre><code>    int rngSeed=123;
    Random rnd = new Random(rngSeed);
    int width=28;
    int height=28;
    int batchSize = 128;
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(12345)
            .iterations(1)
            .weightInit(WeightInit.XAVIER) 
            .updater(Updater.ADAGRAD)
            .activation(Activation.RELU)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .learningRate(0.05)
            .regularization(true).l2(0.0001)
            .list()
            .layer(0, new DenseLayer.Builder().nIn(784).nOut(250)
                    .build())
            .layer(1, new DenseLayer.Builder().nIn(250).nOut(10)
                    .build())
            .layer(2, new DenseLayer.Builder().nIn(10).nOut(250)
                    .build())
            .layer(3, new OutputLayer.Builder().nIn(250).nOut(784)
                    .lossFunction(LossFunctions.LossFunction.MSE)
                    .build())
            .pretrain(false).backprop(true)
            .build();

    MultiLayerNetwork net = new MultiLayerNetwork(conf);
    net.setListeners(Collections.singletonList((IterationListener) new ScoreIterationListener(1)));
    File trainData = new File(""mnist_png/training"");
    FileSplit fsTrain = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, rnd);

    ImageRecordReader recorderReader = new ImageRecordReader(height, width);
    recorderReader.initialize(fsTrain);


    DataSetIterator dataIt = new RecordReaderDataSetIterator(recorderReader, batchSize);
    List&lt;INDArray&gt; featuresTrain = new ArrayList&lt;&gt;();
    while(dataIt.hasNext()){
        DataSet ds = dataIt.next();
        featuresTrain.add(ds.getFeatureMatrix());
    }


    System.out.println(""************ training **************"");
    int nEpochs = 30;
    for( int epoch=0; epoch&lt;nEpochs; epoch++ ){
        for(INDArray data : featuresTrain){
            net.fit(data,data);
        }
        System.out.println(""Epoch "" + epoch + "" complete"");
    }
</code></pre>

<p>And it threw an exception while training:</p>

<pre><code>Exception in thread ""main"" org.deeplearning4j.exception.DL4JInvalidInputException: Input that is not a matrix; expected matrix (rank 2), got rank 4 array with shape [128, 1, 28, 28]
    at org.deeplearning4j.nn.layers.BaseLayer.preOutput(BaseLayer.java:363)
    at org.deeplearning4j.nn.layers.BaseLayer.activate(BaseLayer.java:384)
    at org.deeplearning4j.nn.layers.BaseLayer.activate(BaseLayer.java:405)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:590)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:713)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:1821)
    at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:151)
    at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:54)
    at org.deeplearning4j.optimize.Solver.optimize(Solver.java:51)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1443)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1408)
    at org.deeplearning4j.examples.dataExamples.AnomalyTest.main(AnomalyTest.java:86)
</code></pre>

<p>It seem that my input dataset has 4 columns while it need just 2 columes, so the question is how to convert imagerecorderread or something else to make it running properly?</p>
","<deeplearning4j>","2017-01-10 03:43:30","","0","6714349","2017-01-10 04:28:18","1","0","1","","6714349","60","5","0"
"36523021","<p>here is what I'm doing on a single node, local spark cluster:</p>

<pre><code>git clone https://github.com/deeplearning4j/dl4j-spark-cdh5-examples.git
cd dl4j-spark-cdh5-examples.git
mvn package
export SPARK_WORKER_MEMORY=13g
spark-submit --class org.deeplearning4j.examples.cnn.MnistExample ./target/dl4j-spark-cdh5-examples-1.0-SNAPSHOT.jar
</code></pre>

<p>And here is what I'm getting:</p>

<pre><code>Caused by: java.lang.OutOfMemoryError: Java heap space
</code></pre>

<p>Here the full stack traces:</p>

<blockquote>
  <p>spark-submit --class org.deeplearning4j.examples.cnn.MnistExample
  ./target/dl4j-spark-cdh5-examples-1.0-SNAPSHOT.jar 21:21:13,414 INFO ~
  Load data....</p>
  
  <p>WARNING: COULD NOT LOAD NATIVE SYSTEM BLAS ND4J performance WILL be
  reduced Please install native BLAS library such as OpenBLAS or
  IntelMKL See <a href=""http://nd4j.org/getstarted.html#open"" rel=""nofollow"">http://nd4j.org/getstarted.html#open</a> for further details</p>
  
  <p>21:21:20,571 INFO ~ Build model.... 21:21:20,776 WARN ~ Objective
  function automatically set to minimize. Set stepFunction in neural net
  configuration to change default settings. 21:21:20,886 INFO ~ ---
  Starting network training --- [Stage 0:> (0 + 6) / 6]</p>
  
  <p>[Stage 0:> (0 + 6) / 6]Exception in thread ""dispatcher-event-loop-3""
  java.lang.OutOfMemoryError: Java heap space 21:24:12,358 ERROR ~
  Exception in task 0.0 in stage 0.0 (TID 0)
  java.lang.IllegalStateException: unread block data at
  java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2421)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1382)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at
  org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
  at
  org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
  at
  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
  at
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744) 21:24:12,358 ERROR ~
  Exception in task 5.0 in stage 0.0 (TID 5) java.lang.OutOfMemoryError:
  Java heap space at java.lang.reflect.Array.newInstance(Array.java:70)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)
  21:24:12,358 ERROR ~ Exception in task 3.0 in stage 0.0 (TID 3)
  java.lang.OutOfMemoryError: Java heap space at
  java.lang.reflect.Array.newInstance(Array.java:70) at
  java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)
  21:24:12,358 ERROR ~ Exception in task 1.0 in stage 0.0 (TID 1)
  java.lang.IllegalStateException: unread block data at
  java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2421)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1382)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) at
  org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
  at
  org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
  at
  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
  at
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744) 21:24:12,358 ERROR ~
  Exception in task 2.0 in stage 0.0 (TID 2) java.lang.OutOfMemoryError:
  Java heap space at java.lang.reflect.Array.newInstance(Array.java:70)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)
  21:24:12,375 ERROR ~ Uncaught exception in thread Thread[Executor task
  launch worker-5,5,main] java.lang.OutOfMemoryError: Java heap space at
  java.lang.reflect.Array.newInstance(Array.java:70) at
  java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)
  21:24:12,375 ERROR ~ Uncaught exception in thread Thread[Executor task
  launch worker-3,5,main] java.lang.OutOfMemoryError: Java heap space at
  java.lang.reflect.Array.newInstance(Array.java:70) at
  java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)
  21:24:12,375 ERROR ~ Uncaught exception in thread Thread[Executor task
  launch worker-2,5,main] java.lang.OutOfMemoryError: Java heap space at
  java.lang.reflect.Array.newInstance(Array.java:70) at
  java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)
  21:24:12,383 ERROR ~ Task 5 in stage 0.0 failed 1 times; aborting job
  Exception in thread ""main"" org.apache.spark.SparkException: Job
  aborted due to stage failure: Task 5 in stage 0.0 failed 1 times, most
  recent failure: Lost task 5.0 in stage 0.0 (TID 5, localhost):
  java.lang.OutOfMemoryError: Java heap space at
  java.lang.reflect.Array.newInstance(Array.java:70) at
  java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)</p>
  
  <p>Driver stacktrace: at
  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
  at
  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
  at
  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
  at
  scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at
  org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
  at
  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
  at
  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
  at scala.Option.foreach(Option.scala:236) at
  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
  at
  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
  at
  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
  at
  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) at
  org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832) at
  org.apache.spark.SparkContext.runJob(SparkContext.scala:1845) at
  org.apache.spark.SparkContext.runJob(SparkContext.scala:1858) at
  org.apache.spark.SparkContext.runJob(SparkContext.scala:1929) at
  org.apache.spark.rdd.RDD.count(RDD.scala:1157) at
  org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:440)
  at
  org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:46)
  at
  org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fitDataSet(SparkDl4jMultiLayer.java:239)
  at
  org.deeplearning4j.examples.cnn.MnistExample.main(MnistExample.java:132)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
  at
  org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
  at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
  at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at
  org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by:
  java.lang.OutOfMemoryError: Java heap space at
  java.lang.reflect.Array.newInstance(Array.java:70) at
  java.io.ObjectInputStream.readArray(ObjectInputStream.java:1670) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.doReadObject(BaseDataBuffer.java:880)
  at
  org.nd4j.linalg.api.buffer.BaseDataBuffer.readObject(BaseDataBuffer.java:868)
  at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606) at
  java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) at
  java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
  at
  java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
  at
  java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
  at
  java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
  at
  org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply$mcV$sp(ParallelCollectionRDD.scala:74)
  at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1204)
  21:24:12,769 ERROR ~ Exception in task 4.0 in stage 0.0 (TID 4)
  org.apache.spark.TaskKilledException at
  org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:204)
  at
  java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at
  java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744) 21:30:18,649 ERROR ~ Uncaught
  exception in thread Thread-3 org.apache.spark.SparkException: Error
  sending message [message = StopBlockManagerMaster] at
  org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
  at
  org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
  at
  org.apache.spark.storage.BlockManagerMaster.tell(BlockManagerMaster.scala:225)
  at
  org.apache.spark.storage.BlockManagerMaster.stop(BlockManagerMaster.scala:217)
  at org.apache.spark.SparkEnv.stop(SparkEnv.scala:97) at
  org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
  at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)
  at org.apache.spark.SparkContext.stop(SparkContext.scala:1755) at
  org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
  at
  org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
  at
  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
  at
  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
  at
  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
  at
  org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
  at
  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
  at
  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
  at
  org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)</p>
</blockquote>

<p>Any ideas?</p>
","<apache-spark><mnist><deeplearning4j>","2016-04-09 21:38:09","","3","3656912","2016-04-09 23:16:44","1","0","2","","3656912","1520","527","0"
"32290819","<p>I tried to execute the following example on DL4J (loading pre-trained vectors file):</p>

<pre><code>File gModel = new File(""./GoogleNews-vectors-negative300.bin.gz"");

Word2Vec vec = WordVectorSerializer.loadGoogleModel(gModel, true);

InputStreamReader r = new InputStreamReader(System.in);

BufferedReader br = new BufferedReader(r);

for (; ; ) {
    System.out.print(""Word: "");
    String word = br.readLine();

    if (""EXIT"".equals(word)) break;

    Collection&lt;String&gt; lst = vec.wordsNearest(word, 20);

    System.out.println(word + "" -&gt; "" + lst);
}
</code></pre>

<p>But it is super slow (taking ~10 minutes to calculate the nearest words, though they are correct).</p>

<p>There is enough memory (<code>-Xms20g -Xmx20g</code>).</p>

<p>When I run the same Word2Vec example from <a href=""https://code.google.com/p/word2vec/"" rel=""noreferrer"">https://code.google.com/p/word2vec/</a></p>

<p>it gives the nearest words very quickly. </p>

<p>DL4J uses ND4J which claims to be twice as fast as Numpy: <a href=""http://nd4j.org/benchmarking"" rel=""noreferrer"">http://nd4j.org/benchmarking</a></p>

<p>Is there anything wrong with my code?</p>

<p>UPDATE: It is based on <a href=""https://github.com/deeplearning4j/dl4j-0.4-examples.git"" rel=""noreferrer"">https://github.com/deeplearning4j/dl4j-0.4-examples.git</a> (I didn't touch any dependencies, just tried to read the Google pre-trained vectors file). Word2VecRawTextExample works just fine (but the data size is relatively small).</p>
","<machine-learning><deep-learning><word2vec><deeplearning4j><nd4j>","2015-08-29 21:27:51","","7","871953","2017-11-21 12:01:00","1","0","2","","871953","10813","192","8"
"38250118","<p>I want the DeepLearning4j Word2Vec with incorporate with Spark.I have around 80000 words data for which I want to get the vector representation. Later on, I want to find the synonyms using that vector representation. I am struggling where to write the Word2Vec code portion so that it can be provided to all the vCPU's?</p>
","<java><apache-spark><nlp><word2vec><deeplearning4j>","2016-07-07 15:57:16","","0","868865","2016-07-09 05:36:02","1","1","0","","868865","19","3","0"
"41657644","<p>My apologies since my question may sound stupid question. But I am quite new in deep learning and caffe.
How can we detect how many iterations are required to fine-tune a pre-trained on our own dataset? For example, I am running fcn32 for my own data with 5 classes. When can I stop the fine-tuning process by looking at the loss and accuracy of training phase?</p>

<p>Many thanks</p>
","<deep-learning><caffe><pycaffe><deeplearning4j>","2017-01-15 03:52:19","","0","6494707","2017-01-16 07:53:08","3","0","2","","6494707","715","63","1"
"41670022","<p>I am doing FCN32 semantic segmentation on my data. I ran the algorithm to fine-tune for my data (grayscale images with only one channel), till 80,000 iterations; however, the loss and accuracy are fluctuating and the output image completely black. Even, the loss is so high after 80,000 iterations. I thought the classifier cannot do training well on my data. So, I am going to train from scratch. 
On the other hand, my data has imbalanced class members. The background pixels are more than the other four classes. Some researchers are suggesting using weighted loss. Does anyone have any idea? Am I doing the right way? How can I add this weighted loss to train_val.prototxt? </p>

<p>I will be thankful if you know any resources/examples related to training with weighted loss, please share with me here.</p>

<p>Thanks again</p>
","<neural-network><deep-learning><caffe><pycaffe><deeplearning4j>","2017-01-16 05:31:44","","5","6494707","2017-01-16 06:44:58","1","0","","","6494707","715","63","1"
"50892639","<p>I am trying to determine the exact mathematics used to train feed forward networks used for classification in deeplearning4j with stochastic gradient descent.  I have tried stepping through the code but am getting lost in the forest.</p>

<p>Is this documented anywhere?</p>
","<math><deeplearning4j>","2018-06-16 23:58:58","","-2","3113481","2018-07-19 02:36:11","1","0","","","3113481","1","0","0"
"50374517","<p><strong>I'am trying to implement a custom loss function (DL4J)</strong>, following this code example: <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/misc/lossfunctions/CustomLossL1L2.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/misc/lossfunctions/CustomLossL1L2.java</a>.</p>

<p><strong>I need to extends ILossFunction</strong> and to override some methods.</p>

<p><strong>Problem :</strong>
The function is returning <code>org.nd4j.linalg.primitives.Pair&lt;Double, INDArray&gt;</code>.</p>

<p>I tried :</p>

<pre><code>  override def computeGradientAndScore(
    labels: INDArray,
    preOutput: INDArray,
    activationFn: IActivation,
    mask: INDArray,
    average: Boolean
  ): Pair[Double, INDArray] = {
     Pair.makePair(
        computeScore(labels, preOutput, activationFn, mask, average),
        computeGradient(labels, preOutput, activationFn, mask)
    )
  }
</code></pre>

<p>And get the following compilation error :</p>

<pre><code>[info] Compiling 2 Scala sources to PATH
[error] PATH/CosineSimilarity.scala:78: overriding method computeGradientAndScore in trait ILossFunction of type (x$1: org.nd4j.linalg.api.ndarray.INDArray, x$2: org.nd4j.linalg.api.ndarray.INDArray, x$3: org.nd4j.linalg.activations.IActivation, x$4: org.nd4j.linalg.api.ndarray.INDArray, x$5: Boolean)org.nd4j.linalg.primitives.Pair[Double,org.nd4j.linalg.api.ndarray.INDArray];
[error]  method computeGradientAndScore has incompatible type
[error]   override def computeGradientAndScore(labels: INDArray, preOutput: INDArray, activationFn: IActivation, mask: INDArray, average: Boolean): Pair[Double, INDArray] = {
[error]                ^
[error] one error found
[error] (root/compile:compileIncremental) Compilation failed
[error] Total time: 4 s, completed 16 mai 2018 16:45:48
</code></pre>

<p><strong>Question :</strong>
How can I override this method ?</p>
","<java><scala><deeplearning4j>","2018-05-16 15:14:18","","0","9006687","2018-05-17 07:55:20","1","0","","","9006687","57","11","0"
"41651701","<p>I am trying to get the output from FCN 32. I trained FCN32 with pascalcontext-fcn32-heavy.caffemodel pre-trained model. I could run for grayscale images with 5 classes. However, during inference, the output is all zero (a black image). This is inference code:</p>

<pre><code>import numpy as np
from PIL import Image
import sys
import scipy.io as sio
from caffe.proto import caffe_pb2
import caffe
    caffe.set_device(0) 
    caffe.set_mode_gpu()

    # load image, subtract mean, and make dims C x H x W for Caffe

   img_name='/home/ss/caffe-pascalcontext-fcn32s/dataset/Test/PNG/image-061-023.png'    #+
    im = Image.open(img_name)

    in_ = np.array(im, dtype=np.float32)
    in_ = np.expand_dims(in_, axis=0)               #+
    print in_.shape
    #Read mean image
    '''####################'''
    mean_blob = caffe_pb2.BlobProto()
    with open('/home/ss/caffe-pascalcontext-fcn32s/input/FCN32_mean.binaryproto') as f:
        mean_blob.ParseFromString(f.read())
    mean_array = np.asarray(mean_blob.data, dtype=np.float32).reshape(
        (mean_blob.channels, mean_blob.height, mean_blob.width))
    in_ -= mean_array

    net_root = '/home/ss/caffe-pascalcontext-fcn32s'

    MODEL_DEF = net_root + '/deploy.prototxt'
    PRETRAINED = net_root + '/snapshot/FCN32s_train_iter_40000.caffemodel'
    # load net
    #net = caffe.Net('deploy.prototxt', 'snapshot/train_iter_640000.caffemodel', caffe.TEST)
    net = caffe.Net(MODEL_DEF,PRETRAINED, caffe.TEST)
    #net = caffe.Net('deploy.prototxt', 'snapshot_bak1/train_iter_400000.caffemodel', caffe.TEST)

    # shape for input (data blob is N x C x H x W), set data
    # put img to net
    net.blobs['data'].reshape(1, *in_.shape)  # 1: batch size, *in_.shape 3 channel ?
    net.blobs['data'].data[...] = in_

    # run net and take argmax for prediction
    output = net.forward()

    # print
    def print_param(output):
        # the blobs
        print '--------------------------'
        print 'the blobs'
        for k, v in net.blobs.items():
            print k, v.data.shape

        # the parameters
        print '--------------------------'
        print 'the paramsters'
        for k, v in net.params.items():
            print k, v[0].data.shape

        # the conv layer weights
        print '--------------------------'
        print 'the conv layer weights'
        print net.params['conv1_1'][0].data

        # the data blob 
        print '--------------------------'
        print 'the data blob'
        print net.blobs['data'].data

        # the conv1_1 blob
        print '--------------------------'
        print 'the conv1_1 blob'
        print net.blobs['conv1_1'].data

        # the pool1 blob
        print '--------------------------'
        print 'the pool1 blob'
        print net.blobs['pool1'].data

        weights = net.blobs['fc6'].data[0]
        print 'blobs fc6'
        print np.unique(weights)
        weights = net.blobs['fc7'].data[0]
        print 'blobs fc7'
        print np.unique(weights)
        weights = net.blobs['score_fr_sign'].data[0]
        print 'blobs score_fr_sign'
        print np.unique(weights)
        weights = net.blobs['upscore_sign'].data[0]
        print 'blobs upscore_sign'
        print np.unique(weights)
        weights = net.blobs['score'].data[0]
            print weights.shape             #+
            sio.savemat('scores.mat',{'weights':weights})   #+
        print 'blobs score'
        print np.unique(weights)

    print_param(output)

    out = net.blobs['score'].data[0].argmax(axis=0)
    print out           #+

    #np.savetxt(""vote"", out, fmt=""%02d"")
    np.savetxt(""vote"", out, fmt=""%d"")

    print im.height
    print im.width
    print out.shape, len(out.shape)

    def array2img(out):
        out1 = np.array(out, np.unit8)
        img = Image.fromarray(out1,'L')
        for x in range(img.size[0]):
            for y in range(img.size[1]):
                if not img.getpixel((x, y)) == 0:
                    print 'PLz', str(img.getpixel((x, y)))

        img.show()


    def show_pred_img(file_name):
        file = open(file_name, 'r')
        lines = file.read().split('\n')

        #img_name = str(sys.argv[1])
        im = Image.open(img_name)
        im_pixel = im.load()

        img = Image.new('RGB', im.size, ""black"")
        pixels = img.load()

        w, h = 0, 0
        for l in lines:
            w = 0
            if len(l) &gt; 0:
                word = l.split(' ')
                for x in word:
                    if int(x) == 1:
                        pixels[w, h] = im_pixel[w, h]
                    w += 1
                h += 1
        print im.size
        #img.show()
        img.save(img_name+'_result.png')
    show_pred_img('vote')
</code></pre>

<p>This the log information of inference: </p>

<pre><code>the blobs
data (1, 1, 256, 256)
data_input_0_split_0 (1, 1, 256, 256)
data_input_0_split_1 (1, 1, 256, 256)
conv1_1 (1, 64, 454, 454)
conv1_2 (1, 64, 454, 454)
pool1 (1, 64, 227, 227)
conv2_1 (1, 128, 227, 227)
conv2_2 (1, 128, 227, 227)
pool2 (1, 128, 114, 114)
conv3_1 (1, 256, 114, 114)
conv3_2 (1, 256, 114, 114)
conv3_3 (1, 256, 114, 114)
pool3 (1, 256, 57, 57)
conv4_1 (1, 512, 57, 57)
conv4_2 (1, 512, 57, 57)
conv4_3 (1, 512, 57, 57)
pool4 (1, 512, 29, 29)
conv5_1 (1, 512, 29, 29)
conv5_2 (1, 512, 29, 29)
conv5_3 (1, 512, 29, 29)
pool5 (1, 512, 15, 15)
fc6 (1, 4096, 9, 9)
fc7 (1, 4096, 9, 9)
score_fr_sign (1, 5, 9, 9)
upscore_sign (1, 5, 320, 320)
score (1, 5, 256, 256)
--------------------------
the paramsters
conv1_1 (64, 1, 3, 3)
conv1_2 (64, 64, 3, 3)
conv2_1 (128, 64, 3, 3)
conv2_2 (128, 128, 3, 3)
conv3_1 (256, 128, 3, 3)
conv3_2 (256, 256, 3, 3)
conv3_3 (256, 256, 3, 3)
conv4_1 (512, 256, 3, 3)
conv4_2 (512, 512, 3, 3)
conv4_3 (512, 512, 3, 3)
conv5_1 (512, 512, 3, 3)
conv5_2 (512, 512, 3, 3)
conv5_3 (512, 512, 3, 3)
fc6 (4096, 512, 7, 7)
fc7 (4096, 4096, 1, 1)
score_fr_sign (5, 4096, 1, 1)
upscore_sign (5, 1, 64, 64)
--------------------------
the conv layer weights
[[[[ 0.  0.  0.]
   [ 0.  0.  0.]
   [ 0.  0.  0.]]]

...
 .
 .
 .       

 [[[ 0.  0.  0.]
   [ 0.  0.  0.]
   [ 0.  0.  0.]]]]
--------------------------
the data blob
[[[[ 29.32040787  20.31391525  20.30148506 ...,  10.41113186  11.42486095
      6.42949915]
   [ 33.32374954  21.31280136  22.30037117 ...,   9.40779209  10.42189217
      8.43079758]
   [ 36.32300568  25.30816269  25.29183578 ...,  10.40148449  11.41818142
     10.42838573]
   ..., 
   [ 34.64990616  31.65658569  30.65714264 ...,   4.           2.99981451
      0.99962896]
   [ 39.65788651  33.65769958  29.65974045 ...,   5.99981451   4.99944353
      0.99888682]
   [ 41.6641922   34.66493607  30.66567802 ...,   5.99962902   2.99907231
      3.99833035]]]]
--------------------------
the conv1_1 blob
[[[[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  ..., 
  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]]]
--------------------------
the pool1 blob
[[[[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  ..., 
  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]

  [[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]]]
blobs fc6
[ 0.]
blobs fc7
[ 0.]
blobs score_fr_sign
[-1.61920226 -1.34294271  0.07809996  0.60521388  2.2788291 ]
blobs upscore_sign
[-1.61920238 -1.61920226 -1.61920214 ...,  2.27882886  2.2788291
  2.27882934]
(5, 256, 256)
blobs score
[-1.61920238 -1.61920226 -1.61920214 -1.59390223 -1.59390211 -1.5689975
 -1.54330218 -1.54330206 -1.51918805 -1.49270213 -1.49270201 -1.4709599
 -1.46937859 -1.44210207 -1.44210196 -1.42273164 -1.41956913 -1.39150202
 -1.3915019  -1.37608469 -1.37450349 -1.36975968 -1.34294283 -1.34294271
 -1.3429426  -1.34090197 -1.34090185 -1.32943773 -1.32627523 -1.32195926
 -1.31995022 -1.30130363 -1.2903018  -1.28437209 -1.2827909  -1.27999234
 -1.27999222 -1.27804708 -1.27014089 -1.25999236 -1.23970175 -1.23930645
 -1.23802543 -1.23802531 -1.23614395 -1.22981894 -1.22033143 -1.21999264
 -1.21868122 -1.19605839 -1.19605827 -1.195822   -1.19424069 -1.18949699
 -1.1891017  -1.18910158 -1.18159068 -1.17999291 -1.17736995 -1.17052197
 -1.15409136 -1.15233755 -1.14917505 -1.14285004 -1.14130461 -1.13999307
 -1.13850164 -1.13850152 -1.13605869 -1.13336253 -1.12071252 -1.11212444
 -1.11043441 -1.1088531  -1.10410941 -1.10261631 -1.09999335 -1.09620309
 -1.09474754 -1.08790159 -1.08790147 -1.08513427 -1.07090306 -1.07015753
 -1.07015741 -1.06853116 -1.06536865 -1.06523943 -1.06392801 -1.05999362
 -1.05904365 -1.05343628 -1.04955614 -1.03730154 -1.03730142 -1.03690612
 -1.02820921 -1.02819049 -1.02786267 -1.02662802 -1.02523971 -1.0218842
 -1.02109361 -1.0199939  -1.013978   -1.01212502 -1.00290918 -0.99179727
 -0.99048585 -0.98867792 -0.98788732 -0.98670143 -0.98670137 -0.9865514
 -0.98622358 -0.98622352 -0.98472482 -0.97999406 -0.97839981 -0.97128415
 -0.97081381 -0.9689123  -0.95626229 -0.95573193 -0.95310903 -0.94914663
 -0.94786316 -0.94756538 -0.9442566  -0.94425654 -0.94282162 -0.94044977
 -0.93999434 -0.93491536 -0.92950261 -0.9238466  -0.92097807 -0.91966659
 -0.9157322  -0.91040593 -0.90961534 -0.90917486 -0.90724343 -0.90228963
 -0.90091842 -0.89999455 -0.89143091 -0.88819134 -0.88622415 -0.88360125
 -0.8787809  -0.87835538 -0.87324655 -0.8716653  -0.87048656 -0.86692154
 -0.86032271 -0.86032265 -0.85999483 -0.85901529 -0.85278171 -0.85147029
 -0.84794647 -0.84753585 -0.84688014 -0.8409785  -0.83608711 -0.8329246
 -0.83179826 -0.8265996  -0.81999505 -0.81933933 -0.81835574 -0.81835568
 -0.81711209 -0.81671637 -0.81147051 -0.80556893 -0.80360168 -0.80050892
 -0.79892766 -0.79418391 -0.79310995 -0.78720838 -0.78627765 -0.7858969
 -0.78196251 -0.77999532 -0.77540517 -0.76622486 -0.76493073 -0.76176822
 -0.75544322 -0.75507742 -0.75442165 -0.75245446 -0.7472086  -0.73933983
 -0.73093385 -0.72935259 -0.72884804 -0.72460884 -0.72425795 -0.72294647
 -0.71901208 -0.71245474 -0.70327443 -0.69693691 -0.6937744  -0.69343841
 -0.69081551 -0.68556964 -0.67770082 -0.66452122 -0.66393042 -0.66293997
 -0.66261894 -0.65868455 -0.65212721 -0.63442242 -0.63210559 -0.63179946
 -0.6265536  -0.60622585 -0.60491437 -0.60127115 -0.60097998 -0.57802927
 -0.57540637 -0.55114424 -0.54983276 -0.52425915 -0.49868551  0.02900147
  0.03048873  0.03197598  0.03205225  0.03346324  0.03361578  0.03495049
  0.0351793   0.03525557  0.03643775  0.03674283  0.03689536  0.037925
  0.03830635  0.03853516  0.03861143  0.03941226  0.03986987  0.04017495
  0.04032749  0.04089952  0.0414334   0.04181475  0.04204356  0.04211983
  0.04238677  0.04299692  0.04345454  0.04375962  0.04387403  0.04391216
  0.04456045  0.04509434  0.04536128  0.04547568  0.04570449  0.04578076
  0.04612397  0.04673413  0.04684854  0.04719175  0.04749683  0.04759216
  0.04764936  0.0476875   0.04837392  0.04890781  0.04925102  0.04928916
  0.04951797  0.04959423  0.05001372  0.05003278  0.05003279  0.05062388
  0.05108149  0.05138657  0.05153911  0.05165351  0.05233994  0.05247341
  0.05247341  0.05287382  0.05325517  0.05348398  0.05356025  0.054056
  0.05466616  0.05491403  0.05491403  0.05512378  0.05542885  0.05558139
  0.05645849  0.05699238  0.05735466  0.05735466  0.05737372  0.05760253
  0.0576788   0.05886098  0.05931859  0.05962367  0.05977621  0.05979528
  0.05979528  0.06126347  0.06164481  0.06187363  0.06194989  0.0622359
  0.06223591  0.06366596  0.06397104  0.06412357  0.06467653  0.06606845
  0.06629726  0.06637353  0.06711715  0.06847093  0.06862348  0.06955777
  0.06955778  0.07087342  0.0709497   0.0719984   0.0719984   0.07327592
  0.07443902  0.07443903  0.0756784   0.07687964  0.07687965  0.07809995
  0.07809996  0.07809997  0.22473885  0.23626392  0.24778898  0.24838002
  0.25931406  0.26049611  0.27083912  0.27261221  0.27320322  0.28236419
  0.28472832  0.28591037  0.29388925  0.29684439  0.29861748  0.29920852
  0.30541432  0.3089605   0.31132463  0.31250668  0.31693938  0.3210766
  0.32403174  0.32580483  0.32639587  0.32846448  0.33319271  0.33673888
  0.33910298  0.33998954  0.34028506  0.34530881  0.349446    0.35151461
  0.35240114  0.35417423  0.35476527  0.35742489  0.36215314  0.36303967
  0.36569929  0.36806342  0.36880219  0.36880222  0.36924547  0.36954099
  0.37486026  0.37899747  0.38165709  0.38195261  0.3837257   0.38431671
  0.38756737  0.38771513  0.38771516  0.39229563  0.39584181  0.39820591
  0.39938796  0.40027452  0.40559378  0.40662807  0.40973097  0.41268614
  0.4144592   0.41505024  0.41889194  0.42362016  0.42554098  0.42554101
  0.42716634  0.42953047  0.43071252  0.43750936  0.44164655  0.44445392
  0.44445395  0.44460171  0.44637477  0.44696581  0.45612678  0.45967296
  0.46203706  0.46321911  0.46336687  0.4633669   0.4747442   0.47769934
  0.47947243  0.48006344  0.48227981  0.48227984  0.49336162  0.49572572
  0.49690777  0.50119275  0.51197904  0.5137521   0.51434314  0.52010566
  0.52010572  0.53059644  0.53177851  0.53901857  0.53901863  0.54921389
  0.54980487  0.55793154  0.56783128  0.57684445  0.57684451  0.58644873
  0.59575737  0.59575742  0.60521382  0.60521388  0.60521394  0.84621561
  0.88961124  0.93300694  0.93523234  0.97640258  0.98085344  1.01979828
  1.02647448  1.02869999  1.06319392  1.07209563  1.07654643  1.10658967
  1.11771667  1.12439299  1.12661839  1.14998531  1.16333783  1.17223942
  1.17669034  1.19338095  1.20895886  1.22008598  1.22676229  1.22898769
  1.23677659  1.25458002  1.26793253  1.27683413  1.28017235  1.28128505
  1.30020106  1.31577897  1.32356799  1.32690609  1.3335824   1.3358078
  1.34582222  1.36362553  1.36696362  1.37697804  1.38587976  1.38866138
  1.3886615   1.39033055  1.39144325  1.41147208  1.42704999  1.43706429
  1.43817711  1.44485331  1.4470787   1.45931852  1.45987487  1.45987499
  1.47712183  1.49047434  1.49937606  1.50382698  1.50716507  1.52719378
  1.53108823  1.53108835  1.5427717   1.55389881  1.56057513  1.56280053
  1.57726574  1.59506905  1.6023016   1.60230172  1.60842156  1.61732328
  1.62177408  1.6473664   1.66294444  1.67351508  1.6735152   1.67407143
  1.68074775  1.68297315  1.71746719  1.7308197   1.7397213   1.74417222
  1.74472845  1.74472857  1.78756785  1.79869497  1.80537117  1.80759656
  1.81594181  1.81594193  1.81594205  1.85766852  1.86657023  1.87102103
  1.88715529  1.88715541  1.9277693   1.9344455   1.9366709   1.95836878
  1.99786997  2.00232077  2.02958202  2.02958226  2.06797075  2.07019615
  2.10079551  2.10079575  2.1380713   2.17200899  2.20817208  2.24322224
  2.24322248  2.27882886  2.2788291   2.27882934]
256
256
(256, 256) 2
(256, 256)
</code></pre>

<p>I have two major questions:</p>

<ol>
<li>I am wondering why the output is black? and</li>
<li>How can I know when to stop running the algorithm (i.e., iteration
number)? I really do not know what is the optimum iteration number and
loss value that I can stop fine tuning in that stage. I stopped
training in <code>40,000 iterations</code>, I have no idea about this.</li>
<li>Is it necessary that the result of segmentation be a grayscale image
as well (like input), or creating RGB result image does not make any
difference in the output?</li>
</ol>

<p>I really do not know how much I am doing the right way. Quite CONFUSED :(
Does anyone have any suggestion? I really appreciate your help.</p>
","<python-2.7><deep-learning><caffe><pycaffe><deeplearning4j>","2017-01-14 15:34:50","","0","6494707","2017-01-21 09:27:15","2","1","","","6494707","715","63","1"
"52691773","<p>I'm trying to use Nd4j in a Kotlin project in Intellij IDEA.  In Project Structure -> Libraries, I used the ""From Maven"" command to add the following libraries.</p>

<pre><code>org.deeplearning4j:deeplearning4j-core:1.0.0-beta
org.nd4j:nd4j-native-platform:1.0.0-beta
org.datavec:datavec-api:1.0.0-beta
</code></pre>

<p>With those libraries I can compile my project, but when I run it fails with an exception.</p>

<pre><code>Caused by: java.lang.RuntimeException: ND4J is probably missing dependencies. For more information, please refer to: http://nd4j.org/getstarted.html
    at org.nd4j.nativeblas.NativeOpsHolder.&lt;init&gt;(NativeOpsHolder.java:51)
    at org.nd4j.nativeblas.NativeOpsHolder.&lt;clinit&gt;(NativeOpsHolder.java:19)
    ... 10 more
Caused by: java.lang.UnsatisfiedLinkError: no jnind4jcpu in java.library.path
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
    at java.lang.Runtime.loadLibrary0(Runtime.java:870)
    at java.lang.System.loadLibrary(System.java:1122)
    at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:1220)
    at org.bytedeco.javacpp.Loader.load(Loader.java:980)
    at org.bytedeco.javacpp.Loader.load(Loader.java:879)
    at org.nd4j.nativeblas.Nd4jCpu.&lt;clinit&gt;(Nd4jCpu.java:10)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.bytedeco.javacpp.Loader.load(Loader.java:938)
    at org.bytedeco.javacpp.Loader.load(Loader.java:879)
    at org.nd4j.nativeblas.Nd4jCpu$NativeOps.&lt;clinit&gt;(Nd4jCpu.java:1310)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at org.nd4j.nativeblas.NativeOpsHolder.&lt;init&gt;(NativeOpsHolder.java:29)
    ... 11 more
Caused by: java.lang.UnsatisfiedLinkError: no nd4jcpu in java.library.path
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
    at java.lang.Runtime.loadLibrary0(Runtime.java:870)
    at java.lang.System.loadLibrary(System.java:1122)
    at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:1220)
    at org.bytedeco.javacpp.Loader.load(Loader.java:965)
    ... 21 more
</code></pre>

<p>Looking through the project folder I see that IDEA has downloaded lots of jar files for nd4j-native-platform, but there's no sign of any JNI libraries.  What else do I have to do?</p>
","<maven><intellij-idea><kotlin><deeplearning4j><nd4j>","2018-10-07 18:51:07","","0","717738","2018-10-19 07:44:22","2","5","","","717738","835","7","2"
"50895514","<p><br>
Is ND4J faster than the normal Java arrays? I've done a basic test by initializing a 100x100 ND4J array then printing it, then did the same for a normal array. The execution time of the normal array is 10x faster. Any idea?</p>

<p>ND4J code:</p>

<pre><code>INDArray a = Nd4j.ones(100,100);
System.out.println(a);
</code></pre>

<p>Execution time is around 1700 ms. For standard arrays it is around 150 ms.</p>
","<java><arrays><deeplearning4j><nd4j>","2018-06-17 09:54:29","","0","9144781","2018-06-18 06:34:13","0","4","","","9144781","31","2","0"
"51390858","<p>I saw how to do this from a DataSet object, and I saw a setLabel method, and I saw a getLabelMaskArrays, but none of these are what I'm looking for.</p>

<p>Am I just blind or is there not a way?</p>

<p>Thanks</p>
","<deeplearning4j><dl4j>","2018-07-17 22:08:54","","0","6080613","2018-07-18 00:48:23","1","0","","","6080613","13","0","0"
"41280804","<p>I try to model a CNN  with deeplearing4j using SVHN dataset (<a href=""http://ufldl.stanford.edu/housenumbers/"" rel=""nofollow noreferrer"">http://ufldl.stanford.edu/housenumbers/</a>), in particular I'm using </p>

<blockquote>
  <p>Format 2: Cropped Digits</p>
</blockquote>

<p>This is matlab's files and each one contains a struct with a tensor (4-D) and an array with label. I would open this one into my deeplearing4j code, so I wondered and I find this class <code>MatlabRecordReader.java</code> into <code>deeplearning4j/DataVec</code> (<a href=""https://github.com/deeplearning4j/DataVec/blob/master/datavec-api/src/main/java/org/datavec/api/records/reader/impl/misc/MatlabRecordReader.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/DataVec/blob/master/datavec-api/src/main/java/org/datavec/api/records/reader/impl/misc/MatlabRecordReader.java</a>) but I can't understand how use it. Anybody has experience whit this?
Thanks in advance</p>
","<deeplearning4j>","2016-12-22 10:20:43","","1","859268","2018-02-06 22:26:07","1","1","","","859268","964","68","24"
"52752681","<p>I'm trying to use ND4j Adam optimizer with a objective that was implemented based on ND4J SameDiff auto differentiation. I don't want to use the <code>NeuralNetConfiguration.Builder()</code> or <code>ComputationGraph</code> from DeepLearning4j, I only want to use the updater/optimizers with some gradients that is computed with some custom code. </p>

<p>I found that we can use something similar to below, </p>

<pre><code>// list of parameters to be optimized 
SDVariable[] params; // I have this already initialized. 

// list of gradients computed for each parameter in params
INDArray gradients; // I have computed already computed this

// now I want to use above variables to perform a single gradient descent step based on Adam optimizer
// based on the source code, we can use something similar to below
Adam adam = Adam.builder().build();
GradientUpdater updater = adam.instantiate(INDArray viewArray, boolean initializeViewArray);
updater.applyUpdater(INDArray gradient, int iteration, int epoch);
</code></pre>

<p>But I'm not sure what are the values that we should set to each parameter. Especially, what are the <code>viewArray</code> and <code>initializeViewArray</code> parameters? </p>

<p>I appreciate if someone can help me to understand the best way of doing it.
Thanks!!!</p>
","<java><deep-learning><deeplearning4j><nd4j>","2018-10-11 05:16:34","","0","4869596","2018-10-11 06:00:13","0","0","","","4869596","6","0","0"
"48995852","<p>I couldn't find any full example of an autoencoder in DL4J documentation. I see a good general description of Autoencoders <a href=""https://deeplearning4j.org/stackeddenoisingautoencoder"" rel=""nofollow noreferrer"">here</a> with a small piece of code for just the MultiLayerConfiguration, but the code is not full. Is there any full example where a dataset is loaded, pre-processed and then inserted into the network and a prediction is generated? For example, an example working with the <a href=""https://grouplens.org/datasets/movielens/"" rel=""nofollow noreferrer"">Movielens</a> dataset, or any other. Thank you.</p>
","<autoencoder><deeplearning4j><dl4j>","2018-02-26 19:37:03","","0","9144781","2018-02-27 07:48:54","1","0","","","9144781","31","2","0"
"47803989","<p>I created a UIMA stack using OpenNLP that runs locally across all cores. It does a variety of tasks including reading from a CSV file, inserting text to a database, parsing the text, POS tagging text, chunking text, etc. I also got it to run a variety of tasks across a spark cluster. </p>

<p>We want to add some machine learning algorithms to the stack and DeepLearning4j came up as a very viable option. Unfortunately, it was not clear how to integrate DL4J within what we currently have or if it simply replicates the stack I have now. </p>

<p>What I have not found in the UIMA, ClearTK, and Deeplearning4j sites is how these three libraries fit together. Does DeepLearning4J implement a ClearTK set of abstract classes that calls OpenNLP functions? What benefit does ClearTK provide? Do I worry about how DeepLearning4J implements anything with the ClearTK framework?</p>

<p>Thanks!</p>
","<opennlp><uima><deeplearning4j>","2017-12-14 00:07:35","","-1","9096398","2017-12-14 12:13:15","2","0","","","9096398","31","0","0"
"47218398","<p>I know deeplearning4j can import models from Keras ( <a href=""https://deeplearning4j.org/model-import-keras"" rel=""nofollow noreferrer"">https://deeplearning4j.org/model-import-keras</a> ), but I'm interested in the opposite way. Therefore,</p>

<ul>
<li>is there any way to export deeplearning4j models to Keras?</li>
</ul>

<p>This could be either directly or by somehow converting a model stored using the ModelSerializer <a href=""https://deeplearning4j.org/modelpersistence"" rel=""nofollow noreferrer"">https://deeplearning4j.org/modelpersistence</a> )? In particular, I would be interested in using trained models in keras.js ( <a href=""https://github.com/transcranial/keras-js"" rel=""nofollow noreferrer"">https://github.com/transcranial/keras-js</a> ).</p>
","<keras><deeplearning4j>","2017-11-10 08:21:11","","3","3469592","2017-11-10 12:36:58","1","0","1","","3469592","1340","44","1"
"50855345","<p>Now, I am trying to learn GloVe using Deeplearning4j. The learning process itself is good progressing.</p>

<p>So, I would like to monitor this learning process with <strong>UIServer</strong> while referring to <a href=""https://deeplearning4j.org/visualization"" rel=""nofollow noreferrer"">this page</a>. On this page, UIServer is enabled with the code shown below.</p>

<pre><code>// Define model
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    // omitted
    .build();
MultiLayerNetwork net = new MultiLayerNetwork(conf);

// Enable UIServer
UIServer uiServer = UIServer.getInstance();
StatsStorage statsStorage = new InMemoryStatsStorage();
uiServer.attach(statsStorage);
net.setListeners(new StatsListener(statsStorage));
</code></pre>

<p>However, since the <strong>setListeners</strong> method does not exist in the WordVectors interface including GloVe etc., it can not be monitored by a general method. Is there a good way to monitor with the GUI?</p>

<p>The current code is shown below.</p>

<pre><code>// Define model
Glove glove = new Glove.Builder()
    // omitted
    .build();

/**** I wanted to enable UIServer on this line. ****/

// Start training
glove.fit();
</code></pre>
","<java><deeplearning4j>","2018-06-14 10:30:16","","0","9117111","2018-07-19 02:40:24","1","0","","","9117111","8","0","0"
"51487244","<p>Under normal circumstances, I can save a ComputationGraph (a Convolutional Neural Network) to a file and load it in a later run and it works fine.</p>

<p>However, when I include it in a jar and I try to load it, it fails.</p>

<p>Is there some way to load ComputationGraph objects from inside of a Jar as a resource?</p>

<p>I got it working for the Word2Vec objects, but it fails on ComputationGraph objects.</p>

<p>Thank you</p>
","<java><deeplearning4j><dl4j><computation-graph>","2018-07-23 21:06:59","","0","6080613","2018-07-23 23:32:51","1","0","","","6080613","13","0","0"
"39424850","<p>I'm implementing an auto-encoder for anomaly detection of IoT sensor data. My data set comes from a simulation, but basically it is accelerometer data - three dimensions, one for each axis.</p>

<p>I'm reading it from a CSV file, column 2-4 contain the data - sorry for the code quality, it is quick and dirty:</p>

<pre><code>private static DataSetIterator getTrainingData(int batchSize, Random rand) {
    double[] ix = new double[nSamples];
    double[] iy = new double[nSamples];
    double[] iz = new double[nSamples];
    double[] ox = new double[nSamples];
    double[] oy = new double[nSamples];
    double[] oz = new double[nSamples];
    Reader in;
    try {
        in = new FileReader(""/Users/romeokienzler/Downloads/lorenz_healthy.csv"");

        Iterable&lt;CSVRecord&gt; records;

        records = CSVFormat.DEFAULT.parse(in);
        int index = 0;
        for (CSVRecord record : records) {
            String[] recordArray = record.get(0).split("";"");
            ix[index] = Double.parseDouble(recordArray[1]);
            iy[index] = Double.parseDouble(recordArray[2]);
            iz[index] = Double.parseDouble(recordArray[3]);
            ox[index] = Double.parseDouble(recordArray[1]);
            oy[index] = Double.parseDouble(recordArray[2]);
            oz[index] = Double.parseDouble(recordArray[3]);
            index++;
        }
        INDArray ixNd = Nd4j.create(ix);
        INDArray iyNd = Nd4j.create(iy);
        INDArray izNd = Nd4j.create(iz);
        INDArray oxNd = Nd4j.create(ox);
        INDArray oyNd = Nd4j.create(oy);
        INDArray ozNd = Nd4j.create(oz);
        INDArray iNd = Nd4j.hstack(ixNd, iyNd, izNd);
        INDArray oNd = Nd4j.hstack(oxNd, oyNd, ozNd);
        DataSet dataSet = new DataSet(iNd, oNd);
        List&lt;DataSet&gt; listDs = dataSet.asList();
        Collections.shuffle(listDs, rng);
        return new ListDataSetIterator(listDs, batchSize);
    } catch (IOException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
        System.exit(-1);
        return null;
    }
}
</code></pre>

<p>This is the net:</p>

<pre><code>    public static void main(String[] args) {
        // Generate the training data
        DataSetIterator iterator = getTrainingData(batchSize, rng);

        // Create the network
        int numInput = 3;
        int numOutputs = 3;
        int nHidden = 1;
        int listenerFreq = batchSize / 5;

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().seed(seed)
                .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)
                .gradientNormalizationThreshold(1.0).iterations(iterations).momentum(0.5)
                .momentumAfter(Collections.singletonMap(3, 0.9))
                .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).list(2)
                .layer(0,
                        new AutoEncoder.Builder().nIn(numInput).nOut(nHidden).weightInit(WeightInit.XAVIER)
                                .lossFunction(LossFunction.RMSE_XENT).corruptionLevel(0.3).build())
                .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD).activation(""softmax"").nIn(nHidden)
                        .nOut(numOutputs).build())
                .pretrain(true).backprop(false).build();

        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();
        model.setListeners(Collections.singletonList((IterationListener) new ScoreIterationListener(listenerFreq)));

        for (int i = 0; i &lt; nEpochs; i++) {
            iterator.reset();
            model.fit(iterator);
        }

    }
</code></pre>

<p>I'm getting the following error:
Shapes do not match: x.shape=[1, 9000], y.shape=[1, 3]</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: Shapes do not match: x.shape=[1, 9000], y.shape=[1, 3]
    at org.nd4j.linalg.api.parallel.tasks.cpu.CPUTaskFactory.getTransformAction(CPUTaskFactory.java:92)
    at org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner.doTransformOp(DefaultOpExecutioner.java:409)
    at org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner.exec(DefaultOpExecutioner.java:62)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:2660)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:2641)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.sub(BaseNDArray.java:2419)
    at org.deeplearning4j.nn.layers.feedforward.autoencoder.AutoEncoder.computeGradientAndScore(AutoEncoder.java:123)
    at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:132)
    at org.deeplearning4j.optimize.solvers.BaseOptimizer.optimize(BaseOptimizer.java:151)
    at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52)
    at org.deeplearning4j.nn.layers.BaseLayer.fit(BaseLayer.java:486)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.pretrain(MultiLayerNetwork.java:170)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1134)
    at org.deeplearning4j
</code></pre>

<p>.examples.feedforward.autoencoder.AnomalyDetector.main(AnomalyDetector.java:136)</p>

<p>But I'm not defining dimension anywhere and IMHO the dimensions of input and output should be (3,3000) and (3,3000). Where is my mistake?</p>

<p>Thanks a lot in advance...</p>

<p>EDIT: UPDATE to latest release 13.9.16 
I'm getting the same error (semantically), here is what I'm doing now:</p>

<pre><code>private static DataSetIterator getTrainingData(int batchSize, Random rand) {
    double[] ix = new double[nSamples];
    double[] iy = new double[nSamples];
    double[] iz = new double[nSamples];
    double[] ox = new double[nSamples];
    double[] oy = new double[nSamples];
    double[] oz = new double[nSamples];
    try {
        RandomAccessFile in = new RandomAccessFile(new File(""/Users/romeokienzler/Downloads/lorenz_healthy.csv""),
                ""r"");
        int index = 0;
        String record;
        while ((record = in.readLine()) != null) {
            String[] recordArray = record.split("";"");
            ix[index] = Double.parseDouble(recordArray[1]);
            iy[index] = Double.parseDouble(recordArray[2]);
            iz[index] = Double.parseDouble(recordArray[3]);
            ox[index] = Double.parseDouble(recordArray[1]);
            oy[index] = Double.parseDouble(recordArray[2]);
            oz[index] = Double.parseDouble(recordArray[3]);
            index++;
        }
        INDArray ixNd = Nd4j.create(ix);
        INDArray iyNd = Nd4j.create(iy);
        INDArray izNd = Nd4j.create(iz);
        INDArray oxNd = Nd4j.create(ox);
        INDArray oyNd = Nd4j.create(oy);
        INDArray ozNd = Nd4j.create(oz);
        INDArray iNd = Nd4j.hstack(ixNd, iyNd, izNd);
        INDArray oNd = Nd4j.hstack(oxNd, oyNd, ozNd);
        DataSet dataSet = new DataSet(iNd, oNd);
        List&lt;DataSet&gt; listDs = dataSet.asList();
        Collections.shuffle(listDs, rng);
        return new ListDataSetIterator(listDs, batchSize);
    } catch (IOException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
        System.exit(-1);
        return null;
    }
}
</code></pre>

<p>And here the net:</p>

<pre><code>// Set up network. 784 in/out (as MNIST images are 28x28).
    // 784 -&gt; 250 -&gt; 10 -&gt; 250 -&gt; 784
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().seed(12345).iterations(1)
            .weightInit(WeightInit.XAVIER).updater(Updater.ADAGRAD).activation(""relu"")
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).learningRate(learningRate)
            .regularization(true).l2(0.0001).list().layer(0, new DenseLayer.Builder().nIn(3).nOut(1).build())
            .layer(1, new OutputLayer.Builder().nIn(1).nOut(3).lossFunction(LossFunctions.LossFunction.MSE).build())
            .pretrain(false).backprop(true).build();

    MultiLayerNetwork net = new MultiLayerNetwork(conf);
    net.setListeners(Collections.singletonList((IterationListener) new ScoreIterationListener(1)));

    // Load data and split into training and testing sets. 40000 train,
    // 10000 test
    DataSetIterator iter = getTrainingData(batchSize, rng);

    // Train model:
    int nEpochs = 30;
    while (iter.hasNext()) {
        DataSet ds = iter.next();
        for (int epoch = 0; epoch &lt; nEpochs; epoch++) {
            net.fit(ds.getFeatures(), ds.getLabels());
            System.out.println(""Epoch "" + epoch + "" complete"");
        }
    }
</code></pre>

<p>My error is:</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalStateException: Mis matched lengths: [9000] != [3]
    at org.nd4j.linalg.util.LinAlgExceptions.assertSameLength(LinAlgExceptions.java:39)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:2786)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:2767)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.sub(BaseNDArray.java:2547)
    at org.deeplearning4j.nn.layers.BaseOutputLayer.getGradientsAndDelta(BaseOutputLayer.java:182)
    at org.deeplearning4j.nn.layers.BaseOutputLayer.backpropGradient(BaseOutputLayer.java:161)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.calcBackpropGradients(MultiLayerNetwork.java:1125)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.backprop(MultiLayerNetwork.java:1077)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:1817)
    at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:152)
    at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:54)
    at org.deeplearning4j.optimize.Solver.optimize(Solver.java:51)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1445)
    at org.deeplearning4j.examples.feedforward.anomalydetection.IoTAnomalyExample.main(IoTAnomalyExample.java:110)
</code></pre>

<p>I'm pretty sure I'm messing up with the training data - the shape of the training data is 3000 rows, 3 columns - same for the target (the very same data because I want to build an autoencoder) - test data can be found here:
<a href=""https://pmqsimulator-romeokienzler-2310.mybluemix.net/data"" rel=""nofollow"">https://pmqsimulator-romeokienzler-2310.mybluemix.net/data</a></p>

<p>Any ideas?</p>
","<deep-learning><autoencoder><deeplearning4j>","2016-09-10 09:52:26","","0","3656912","2016-09-13 01:27:02","1","1","1","","3656912","1520","527","0"
"53573501","<p>I cannot run a simple program that I wrote to start understanding Deeplearning4j.</p>

<p>I tried the code from this link:
<a href=""https://www.baeldung.com/deeplearning4j"" rel=""nofollow noreferrer"">Deep Learning In Java Using Deeplearning4J</a></p>

<p>unfortunately it didn't work for me. In fact I have this error:</p>

<blockquote>
  <p>SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
  SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J:
  See <a href=""http://www.slf4j.org/codes.html#StaticLoggerBinder"" rel=""nofollow noreferrer"">http://www.slf4j.org/codes.html#StaticLoggerBinder</a> for further
  details. Exception in thread ""main""
  java.lang.ExceptionInInitializerError     at
  org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertWritables(RecordReaderMultiDataSetIterator.java:377)
    at
  org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertFeaturesOrLabels(RecordReaderMultiDataSetIterator.java:271)
    at
  org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.nextMultiDataSet(RecordReaderMultiDataSetIterator.java:234)
    at
  org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next(RecordReaderMultiDataSetIterator.java:177)
    at
  org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:306)
    at
  org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:393)
    at
  org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:51)
    at com.alessio.text.App.main(App.java:38) Caused by:
  java.lang.RuntimeException:
  org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException:
  Please ensure that you have an nd4j backend on your classpath. Please
  see: <a href=""http://nd4j.org/getstarted.html"" rel=""nofollow noreferrer"">http://nd4j.org/getstarted.html</a>  at
  org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:6089)  at
  org.nd4j.linalg.factory.Nd4j.(Nd4j.java:201)  ... 8 more<code>enter
  code here</code> Caused by:
  org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException:
  Please ensure that you have an nd4j backend on your classpath. Please
  see: <a href=""http://nd4j.org/getstarted.html"" rel=""nofollow noreferrer"">http://nd4j.org/getstarted.html</a>  at
  org.nd4j.linalg.factory.Nd4jBackend.load(Nd4jBackend.java:258)    at
  org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:6086)  ... 9 more</p>
</blockquote>

<p>I'll appreciate any advice. Thanks in advance</p>
","<java><neural-network><deeplearning4j>","2018-12-01 17:52:05","","0","10732477","2018-12-02 00:28:57","1","2","","","10732477","34","19","0"
"41143092","<p>I'm trying to train a Restricted Boltzmann Machine (RBM) with DeepLearning4J 0.7 but without success. All the examples I found are either not doing anything useful or not working anymore with DeepLearning4J 0.7. </p>

<p>I need to train a single RBM with Contrastive Divergence and then compute the reconstruction error. </p>

<p>Here is what I have so far: </p>

<pre><code>import org.nd4j.linalg.factory.Nd4j;
import org.deeplearning4j.datasets.fetchers.MnistDataFetcher;
import org.deeplearning4j.nn.conf.layers.RBM;
import org.deeplearning4j.nn.api.Layer;
import static org.deeplearning4j.nn.conf.layers.RBM.VisibleUnit;
import static org.deeplearning4j.nn.conf.layers.RBM.HiddenUnit;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.nd4j.linalg.api.ndarray.INDArray;

public class experiment3 {
    private static final Logger log = LoggerFactory.getLogger(experiment3.class);

    public static void main(String[] args) throws Exception {
        DataSetIterator mnistTrain = new MnistDataSetIterator(100, 60000, true);

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .regularization(false)
            .iterations(1)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .list()
            .layer(0, new RBM.Builder()
                    .nIn(784).nOut(500)
                    .weightInit(WeightInit.XAVIER)
                    .lossFunction(LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY)
                    .updater(Updater.NESTEROVS)
                    .learningRate(0.1)
                    .momentum(0.9)
                    .k(1)
                    .build())
            .pretrain(true).backprop(false)
            .build();

        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();
        model.setListeners(new ScoreIterationListener(600));

        for(int i = 0; i &lt; 50; i++) {
            model.fit(mnistTrain);
        }
    }
}
</code></pre>

<p>It compiles and print some score at each epoch, but the score augments when it should be diminishing and I have not found any way to do reconstruction. </p>

<p>I have tried to use the reconstruct function and compute the distance: </p>

<pre><code>        while(mnistTrain.hasNext()){
            DataSet next = mnistTrain.next();
            INDArray in = next.getFeatureMatrix();
            INDArray out = model.reconstruct(in, 1); // tried with 0 but arrayindexoutofbounds

            log.info(""distance(1):"" + in.distance1(out));
        }
</code></pre>

<p>but the distance is always 0.0 for each element even when the model has not been trained for a single epoch, which is impossible.</p>

<p>Is this the correct way of training a RBM ? How can I reconstruct input with a single RBM ? </p>
","<java><machine-learning><deep-learning><deeplearning4j><rbm>","2016-12-14 12:49:09","","3","802362","2018-03-22 18:24:53","3","3","0","","802362","3981","1876","44"
"42612467","<p>How to check if we're having a vanishing/exploding gradient in deeplearning4j, more specifically for recurrent neural networks? I mean, what parameters to look for and what methods should we call to get the value of such parameters?</p>
","<deep-learning><deeplearning4j>","2017-03-05 18:45:56","","2","1478061","2019-01-05 16:26:39","1","1","","","1478061","321","167","12"
"48657741","<p>I want to read a simple CSV file with just a list of numbers using Datavec, for use within Deeplearning4j.
I've tried numerous examples but keep getting errors.
e.g. when I execute this:</p>

<pre><code>    RecordReader rrTest = new CSVRecordReader();
    rrTest.initialize(new FileSplit(new File(INPUT_FILE)));
    DataSetIterator testIter = new RecordReaderDataSetIterator(rrTest, 150, 0, 1);
</code></pre>

<p>I get this error:</p>

<pre><code>Exception in thread ""main"" org.nd4j.linalg.exception.ND4JIllegalStateException: Invalid shape: Requested INDArray shape [144, 0] contains dimension size values &lt; 1 (all dimensions must be 1 or more).
</code></pre>

<p>Changing the 'labelIndex' from 0 to 1 gives the same error.</p>

<p>The data in the file looks like this:</p>

<pre><code>112
118
132
129
121
135
148
148
136
119
104
118
115
</code></pre>

<p>How do I read this file? I guess the result should be a DataSet, as input for a dl4j.</p>
","<csv><deeplearning4j><recordreader>","2018-02-07 06:59:48","","0","9309807","2018-02-07 09:42:13","1","0","","","9309807","1","0","0"
"40465045","<p>I need to implement an existing Caffe model with DeepLearning4j. However i am new to DL4J so dont know how to implement. Searching through docs and examples had little help, the terminolgy of those two are very different.
How would you write the below caffe prototxt in dl4j ? </p>

<p>Layer1:</p>

<pre><code>layers {
  name: ""myLayer1""
  type: CONVOLUTION
  bottom: ""data""
  top: ""myLayer1""
  blobs_lr: 1
  blobs_lr: 2
  convolution_param {
    num_output: 20
    kernel_w: 2
    kernel_h: 2
    stride_w: 1
    stride_h: 1
    weight_filler {
    type: ""msra""
    variance_norm: AVERAGE
    }
    bias_filler {
       type: ""constant""
    }
 }
}
</code></pre>

<p><strong>Layer 2</strong></p>

<pre><code> layers {
   name: ""myLayer1Relu""
   type: RELU
   relu_param {
   negative_slope: 0.3
 }
 bottom: ""myLayer1""
 top: ""myLayer1""
 }
</code></pre>

<p><strong>Layer 3</strong></p>

<pre><code>  layers {
   name: ""myLayer1_dropout""
   type: DROPOUT
   bottom: ""myLayer1""
   top: ""myLayer1""
   dropout_param {
     dropout_ratio: 0.2
   }
 }
</code></pre>

<p><strong>Layer 4</strong></p>

<pre><code>layers {
  name: ""final_class""
  type: INNER_PRODUCT
  bottom: ""myLayer4""
  top: ""final_class""
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 10
    weight_filler {
      type: ""xavier""
      variance_norm: AVERAGE
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
</code></pre>
","<configuration><caffe><file-conversion><deeplearning4j>","2016-11-07 12:19:20","","1","879617","2016-12-08 16:34:15","2","0","1","","879617","125","113","6"
"40487952","<p>I have a custom 7(height) and 24(width) matrix input to be used for training. The output are labels with Age ( Young, Mature, Old).
I would like to go with Deeplearning4J Convolutional Neural Networks.</p>

<p>After building a very basic Convolutional Neural Network the very first training item gives the following error and i have no clue what is this about.</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: Invalid size index 2 wher it's &gt;= rank 2
at org.nd4j.linalg.api.ndarray.BaseNDArray.size(BaseNDArray.java:4066)
at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:192)
at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:247)
at org.deeplearning4j.nn.graph.vertex.impl.LayerVertex.doForward(LayerVertex.java:88)
at org.deeplearning4j.nn.graph.ComputationGraph.feedForward(ComputationGraph.java:983)
at org.deeplearning4j.nn.graph.ComputationGraph.computeGradientAndScore(ComputationGraph.java:889)
</code></pre>

<p>My DL4J code </p>

<pre><code>//Model Config here
MultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder()
    .seed(seed)
    .iterations(iterations)
    .regularization(true).l2(0.0005)
    .learningRate(0.01)//.biasLearningRate(0.02)
    //.learningRateDecayPolicy(LearningRatePolicy.Inverse).lrPolicyDecayRate(0.001).lrPolicyPower(0.75)
    .weightInit(WeightInit.XAVIER)
    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
    .updater(Updater.NESTEROVS).momentum(0.9) 
    .list()
    .layer(0, new ConvolutionLayer.Builder(4, 1)
        //nIn and nOut specify depth. nIn here is the nChannels and nOut is the number of filters to be applied
            .name(""hzvt1"")
        .nIn(nChannels)
        .stride(1, 1)
        .nOut(26)
        .activation(""relu"")//.activation(""identity"")
        .build())
    .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
        .nOut(outputNum)
        .activation(""softmax"")
        .build())  
    .setInputType(InputType.convolutional(nChannels,height,width))
    .backprop(true).pretrain(false);

//Model build here            
model.fit(wmTrain);MultiLayerConfiguration conf = builder.build();
model.fit(wmTrain);MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();            

//Training data creation here 
INDArray weekMatrix = Nd4j.ones(DLAgeGender.nChannels,DLAgeGender.height*DLAgeGender.width);       
double[] vector = new double[] { 0.0, 1.0, 0.0 };
INDArray intLabels = Nd4j.create(vector);
DataSet ds=new DataSet(weekMatrix,intLabels);
//Train the first item
model.fit(wmTrain);
</code></pre>

<p>I am using DL4J version 0.6 , Java Version 1.8, maven 3.3+ </p>

<p>I suspect a bug in the library.</p>
","<java><convolution><deeplearning4j>","2016-11-08 13:18:07","","0","879617","2016-11-10 07:13:47","1","0","","","879617","125","113","6"
"39338401","<p>I'm trying to run a DL4J example in a jupyter Notebook (IBM DSExperience) so I've ported the MLP Classifier example from Java to Scala, added the dependencies but currently struggling with the backend selection. I thought is would be sufficient just adding the following JAR
%AddJar <a href=""http://central.maven.org/maven2/org/nd4j/nd4j-x86/0.4-rc3.8/nd4j-x86-0.4-rc3.8.jar"" rel=""nofollow"">http://central.maven.org/maven2/org/nd4j/nd4j-x86/0.4-rc3.8/nd4j-x86-0.4-rc3.8.jar</a> so that the classloader in org.nd4j.linalg.factory.Nd4jBackend can pick it up, but unfortunately I get:</p>

<blockquote>
  <p>Name: java.lang.NoClassDefFoundError Message:
  org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException</p>
</blockquote>

<p>I've put the complete code into a <a href=""https://gist.github.com/romeokienzler/ad1ca47dc9ac1751ed681dd7b9947f39"" rel=""nofollow"">GIST</a></p>
","<java><scala><jupyter-notebook><deeplearning4j><nd4j>","2016-09-05 22:33:34","","0","3656912","2016-09-06 01:50:20","1","0","","","3656912","1520","527","0"
"54764896","<p>I have to save and load a keras model in java and then I thought I could use DL4J. The problem is that when I save my model it does not have the Embedding layer with his own weight.
I have the same problem re-loading the model in keras but in this case I can create the same architecture and load only the weight of my model.</p>

<p>In particolar I start from an architecture like this:</p>

<hr>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 300, 300)          219184200 
_________________________________________________________________
lstm_1 (LSTM)                (None, 300, 256)          570368    
_________________________________________________________________
dropout_1 (Dropout)          (None, 300, 256)          0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 128)               197120    
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 258       
=================================================================
</code></pre>

<p>And after save and load I get this (both in keras and in DL4J):</p>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 300)         219184200 
_________________________________________________________________
lstm_1 (LSTM)                (None, None, 256)         570368    
_________________________________________________________________
dropout_1 (Dropout)          (None, None, 256)         0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 128)               197120    
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 258       
=================================================================
</code></pre>

<p>There is a solution or a work around to have this in java?
1) Is it possible to save and load correctly the structure and the weight in keras?</p>

<p>2) is it possible to create a model of this type in java with DL4J or another library?</p>

<p>3) is it possible to implement the conversion word to Embedding in a function and then give to the neural network the input previously converted in Embedding?</p>

<p>4) Can i load the weights in the embedding layer in java with DL4J?</p>

<p>This is the code for my network:</p>

<pre><code>sentence_indices = Input(shape=input_shape, dtype=np.int32)
emb_dim = 300  # embedding di 300 parole in italiano
embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index, emb_dim)

embeddings = embedding_layer(sentence_indices)   

X = LSTM(256, return_sequences=True)(embeddings)
X = Dropout(0.15)(X)
X = LSTM(128)(X)
X = Dropout(0.15)(X)
X = Dense(num_activation, activation='softmax')(X)

model = Model(sentence_indices, X)

sequentialModel = Sequential(model.layers) 
</code></pre>

<p>Thanks in advance.</p>
","<keras><deeplearning4j><dl4j>","2019-02-19 11:10:30","","1","10732477","2019-02-20 09:41:57","2","0","","","10732477","34","19","0"
"43453167","<p>Is there any example for usage of deeplearning4j in android. I downloaded some of the examples from deeplearning4j.org. I did not find one for android.</p>
","<android><for-loop><deeplearning4j>","2017-04-17 14:11:29","","-2","7031681","2017-04-18 01:16:33","1","1","","","7031681","32","0","0"
"41457092","<p>I'm using deeplearning4j to learn text data. </p>

<p>I'm done with word2vec tutorial at deeplearning4j website and successfully </p>

<p>trained word vectors with 100 documents.</p>

<p>but i don't know how to get cosign distance of two different words like below picture</p>

<p><a href=""https://i.stack.imgur.com/WbcCp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WbcCp.png"" alt=""enter image description here""></a></p>

<p>Like this picture, if i insert word 'France' </p>

<p>i want to get </p>

<p>[similar words with france + cosign distance] </p>

<p>i can get [similar words with france]</p>

<p>but i don't know how to get cosign distance value.</p>

<p>any solution?</p>
","<word2vec><deeplearning4j>","2017-01-04 05:57:34","","0","6033925","2017-01-04 06:05:18","2","0","","","6033925","5","0","0"
"43358151","<p>I created a new JavaFX project with a pom.xml for Maven. Now I want to export it to a runnable JAR. Currently, I'm on IntelliJ IDEA 2017.1.1-2</p>

<p>How would one do this?</p>

<p>This is my current pom.xml:</p>

<pre><code>&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;

    &lt;groupId&gt;org.drawpvp&lt;/groupId&gt;
    &lt;artifactId&gt;drawpvp&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;


    &lt;name&gt;DrawPVP&lt;/name&gt;
    &lt;description&gt;Competitive drawing game.&lt;/description&gt;
    &lt;properties&gt;
        &lt;!-- Change the nd4j.backend property to nd4j-cuda-7.5-platform or nd4j-cuda-8.0-platform to use CUDA GPUs --&gt;
        &lt;nd4j.backend&gt;nd4j-native-platform&lt;/nd4j.backend&gt;

        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;nd4j.version&gt;0.8.0&lt;/nd4j.version&gt;
        &lt;dl4j.version&gt;0.8.0&lt;/dl4j.version&gt;
        &lt;datavec.version&gt;0.8.0&lt;/datavec.version&gt;
        &lt;arbiter.version&gt;0.8.0&lt;/arbiter.version&gt;
        &lt;rl4j.version&gt;0.8.0&lt;/rl4j.version&gt;

    &lt;/properties&gt;

    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;snapshots-repo&lt;/id&gt;
            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;
            &lt;releases&gt;
                &lt;enabled&gt;false&lt;/enabled&gt;
            &lt;/releases&gt;
            &lt;snapshots&gt;
                &lt;enabled&gt;true&lt;/enabled&gt;
            &lt;/snapshots&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;

    &lt;distributionManagement&gt;
        &lt;snapshotRepository&gt;
            &lt;id&gt;sonatype-nexus-snapshots&lt;/id&gt;
            &lt;name&gt;Sonatype Nexus snapshot repository&lt;/name&gt;
            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;
        &lt;/snapshotRepository&gt;
        &lt;repository&gt;
            &lt;id&gt;nexus-releases&lt;/id&gt;
            &lt;name&gt;Nexus Release Repository&lt;/name&gt;
            &lt;url&gt;http://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/distributionManagement&gt;

    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
                &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
                &lt;version&gt;${nd4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
        &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;dependencies&gt;
        &lt;!-- ND4J backend. You need one in every DL4J project. Normally define artifactId as either ""nd4j-native-platform"" or ""nd4j-cuda-7.5-platform"" --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;${nd4j.backend}&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;!-- Core DL4J functionality --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
            &lt;version&gt;${dl4j.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
            &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;
            &lt;version&gt;1.0.13&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.jfoenix&lt;/groupId&gt;
            &lt;artifactId&gt;jfoenix&lt;/artifactId&gt;
            &lt;version&gt;1.3.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.datavec&lt;/groupId&gt;
            &lt;artifactId&gt;datavec-api&lt;/artifactId&gt;
            &lt;version&gt;${datavec.version}&lt;/version&gt;
        &lt;/dependency&gt;


&lt;/dependencies&gt;
&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;com.zenjava&lt;/groupId&gt;
            &lt;artifactId&gt;javafx-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;8.8.3&lt;/version&gt;
            &lt;configuration&gt;
                &lt;mainClass&gt;gui.Main&lt;/mainClass&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>How would one export a JAR for this?</p>
","<java><maven><intellij-idea><javafx><deeplearning4j>","2017-04-12 00:09:31","","2","5590273","2017-04-12 08:13:30","2","5","0","","5590273","66","1","0"
"54744552","<p>I have a Sequential Model built in Keras and after trained it give me good prediction but when i save and then load the model i don't obtain the same prediction on the same dataset. Why?
Note that I checked the weight of the model and they are the same as well as the architecture of the model, checked with model.summary() and model.getWeights(). This is very strange in my opinion and I have no idea how to deal with this problem.
I don't have any error but the prediction are different</p>

<ol>
<li><p>I tried to use model.save() and load_model()</p></li>
<li><p>I tried to use model.save_weights() and after that re-built the model and then load the model</p></li>
</ol>

<p>I have the same problem with both options.</p>

<pre><code>def Classifier(input_shape, word_to_vec_map, word_to_index, emb_dim, num_activation):

    sentence_indices = Input(shape=input_shape, dtype=np.int32)
    emb_dim = 300  # embedding di 300 parole in italiano
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index, emb_dim)

    embeddings = embedding_layer(sentence_indices)   

    X = LSTM(256, return_sequences=True)(embeddings)
    X = Dropout(0.15)(X)
    X = LSTM(128)(X)
    X = Dropout(0.15)(X)
    X = Dense(num_activation, activation='softmax')(X)

    model = Model(sentence_indices, X)

    sequentialModel = Sequential(model.layers)    
    return sequentialModel

    model = Classifier((maxLen,), word_to_vec_map, word_to_index, maxLen, num_activation)
    ...
    model.fit(Y_train_indices, Z_train_oh, epochs=30, batch_size=32, shuffle=True)

    # attempt 1
    model.save('classificationTest.h5', True, True)
    modelRNN = load_model(r'C:\Users\Alessio\classificationTest.h5')

    # attempt 2
    model.save_weights(""myWeight.h5"")

    model = Classifier((maxLen,), word_to_vec_map, word_to_index, maxLen, num_activation)
    model.load_weights(r'C:\Users\Alessio\myWeight.h5') 

    # PREDICTION TEST
    code_train, category_train, category_code_train, text_train = read_csv_for_email(r'C:\Users\Alessio\Desktop\6Febbraio\2test.csv')

    categories, code_categories = get_categories(r'C:\Users\Alessio\Desktop\6Febbraio\2test.csv')

    X_my_sentences = text_train
    Y_my_labels = category_code_train
    X_test_indices = sentences_to_indices(X_my_sentences, word_to_index, maxLen)
    pred = model.predict(X_test_indices)

    def codeToCategory(categories, code_categories, current_code):

        i = 0;
        for code in code_categories:
            if code == current_code:
                return categories[i]
            i = i + 1 
        return ""no_one_find""   

    # result
    for i in range(len(Y_my_labels)):
        num = np.argmax(pred[i])

    # Pretrained embedding layer
    def pretrained_embedding_layer(word_to_vec_map, word_to_index, emb_dim):
    """"""
    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.

    Arguments:
    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    embedding_layer -- pretrained layer Keras instance
    """"""

    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)

    ### START CODE HERE ###
    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)
    emb_matrix = np.zeros((vocab_len, emb_dim))

    # Set each row ""index"" of the embedding matrix to be the word vector representation of the ""index""th word of the vocabulary
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]

    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. 
    embedding_layer = Embedding(vocab_len, emb_dim)
    ### END CODE HERE ###

    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the ""None"".
    embedding_layer.build((None,))

    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
    embedding_layer.set_weights([emb_matrix])

    return embedding_layer
</code></pre>

<p>Do you have any kind of suggestion? </p>

<p>Thanks in Advance.</p>

<p>Edit1: if use the code of saving and loading in the same ""page"" (I'm using notebook jupyter) it works fine. If I change ""page"" it doesn't work. Could it be that there is something related with the tensorflow session?</p>

<p>Edit2: my final goal is to load a model, trained in Keras, with Deeplearning4J in java. So if you know a solution for ""transforming"" the keras model in something else readable in DL4J it will help anyway.</p>

<p>Edit3: add function pretrained_embedding_layer()</p>

<p>Edit4: dictionaries from word2Vec model read with gensim</p>

<pre><code>from gensim.models import Word2Vec
model = Word2Vec.load('C:/Users/Alessio/Desktop/emoji_ita/embedding/glove_WIKI')

def getMyModels (model):
word_to_index = dict({})
index_to_word = dict({})
word_to_vec_map = dict({})
for idx, key in enumerate(model.wv.vocab):
    word_to_index[key] = idx
    index_to_word[idx] = key
    word_to_vec_map[key] = model.wv[key]
return word_to_index, index_to_word, word_to_vec_map
</code></pre>
","<python><tensorflow><keras><neural-network><deeplearning4j>","2019-02-18 09:56:58","","1","10732477","2019-02-18 14:39:38","1","2","0","","10732477","34","19","0"
"45010195","<p>I'm reading <a href=""http://www.deeplearningbook.org"" rel=""nofollow noreferrer"">Deep Learning Book</a> and puzzled by this ""undefined identifier"" (the Pa in the image, line 4). It appears at Page 208. Can you tell me just what Pa() means? Just a tip so that I can refer to Google. Thanks a lot! </p>

<p><a href=""https://tdl.recolic.net/tmp/snap-0710-184503.png"" rel=""nofollow noreferrer"">Link to origin image | I'm not allowed to post image directly</a></p>
","<machine-learning><deep-learning><deeplearning4j>","2017-07-10 10:54:01","","4","7295761","2017-07-10 10:55:39","1","1","","","7295761","117","78","0"
"45093562","<p>I would like to extract weights of 1d CNN layer, and understand how exactly the prediction values are computed. I am not able to re-produce the prediction values using the weights from <code>get_weights()</code> function.</p>

<p>In order to explain my understanding, here is a small data set.</p>

<pre><code>n_filter = 64
kernel_size = 10
len_timeseries = 123
n_feature = 3
X = np.random.random(sample_size*len_timeseries*n_feature).reshape(sample_size,len_timeseries,n_feature)
y = np.random.random(sample_size*(len_timeseries-kernel_size+1)*n_filter).reshape(sample_size,
                                                                                  (len_timeseries-kernel_size+1),
                                                                                  n_filter)
</code></pre>

<p>Now, create a simple 1d CNN model as:</p>

<pre><code>model = Sequential()
model.add(Conv1D(n_filter,kernel_size,
                 input_shape=(len_timeseries,n_feature)))
model.compile(loss=""mse"",optimizer=""adam"")
</code></pre>

<p>Fit the model and predict the values of <code>X</code> as:</p>

<pre><code>model.fit(X,y,nb_epoch=1)
y_pred = model.predict(X)
</code></pre>

<p>The dimension of <code>y_pred</code> is <code>(1000, 114, 64)</code> as it should.</p>

<p>Now, I want to reproduce the value of <code>y_pred[irow,0,ilayer]]</code> using weights stored in <code>model.layer</code>. As there is only single layer, <code>len(model.layer)=1</code>. So I extract the weights from the first and the only layer as:</p>

<pre><code>weight = model.layers[0].get_weights()
print(len(weight))
&gt; 2 
weight0 = np.array(weight[0])
print(weight0.shape)
&gt; (10, 1, 3, 64)
weight1 = np.array(weight[1])
print(weight1.shape)
&gt; (64,)
</code></pre>

<p>The weight has length 2 and I assume that the 0th position contain the weights for features and the 1st position contain the bias.
As the <code>weight0.shape=(kernel_size,1,n_feature,n_filter)</code>, I thought that I can obtain the values of <code>y_pred[irow,0,ilayer]</code> by:</p>

<pre><code>ifilter = 0
irow = 0
y_pred_by_hand = weight1[ifilter] + np.sum( weight0[:,0,:,ifilter] * X[irow,:kernel_size,:])
y_pred_by_hand
&gt; 0.5124888777
</code></pre>

<p>However, this value is quite different from <code>y_pred[irow,0,ifilter]</code> as:</p>

<pre><code> y_pred[irow,0,ifilter]
 &gt;0.408206
</code></pre>

<p>Please let me know where I got wrong.</p>
","<python><keras><keras-layer><deeplearning4j>","2017-07-14 02:15:34","","4","1639886","2019-03-08 13:10:57","1","0","3","","1639886","1058","263","9"
"41702060","<p>I was trying to run fcn on my data in caffe. I was able to convert my image sets into <code>lmdb</code> by <code>convert_imageset</code> builtin function caffe. However, once I wanted to train the <code>net</code>, it gave me the following error:</p>

<pre><code>Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
.....
Aborted (core dumped)
</code></pre>

<p>I went through many online resources to solve the memory failure, but most of them suggesting reducing batch size. Even, I reduced the size of images to 256x256. I could not tackle this issue yet. 
I checked the memory of GPU by this command <code>nvidia-smi</code>, and the model is <code>Nvidia GT 730</code> and the memory is <code>1998 MiB</code>. Since the batch size in <code>train_val.prototxt</code> is 1, I can not do anythin in <code>train_val.prototxt</code>. So my questions are:</p>

<ol>
<li>By looking at log file in Terminal,I realized that whenever <code>convert_imageset</code> converting the data into LMDB, it is taking 1000 image in a group. Is it possible I change this number in line <code>143</code> and <code>151</code> of <a href=""https://github.com/BVLC/caffe/blob/master/tools/convert_imageset.cpp#L143"" rel=""nofollow noreferrer""><code>convert_imageset.cpp</code></a> to a smaller (for example 2; to take two image at a time), recompile caffe, and then convert images to lmdb by using convert_imageset? Does it make sense?</li>
<li>If the answer to question 1 is yes, how can I compile caffe again,
should I remove <code>build</code> folder and again do caffe installation from
scratch? </li>
<li>How caffe process the LMDB data? Is it like taking a batch of those 1000 images showing while running convert_imagenet?</li>
</ol>

<p>Your help is really appreciated.
Thanks...</p>
","<deep-learning><caffe><pycaffe><lmdb><deeplearning4j>","2017-01-17 16:20:56","","1","6494707","2017-01-17 16:38:55","1","0","","","6494707","715","63","1"
"48309467","<p>I think DeepLearning4j uses CUDA, which is a NVIDIA thing. I bought this computer for things like neural networks but now I'm disappointed that I have an AMD GPU. Is it somehow possible that I can run DeepLearning4J on AMD?</p>
","<deeplearning4j>","2018-01-17 20:28:14","","0","9078142","2019-01-24 02:43:47","1","0","1","","9078142","70","8","0"
"43003926","<p>I am trying to impute images, like:
<a href=""https://www.researchgate.net/publication/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes</a></p>

<p>The example of mnist image imputation. </p>

<p>How to do it by using deeplearning4j VAE or SDA?</p>
","<deeplearning4j>","2017-03-24 15:51:29","","1","6714349","2017-03-24 20:49:26","1","0","","","6714349","60","5","0"
"49293450","<p><a href=""https://i.stack.imgur.com/TfMe9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TfMe9.png"" alt=""enter image description here""></a></p>

<p>This figure shows a basic block of a residual network. What it has two convolutional layers? What will happen when it has only one convolutional layer?</p>
","<machine-learning><deep-learning><conv-neural-network><deeplearning4j><deep-residual-networks>","2018-03-15 07:09:14","","0","570593","2018-03-15 07:53:03","1","0","","","570593","1553","362","5"
"46389059","<p>I'm trying to install the Deeplearning4j library ( <a href=""https://deeplearning4j.org/index.html"" rel=""nofollow noreferrer"">https://deeplearning4j.org/index.html</a>) but I don't understand how to use install the lib correctly with IntelliJ and Maven so that I can build a .jar file from it. </p>

<p>As long as I'm running the program from IntelliJ everything seems to work.</p>

<p>This is my pom.xml:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0""
     xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
     xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;groupId&gt;DeepLearning&lt;/groupId&gt;
&lt;artifactId&gt;deeplearning&lt;/artifactId&gt;
&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

&lt;dependencies&gt;


    &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-core --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
        &lt;version&gt;0.9.1&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.nd4j/nd4j-native --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
        &lt;artifactId&gt;nd4j-native&lt;/artifactId&gt;
        &lt;version&gt;0.9.1&lt;/version&gt;
    &lt;/dependency&gt;


    &lt;!-- https://mvnrepository.com/artifact/org.nd4j/nd4j-api --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
        &lt;artifactId&gt;nd4j-api&lt;/artifactId&gt;
        &lt;version&gt;0.9.1&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.nd4j/nd4j-native-platform --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
        &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
        &lt;version&gt;0.9.1&lt;/version&gt;
    &lt;/dependency&gt;


    &lt;!-- https://mvnrepository.com/artifact/org.datavec/datavec-api --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.datavec&lt;/groupId&gt;
        &lt;artifactId&gt;datavec-api&lt;/artifactId&gt;
        &lt;version&gt;0.9.1&lt;/version&gt;
    &lt;/dependency&gt;


&lt;/dependencies&gt;


&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;

            &lt;configuration&gt;
                &lt;archive&gt;
                    &lt;manifest&gt;
                        &lt;mainClass&gt;MLPClassifierLinear&lt;/mainClass&gt;
                    &lt;/manifest&gt;
                &lt;/archive&gt;
            &lt;/configuration&gt;

        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p></p>

<p>I am not sure if everything is set up correctly because it's the first time I use maven. </p>

<p>When I run the maven install command and start the .jar file I get an error that says that a JNI error has occurred and a NoClassDefFoundError.</p>

<p>This is the exact error message:</p>

<p>Error: A JNI error has occurred, please check your installation and 
        try again
        Exception in thread ""main"" java.lang.NoClassDefFoundError: 
        org/deeplearning4j/nn/conf/layers/Layer     at java.lang.Class.getDeclaredMethods0(Native Method)   at
    java.lang.Class.privateGetDeclaredMethods(Class.java:2701)  at
    java.lang.Class.privateGetMethodRecursive(Class.java:3048)  at
    java.lang.Class.getMethod0(Class.java:3018)     at
    java.lang.Class.getMethod(Class.java:1784)  at
    sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)
        at
    sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)
        Caused by: java.lang.ClassNotFoundException: 
        org.deeplearning4j.nn.conf.layers.Layer     at java.net.URLClassLoader.findClass(URLClassLoader.java:381)   at
    java.lang.ClassLoader.loadClass(ClassLoader.java:424)   at
    sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)   at
    java.lang.ClassLoader.loadClass(ClassLoader.java:357)   ... 7 more</p>

<p>Can somebody explain me how to use maven correctly so I can build .jar files without getting errors?</p>

<p>Thank you :) </p>
","<java><maven><java-native-interface><deeplearning4j><dl4j>","2017-09-24 10:30:40","","2","6355884","2017-09-24 17:23:10","1","1","2","","6355884","41","3","0"
"35366899","<p>I don't know what it wants from me. I am using </p>

<pre class=""lang-xml prettyprint-override""><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
        &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;
        &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>where </p>

<pre class=""lang-xml prettyprint-override""><code>&lt;deeplearning4j.version&gt;0.4-rc3.8&lt;/deeplearning4j.version&gt;
</code></pre>

<p>but I am getting</p>

<pre><code>Caused by: org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: null
    at org.nd4j.linalg.factory.Nd4jBackend.load(Nd4jBackend.java:148) ~[nd4j-api-0.4-rc3.7.jar:na]
    at org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:4498) ~[nd4j-api-0.4-rc3.7.jar:na]
    ... 53 common frames omitted
</code></pre>

<p>if I try to load the Google word vector model:</p>

<pre><code>@RequestMapping(""/loadModel"")
public Boolean loadModel(@RequestParam(value=""model"") String model) {

    Resource resource = appContext.getResource(""WEB-INF/word-vector-models/"" + model);

    try {
        File modelFile = resource.getFile();

        System.err.println(modelFile.getAbsolutePath());
        WordVectors googleModel = WordVectorSerializer.loadGoogleModel(modelFile, true);
        this.wordVectorsMap.put(model, googleModel);
    } catch (IOException e) {
        e.printStackTrace();
        return false;
    }

    return true;
}
</code></pre>
","<java><deeplearning4j>","2016-02-12 15:57:03","","11","826983","2017-01-26 16:03:00","1","0","2","","826983","6873","1380","450"
"44518140","<p>I've data in my <code>spark</code> <code>dataframe</code> (df) which have 24 features and the 25th column is my target variable. I want to fit my <code>dl4j</code> model on this <code>dataset</code> which takes input in the form of <code>org.nd4j.linalg.api.ndarray.INDArray,  org.nd4j.linalg.dataset.Dataset</code> or <code>org.nd4j.linalg.dataset.api.iterator.DataSetIterator</code>. How can I convert my <code>dataframe</code> to the required type ? </p>

<p>I've also tried using Pipeline method to input spark dataframe to the model directly. But sbt dependency of dl4j-spark-ml is not working. My build.sbt file is :</p>

<pre><code>scalaVersion := ""2.11.8""

libraryDependencies += ""org.deeplearning4j"" %% ""dl4j-spark-ml"" % ""0.8.0_spark_2-SNAPSHOT""

libraryDependencies += ""org.deeplearning4j"" % ""deeplearning4j-core"" % ""0.8.0""

libraryDependencies += ""org.nd4j"" % ""nd4j"" % ""0.8.0""

libraryDependencies += ""org.nd4j"" % ""nd4j-native-platform"" % ""0.8.0""

libraryDependencies += ""org.nd4j"" % ""nd4j-backends"" % ""0.8.0""

libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.0.1""

libraryDependencies += ""org.apache.spark"" %% ""spark-sql"" % ""2.0.1"" 
</code></pre>

<p>Can someone guide me from here ? Thanks in advance. </p>
","<scala><apache-spark><deeplearning4j><nd4j>","2017-06-13 09:55:48","","1","6419722","2017-06-13 11:32:04","1","0","","","6419722","446","24","3"
"45844280","<p>I need to use an embedding layer to encode the word vectors, so the weights of the embedding layer essentially are the word vectors. Obviously I don't want the weights in this case to be updated during back propagation. My question is if embedding layer by design already prohibits weight updates, or I have to do something special about it ?</p>
","<deeplearning4j>","2017-08-23 15:49:27","","0","745236","2017-08-24 14:58:53","1","0","","","745236","1073","24","2"
"43775338","<p>I have made an image classification model using Keras in Python which is in <strong>"".h5""</strong> format. I am trying to use it in my Android application using Deeplearning4j.</p>

<p>I am facing an issue when I try to do image classification by loading a <strong>Mat</strong> image using <a href=""https://deeplearning4j.org/datavecdoc/org/datavec/image/loader/NativeImageLoader.html#asMatrix-org.bytedeco.javacpp.opencv_core.Mat-"" rel=""nofollow noreferrer"">NativeImageLoader</a> constructor. The code is as follows :</p>

<pre><code>NativeImageLoader nativeImageLoader = new NativeImageLoader(60, 60, 3);
INDArray image = nativeImageLoader.asMatrix(testImage);   // testImage is of Mat format

// 0-255 to 0-1
DataNormalization scaler = new ImagePreProcessingScaler(0, 1);
scaler.transform(image);
// Pass through to neural Net
INDArray output = model.output(image);
INDArray labels = model.getLabels();
</code></pre>

<p>When the app builds it gives an error at the second line of the code above i.e <code>INDArray image = nativeImageLoader.asMatrix(testImage);</code></p>

<p>I get the following error when I build the apk:</p>

<pre><code>Error:(1109, 51) error: cannot access BufferedImage
class file for java.awt.image.BufferedImage not found
</code></pre>

<p>I tried to find the solution but <a href=""https://stackoverflow.com/a/6345814/7665986"">this</a> and <a href=""https://stackoverflow.com/a/33210293/7665986"">this</a> says that AWT package is not supported in Android.</p>

<p>Please help me with a solution or a work around. Thank you.</p>
","<android><keras><javacv><deeplearning4j>","2017-05-04 06:25:24","","1","7665986","2017-05-04 07:23:20","1","2","","","7665986","101","2","0"
"45332181","<p>I wonder what's the proper way to reuse a normalizer in ND4J/DL4J. Currently, I save it follows:</p>

<pre><code>final DataNormalization normalizer = new NormalizerStandardize();
normalizer.fit( trainingData );
normalizer.transform( trainingData );
normalizer.transform( testData );

try {
    final NormalizerSerializer normalizerSerializer = new NormalizerSerializer();
    normalizerSerializer.addStrategy( new StandardizeSerializerStrategy() );
    normalizerSerializer.write( normalizer, path );
} catch ( final IOException e ) {
    // ...
}
</code></pre>

<p>And load it via:</p>

<pre><code>try {
    final NormalizerSerializer normalizerSerializer = new NormalizerSerializer();
    normalizerSerializer.addStrategy( new StandardizeSerializerStrategy() );
    final DataNormalization normalizer = normalizerSerializer.restore( path );
} catch ( final Exception e ) { // Throws Exception instead of IOException.
    // ...
}
</code></pre>

<p>Is that OK? Unfortunately, I wasn't able to find more information in the docs.</p>
","<java><deeplearning4j><nd4j>","2017-07-26 16:06:23","","2","3429133","2018-02-07 15:14:05","1","0","","","3429133","1987","503","33"
"43749362","<p>The documentation says the library runs on GPUs. If my powerful laptop doesn't have a GPU, can I still run Deeplearning4J? </p>
","<deeplearning4j>","2017-05-03 00:48:59","","3","697911","2017-05-03 00:58:00","1","0","","","697911","3339","157","6"
"50210359","<p><strong>Context</strong></p>

<p>I am trying to create a model with DL4J.</p>

<p>There is two embeddings : one for user and one for item.</p>

<pre><code>val conf = new NeuralNetConfiguration.Builder()
  .updater(new Sgd(0.01))
  .graphBuilder()
  .addInputs(""item_input"", ""user_input"")
  .addLayer(""item_embedding"", new DenseLayer.Builder().nIn(5).nOut(5).build(), ""item_input"")
  .addLayer(""user_embedding"", new DenseLayer.Builder().nIn(5).nOut(5).build(), ""user_input"")
  // Something
  .build()

val net = new ComputationGraph(conf)
net.init()
</code></pre>

<p><strong>Problem</strong></p>

<p>At the end I would like to compute the cosine similarity between these two embeddings.</p>

<p>Then I want to train the model to maximize the similarity on positive example and minimize it on negative one.</p>

<p><em>Positive example = the user is interrested by the item</em></p>

<p><em>Negative example = the user is not interrested by the item</em></p>

<p><strong>Possible solutions</strong></p>

<p>I have found two possible solutions.</p>

<p>1) Create a custom layer class.</p>

<p>2) Create a custom LossFunction to apply cosine similarity on the output layers.</p>

<p><strong>Questions</strong></p>

<p>1) Is there a layer already implemented that implement a cosine similarity beetween two layers ?</p>

<p>2) If not, how can I implement my own layer ?
The only example I found is the following : <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/misc/customlayers/CustomLayerExampleReadme.md"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/misc/customlayers/CustomLayerExampleReadme.md</a></p>
","<cosine-similarity><deeplearning4j>","2018-05-07 08:51:07","","0","9006687","2018-06-02 04:10:26","1","0","","","9006687","57","11","0"
"44575828","<p>I am working on translating a lasagne neural network into deeplearning4j code. So far I've managed to get the layers in place but I am not sure if the other configurations are okay. I am not an expert in neural networks and cannot easily find the equivalent functions/methods in deeplearning4j. </p>

<p>This is the lasagne python code:</p>

<pre><code>    conv_net = NeuralNet(
    layers=[
        ('input', layers.InputLayer),
        ('conv1a', layers.Conv2DLayer),
        ('conv1', layers.Conv2DLayer),
        ('pool1', layers.MaxPool2DLayer),
        ('dropout1', layers.DropoutLayer),
        ('conv2a', layers.Conv2DLayer),
        ('conv2', layers.Conv2DLayer),
        ('pool2', layers.MaxPool2DLayer),
        ('dropout2', layers.DropoutLayer),
        ('conv3a', layers.Conv2DLayer),
        ('conv3', layers.Conv2DLayer),
        ('pool3', layers.MaxPool2DLayer),
        ('dropout3', layers.DropoutLayer),
        ('hidden4', layers.DenseLayer),
        ('dropout4', layers.DropoutLayer),
        ('hidden5', layers.DenseLayer),
        ('output', layers.DenseLayer),
    ],

    input_shape=(None, NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE),
    conv1a_num_filters=16, conv1a_filter_size=(7, 7), conv1a_nonlinearity=leaky_rectify,
    conv1_num_filters=32, conv1_filter_size=(5, 5), conv1_nonlinearity=leaky_rectify, pool1_pool_size=(2, 2), dropout1_p=0.1,
    conv2a_num_filters=64, conv2a_filter_size=(5, 5), conv2a_nonlinearity=leaky_rectify,
    conv2_num_filters=64, conv2_filter_size=(3, 3), conv2_nonlinearity=leaky_rectify, pool2_pool_size=(2, 2), dropout2_p=0.2,
    conv3a_num_filters=256, conv3a_filter_size=(3, 3), conv3a_nonlinearity=leaky_rectify,
    conv3_num_filters=256, conv3_filter_size=(3, 3), conv3_nonlinearity=leaky_rectify, pool3_pool_size=(2, 2), dropout3_p=0.2,
    hidden4_num_units=1250, dropout4_p=0.75, hidden5_num_units=1000,
    output_num_units=y.shape[1], output_nonlinearity=None,

    batch_iterator_train=AugmentBatchIterator(batch_size=180),

    update_learning_rate=theano.shared(np.cast['float32'](0.03)),
    update_momentum=theano.shared(np.cast['float32'](0.9)),

    on_epoch_finished=[
        AdjustVariable('update_learning_rate', start=0.01, stop=0.0001),
        AdjustVariable('update_momentum', start=0.9, stop=0.999),
        StoreBestModel('wb_' + out_file_name)
    ],

    regression=True,
    max_epochs=600,
    train_split=0.1,
    verbose=1,
)

conv_net.batch_iterator_train.part_flips = flip_idxs
conv_net.load_params_from('wb_keypoint_net3.pk')

conv_net.fit(X, y)
</code></pre>

<p>And here is what I've got so far in deeplearning4j:</p>

<pre><code>  int batch = 100;
    int iterations = data.getX().size(0) / batch + 1;
    int epochs = 600;
    logger.warn(""Building model"");
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .updater(Updater.NESTEROVS).momentum(0.9)
            .activation(Activation.RELU)
            .weightInit(WeightInit.XAVIER)
            .learningRate(0.3)
            .learningRateDecayPolicy(LearningRatePolicy.Score)
            .lrPolicyDecayRate(0.1)
            .regularization(true).l2(1e-4)
            .list()
            .layer(0, new ConvolutionLayer.Builder(7, 7).activation(Activation.LEAKYRELU).nOut(16).build()) //rectified linear units
            .layer(1, new ConvolutionLayer.Builder(5, 5).nOut(32).activation(Activation.LEAKYRELU).build())
            .layer(2, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2).build())
            .layer(3, new DropoutLayer.Builder(0.1).build())
            .layer(4, new ConvolutionLayer.Builder(5, 5).nOut(64).activation(Activation.LEAKYRELU).build())
            .layer(5, new ConvolutionLayer.Builder(3, 3).nOut(64).activation(Activation.LEAKYRELU).build())
            .layer(6, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2).build())
            .layer(7, new DropoutLayer.Builder(0.2).build())
            .layer(8, new ConvolutionLayer.Builder(3, 3).nOut(256).activation(Activation.LEAKYRELU).build())
            .layer(9, new ConvolutionLayer.Builder(3, 3).nOut(256).activation(Activation.LEAKYRELU).build())
            .layer(10, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2).build())
            .layer(11, new DropoutLayer.Builder(0.2).build())
            .layer(12, new DenseLayer.Builder().nOut(1250).build())
            .layer(13, new DropoutLayer.Builder(0.75).build())
            .layer(14, new DenseLayer.Builder().nOut(1000).build())
            .layer(15, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                    .nOut(data.getY().size(1)).activation(Activation.SOFTMAX).build())
            .setInputType(InputType.convolutional(image_size, image_size, num_channels))
            .backprop(true).pretrain(false)
            .build();

    MultiLayerNetwork model = new MultiLayerNetwork(conf);
    DataSet dataSet = new DataSet(data.getX(), data.getY());


    MiniBatchFileDataSetIterator iterator1 = new MiniBatchFileDataSetIterator(dataSet, batch);


    model.init();
    logger.warn(""Train model"");

    model.setListeners(new ScoreIterationListener(iterations));
    UtilSaveLoadMultiLayerNetwork uslmln = new UtilSaveLoadMultiLayerNetwork();
    for (int i = 0; i &lt; epochs; i++) {
        logger.warn(""Started epoch "" + i);
        model.fit(iterator1);
        uslmln.save(model, filename);
     }
</code></pre>

<p>I am mainly interested if the activation function and the configurations are equivalent. The problem is that when I run the neural network in java it seems to not learn at all, the score seems to stay at 0.2 even after 50 epochs with no visible improvements and I am sure that something was misconfigured.</p>

<p>Thanks</p>
","<numpy><deep-learning><theano><lasagne><deeplearning4j>","2017-06-15 19:49:54","","0","8168172","2017-06-17 14:04:21","1","0","","","8168172","1","0","0"
"35623995","<p>I'm training myself over learning neural network. There is a function that I can't make my neural network learn: <code>f(x) = max(x_1, x_2)</code>. It seems like a very simple function with 2 inputs and 1 input but yet a 3 layer neural network trained over a thousand sample with 2000 epochs get it completly wrong. I'm using <code>deeplearning4j</code>.</p>

<p>Is there any reason why the max function would be very hard to learn for a neural network or am I just tuning it wrong?</p>
","<neural-network><deep-linking><deeplearning4j>","2016-02-25 10:00:41","","2","678889","2019-05-01 11:12:50","2","0","1","","678889","304","14","0"
"53212867","<p>I have a dataset where some features are numerical, some categorical, and some are strings (e.g. description). To give an example, lets say I have three features:</p>

<pre><code>| Number | Type | Comment                               |
---------------------------------------------------------
| 1.23   | 1    | Some comment, up to 10000 characters  |
| 2.34   | 2    | Different comment many words          |
... 
</code></pre>

<p>Can I have all of them as input to a multi-layer network in dl4j, where numerical and categorical would be regular input features, but string comment feature will be processed first as word-series by a simple RNN (e.g. Embedding -> LSTM)? In other words, architecture should look something like this:</p>

<pre><code>""Number""  ""Type""  ""Comment""
  |         |         |
  |         |      Embedding
  |         |         |
  |         |       LSTM
  |         |         |
 Main Multi-Layer Network
          | 
        Dense
          |
         ...
          |
       Output
</code></pre>

<p>I think in Keras this can be achieved by Concatenate layer. Is there something like this in DL4J?</p>
","<machine-learning><deep-learning><deeplearning4j><dl4j>","2018-11-08 17:12:15","","0","4461907","2018-11-08 23:00:14","1","0","","","4461907","72","9","0"
"53671623","<p>I'm using deeplearning4j but when i load pre-trained model for text-classification I don't have enough RAM on my pc.</p>

<p>I tried to change eclipse.ini file and add more memory changing Xms and Xmx. Unfortunately it doesn't work for me.</p>

<p><a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory"" rel=""nofollow noreferrer"">https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory</a></p>

<p>In this link seems there is a possible solution to use less RAM even though it cost more time of corse, but I don't care now.</p>

<p>From that link:</p>

<blockquote>
  <p>Memory-mapped files ND4J supports the use of a memory-mapped file
  instead of RAM when using the nd4j-native backend. On one hand, it’s
  slower then RAM, but on other hand, it allows you to allocate memory
  chunks in a manner impossible otherwise.</p>
</blockquote>

<p>Can I add this in a code like this (follow the link)?</p>

<p><a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java</a></p>

<p>Of cours if there is another way (or a better way) write it. I'll appreciate any advice.</p>

<p>Thanks in advance.</p>
","<java><neural-network><deep-learning><text-classification><deeplearning4j>","2018-12-07 14:34:57","","0","10732477","2018-12-08 04:19:14","1","1","","","10732477","34","19","0"
"53826892","<p>I am not sure what’s the common practice to load a pre-trained keras model into a Java code?</p>

<p>I saw <a href=""https://deeplearning4j.org/docs/latest/keras-import-model-import"" rel=""nofollow noreferrer"">deeplearning4j</a> and <a href=""https://www.tensorflow.org/install/lang_java"" rel=""nofollow noreferrer"">Google's native Java support</a>.</p>

<p>Surprisingly, google says: </p>

<blockquote>
  <p>Caution: The TensorFlow Java API is not covered by the TensorFlow API stability guarantees.</p>
</blockquote>

<p>I am really confused, what's going on here and what are consideration I need to take before choosing a tensorflow Java engine. Obviously I would like to support the latest CUDA/ cuDNN. My application runs on PCs it's not a mobile application.</p>
","<java><python><tensorflow><deeplearning4j>","2018-12-18 05:30:06","","0","1031417","2018-12-22 07:51:00","1","0","","","1031417","19430","1482","277"
"53240765","<p>I'm trying to create and train an LSTM Autoencoder on character sequences (strings). This is simply for dimensionality reduction, i.e. to be able to represent strings of up to T=1000 characters as fixed-length vectors of size N. For the sake of this example, let N = 10. Each character is one-hot encoded by arrays of size validChars (in my case validChars = 77).</p>

<p>I'm using ComputationalGraph in be able to later remove decoder layers and use remaining for encoding. By looking at dl4j-examples I have come up with this:</p>

<pre><code>    ComputationGraphConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(12345)
            .l2(0.0001)
            .weightInit(WeightInit.XAVIER)
            .updater(new Adam(0.005))
            .graphBuilder()
            .addInputs(""input"")
            .addLayer(""encoder1"", new LSTM.Builder().nIn(dictSize).nOut(250)
                    .activation(Activation.TANH).build(), ""input"")
            .addLayer(""encoder2"", new LSTM.Builder().nIn(250).nOut(10)
                    .activation(Activation.TANH).build(), ""encoder1"")

            .addVertex(""fixed"", new PreprocessorVertex(new RnnToFeedForwardPreProcessor()), ""encoder2"")
            .addVertex(""sequenced"", new PreprocessorVertex(new FeedForwardToRnnPreProcessor()), ""fixed"")

            .addLayer(""decoder1"", new LSTM.Builder().nIn(10).nOut(250)
                    .activation(Activation.TANH).build(), ""sequenced"")
            .addLayer(""decoder2"", new LSTM.Builder().nIn(250).nOut(dictSize)
                    .activation(Activation.TANH).build(), ""decoder1"")

            .addLayer(""output"", new RnnOutputLayer.Builder()
                    .lossFunction(LossFunctions.LossFunction.MCXENT)
                    .activation(Activation.SOFTMAX).nIn(dictSize).nOut(dictSize).build(), ""decoder2"")

            .setOutputs(""output"")
            .backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(tbpttLength).tBPTTBackwardLength(tbpttLength)
            .build();
</code></pre>

<p>With this, I expected the number of features to follow the path: 
    [77,T] -> [250,T] -> [10,T] -> [10] -> [10,T] -> [250, T] -> [77,T]</p>

<p>I have trained this network, and removed decoder part like so:</p>

<pre><code>    ComputationGraph encoder = new TransferLearning.GraphBuilder(net)
            .setFeatureExtractor(""fixed"")
            .removeVertexAndConnections(""sequenced"")
            .removeVertexAndConnections(""decoder1"")
            .removeVertexAndConnections(""decoder2"")
            .removeVertexAndConnections(""output"")
            .addLayer(""output"", new ActivationLayer.Builder().activation(Activation.IDENTITY).build(), ""fixed"")
            .setOutputs(""output"")
            .setInputs(""input"")
            .build();
</code></pre>

<p>But, when I encode a string of length 1000 with this encoder, it outputs an NDArray of shape [1000, 10], instead of 1-dimensional vector of length 10. My purpose is to represent the whole 1000 character sequence with one vector of length 10. What am I missing?</p>
","<machine-learning><deep-learning><deeplearning4j>","2018-11-10 16:03:07","","0","4461907","2018-11-17 18:46:55","1","0","","","4461907","72","9","0"
"43854330","<p>I downloaded the entire project and import into Eclipse, but fails on this plugin:</p>

<pre><code>Description Resource    Path    Location    Type
Plugin execution not covered by lifecycle configuration: com.lewisd:lint-maven-plugin:0.0.11:check (execution: pom-lint, phase: validate)   pom.xml /dl4j-examples  line 8  Maven Project Build Lifecycle Mapping Problem
</code></pre>

<p>What's the problem and how to get it compiled?</p>
","<eclipse><maven><deeplearning4j>","2017-05-08 17:57:44","","2","697911","2017-05-09 01:33:14","1","0","","","697911","3339","157","6"
"52517196","<p>I experience vastly different prediction results when comparing the output of a neural network trained on a GPU in Python(3.5.5) + Keras (version 2.0.8), against the output of the same neural network on Android (API 24) using DL4J (1.0.0-beta2).</p>

<p>It would be very helpful, if someone can share their experience on how to tackle this problem, thank you!</p>

<p><strong>Importing the model into Android</strong></p>

<p>The neural network was converted to DL4J format by importing it using: </p>

<pre><code>MultiLayerNetwork model = KerasModelImport.importKerasSequentialModelAndWeights(SIMPLE_MLP, false)
</code></pre>

<p>and storing it using DL4Js <em>ModelSerializer</em>.</p>

<p>The model is imported into the Android Application using the DL4J method <em>restoreMultiLayerNetwork()</em></p>

<p><strong>Model Output</strong></p>

<p>The neural network is designed to make a prediction on images, of a fixed input shape: Fixed height, width, 3 channels.</p>

<p><strong>Image preprocessing pipeline in Android:</strong></p>

<p>The image is loaded as an inputstream from the device and stores it in an INDarray:</p>

<pre><code>AndroidNativeImageLoader loader = new AndroidNativeImageLoader(100, 100, 3);

InputStream  inputStream_bitmap = getContentResolver().openInputStream(uri);
INDArray indarray1 = loader.asMatrix(inputStream_bitmap);
</code></pre>

<p><em>AndroidNativeImageLoader()</em> loads and  re-scales the image.</p>

<p>The INDarray 'indarray1' is rescaled to contain values in range [0,1]:</p>

<pre><code>indarray1 = indarray1.divi(255);
</code></pre>

<p>The INDarray is passed through the network to compute the output:</p>

<pre><code>INDArray output = model.output(indarray1);
</code></pre>

<p><strong>Image preprocessing pipeline in Python:</strong></p>

<pre><code>from keras.preprocessing import image
from keras.utils import np_utils
import numpy as np

img = image.load_img(img_path, target_size=(100, 100))
img = image.img_to_array(img)
img = np.expand_dims(img, axis=0)
img = img.astype('float32')/255

output = model.predict(img)
</code></pre>

<p><strong>Problem:</strong></p>

<p>The prediction using Python and Keras differs significantly from the prediction in Android using DL4J. The output is an array of 2 values, each a float in [0,1].
The difference in prediction for a normal .bmp picture taken by a camera is up to 0.99 per element of this output array.</p>

<p><strong>Tests done so far:</strong></p>

<p>When using a monochromatic .bmp image(only red or only blue or only green or completely white), the prediction results are almost the same for both environments. They only differ by 10e-3, which can be explained by training on GPU and applying on CPU.</p>

<p><strong>Conclusion:</strong>
So far I believe, that the image preprocessing on Android is done differently as in Python, as the model output is the same for monochromatic pictures.</p>

<p>Has someone experienced a similar problem? Any help is much appreciated!</p>
","<android><keras><deeplearning4j><dl4j>","2018-09-26 11:56:14","","1","10311158","2018-09-28 13:24:13","1","1","","","10311158","60","4","0"
"53898493","<p>I have a net that has two inputs - one is time-series (recurrent), another is regular feedforward. The following part of code for graph builder should say it all:</p>

<pre><code>    final ComputationGraphConfiguration.GraphBuilder graphBuilder = builder.graphBuilder()
            .backpropType(BackpropType.TruncatedBPTT)
            .tBPTTBackwardLength(tbpttSize)
            .tBPTTForwardLength(tbpttSize)
            .addInputs(""recurrentInput"", ""nonRecurrentInput"")
            .setInputTypes(
                    InputType.recurrent(numFeaturesRecurrent),
                    InputType.feedForward(numFeaturesNonRecurrent))
            .addLayer(""encoder"",
                    new LSTM.Builder()
                            .nIn(numFeaturesRecurrent)
                            .nOut(hiddenRecurrentSize)
                            .activation(Activation.TANH)
                            .build(),
                    ""recurrentInput"")
            .addVertex(""thoughtVector"",
                    new LastTimeStepVertex(""recurrentInput""), ""encoder"")
            .addVertex(""merge"",
                    new MergeVertex(), ""thoughtVector"", ""nonRecurrentInput"")
            ...
</code></pre>

<p>TruncatedBPTT config parameters are applied to the whole input, and I get the following error:</p>

<pre><code>java.lang.IllegalArgumentException: NDArrayIndex is out of range. Beginning index: 50 must be less than its size: 13
    at org.nd4j.linalg.indexing.NDArrayIndex.validate(NDArrayIndex.java:459)
    at org.nd4j.linalg.indexing.NDArrayIndex.resolve(NDArrayIndex.java:364)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.get(BaseNDArray.java:4996)
    at org.deeplearning4j.nn.graph.ComputationGraph.getSubsetsForTbptt(ComputationGraph.java:3619)
    at org.deeplearning4j.nn.graph.ComputationGraph.doTruncatedBPTT(ComputationGraph.java:3568)
    at org.deeplearning4j.nn.graph.ComputationGraph.fitHelper(ComputationGraph.java:1140)
    at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:1098)
    at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:1006)
    at org.mypackage.MultivariatePredictorNet.train(MultivariatePredictorNet.java:140)
    at org.mypackage.MultivariatePredictorNet.main(MultivariatePredictorNet.java:209)
</code></pre>

<p>13 is exactly the number of features in the non-recurrent input. So, how can I make TruncatedBPTT config apply to recurrent input only?</p>
","<machine-learning><deep-learning><deeplearning4j>","2018-12-22 18:58:58","","0","4461907","2018-12-22 18:58:58","0","0","","","4461907","72","9","0"
"44789704","<p>I have a neural network with an input of size n and I want to extend it to a network with an input of size n + m with m > 0. How?</p>

<p>More details:
I am training a classifier that gets as input the probability distribution of an instance and outputs a binary value. For example let's say each instance in my dataset can have 10 different labels (For example MNIST), I have the probability distributions of all the instances. Let's say I want to classify these instances as Good/Bad. Let's say I have trained a model on one dataset (MNIST) and now I want to expand it to another dataset like cifar-100, in which every instance could have 100 different labels. I want to design a model (Neural Network) that could be trained on one dataset and trained on another dataset.
For example, people trained AlexNet on ImageNet and finetune it on another dataset by removing the last layer, and adding another layer with different size.
How could I do similar thing when the constraint is on the input not the output of the network? Is it possible to design a network that could have a variable length input? Or is it possible to change maybe 1,2 layers and finetune the network on another dataset?
[Is the problem clear? :)]</p>
","<neural-network><deep-learning><conv-neural-network><deeplearning4j>","2017-06-27 20:55:45","","1","401084","2017-07-27 09:46:26","1","0","","","401084","1021","80","2"
"45542017","<p>I have built a DL4j project. Everything is fine if I use MNIST dataset as follows:</p>

<pre><code>    DataSetIterator mnistTrain = new MnistDataSetIterator(batchSize, true, rngSeed);
    DataSetIterator mnistTest = new MnistDataSetIterator(batchSize, false, rngSeed);
</code></pre>

<p>However, I want to switch to my own csv file with the following format:</p>

<pre><code>A  |  B  |  C  |  X  |  Y
-------------------------
1  | 100 |  5  |  15 |  6
...
</code></pre>

<p><code>X</code> and <code>Y</code> are the outcomes (or labels). As I plan to perform regression analysis, so both <code>X</code> and <code>Y</code> are real numbers. So I read the csv file using the following code:</p>

<pre><code>    RecordReader recordReaderTrain = new CSVRecordReader(1, "","");
    recordReaderTrain.initialize(new FileSplit(new File(""src/main/resources/data/Data.csv"")));
    DataSetIterator dataIterTrain = new RecordReaderDataSetIterator(recordReaderTrain, batchSize, 3, 2);
</code></pre>

<p><code>3</code> in the code means <code>index of the labels</code> and <code>2</code> means <code>number of possible labels</code>. There no much explanation about these two parameters. I guess they mean the labels start from the 4th column and has 2 labels.</p>

<p>When I run the code, it shows the following exception:</p>

<pre><code>Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 14
</code></pre>

<p>I think it is because dl4j does not recognize <code>15</code> as label. </p>

<p>So my question is: how can I properly read the csv file for a regression analysis?</p>

<p>Many thanks.</p>
","<deeplearning4j>","2017-08-07 08:00:28","","0","7817561","2017-08-07 08:18:08","1","0","","","7817561","53","3","0"
"35762866","<p>My image is represented as org.bytedeco.javacpp.Mat. And I simply want to convert it to Java array of float/int. Reason behind conversion is that I want to use the Java array in other library (Nd4j) for image permute purposes. I have tried below approaches but they doesn't work.</p>

<pre><code>private static int[] MatToFloatArray1(Mat mat) {
        org.bytedeco.javacpp.BytePointer matData = mat.data();
        byte[] d = new byte[matData.capacity()];
        return toIntArray(d);
    }


private static int[] MatToFloatArray2(Mat mat) {
    org.bytedeco.javacpp.BytePointer matData = mat.data();
    IntBuffer intBuffer = matData.asBuffer().asIntBuffer();
    return intBuffer.array();

}
    private static int[] toIntArray(byte[] d) {
        IntBuffer intBuf =
                ByteBuffer.wrap(d)
                        .order(ByteOrder.BIG_ENDIAN)
                        .asIntBuffer();
        int[] array = new int[intBuf.remaining()];
        return array;

}
</code></pre>
","<java><javacpp><deeplearning4j><nd4j><dl4j>","2016-03-03 03:52:10","","0","2350444","2016-03-19 00:27:20","1","2","","","2350444","17","14","0"
"38418859","<p>I my case - at input I have <code>List&lt;List&lt;Float&gt;&gt;</code> (list of word representation vectors). And - have one <code>Double</code> at output from one sequence.</p>

<p>So I building next structure (first index - example number, second - sentence item number, third - word vector element number) : <a href=""http://pastebin.com/KGdjwnki"" rel=""nofollow"">http://pastebin.com/KGdjwnki</a></p>

<p>And in output : <a href=""http://pastebin.com/fY8zrxEL"" rel=""nofollow"">http://pastebin.com/fY8zrxEL</a></p>

<p>But when I masting one of next (<a href=""http://pastebin.com/wvFFC4Hw"" rel=""nofollow"">http://pastebin.com/wvFFC4Hw</a>) to model.output - I getting vector <code>[0.25, 0.24, 0.25, 0.25]</code>, not one value.</p>

<p>What can be wrong? Attached code (at Kotlin). <code>classCount</code> is one.</p>

<pre><code>import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import org.deeplearning4j.nn.conf.NeuralNetConfiguration.Builder
import org.deeplearning4j.nn.api.OptimizationAlgorithm
import org.deeplearning4j.nn.conf.Updater
import org.deeplearning4j.nn.weights.WeightInit
import org.deeplearning4j.nn.conf.layers.GravesLSTM
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer
import org.deeplearning4j.nn.conf.BackpropType
import org.nd4j.linalg.api.ndarray.INDArray
import org.nd4j.linalg.cpu.nativecpu.NDArray
import org.nd4j.linalg.indexing.NDArrayIndex
import org.nd4j.linalg.factory.Nd4j
import org.nd4j.linalg.lossfunctions.LossFunctions
import java.util.*

class ClassifierNetwork(wordVectorSize: Int, classCount: Int) {
    data class Dimension(val x: Array&lt;Int&gt;, val y: Array&lt;Int&gt;)
    val model: MultiLayerNetwork
    val optimization = OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT
    val iterations = 1
    val learningRate = 0.1
    val rmsDecay = 0.95
    val seed = 12345
    val l2 = 0.001
    val weightInit = WeightInit.XAVIER
    val updater = Updater.RMSPROP
    val backtropType = BackpropType.TruncatedBPTT
    val tbpttLength = 50
    val epochs = 50
    var dimensions = Dimension(intArrayOf(0).toTypedArray(), intArrayOf(0).toTypedArray())

    init {
        val baseConfiguration = Builder().optimizationAlgo(optimization)
                .iterations(iterations).learningRate(learningRate).rmsDecay(rmsDecay).seed(seed).regularization(true).l2(l2)
                .weightInit(weightInit).updater(updater)
                .list()
        baseConfiguration.layer(0, GravesLSTM.Builder().nIn(wordVectorSize).nOut(64).activation(""tanh"").build())
        baseConfiguration.layer(1, GravesLSTM.Builder().nIn(64).nOut(32).activation(""tanh"").build())
        baseConfiguration.layer(2, GravesLSTM.Builder().nIn(32).nOut(16).activation(""tanh"").build())
        baseConfiguration.layer(3, RnnOutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MCXENT)
                .activation(""softmax"").weightInit(WeightInit.XAVIER).nIn(16).nOut(classCount).build())
        val cfg = baseConfiguration.build()!!
        cfg.backpropType = backtropType
        cfg.tbpttBackLength = tbpttLength
        cfg.tbpttFwdLength = tbpttLength
        cfg.isPretrain = false
        cfg.isBackprop = true
        model = MultiLayerNetwork(cfg)
    }

    private fun dataDimensions(x: List&lt;List&lt;Array&lt;Double&gt;&gt;&gt;, y: List&lt;Array&lt;Double&gt;&gt;): Dimension {
        assert(x.size == y.size)
        val exampleCount = x.size
        assert(x.size &gt; 0)
        val sentenceLength = x[0].size
        assert(sentenceLength &gt; 0)
        val wordVectorLength = x[0][0].size
        assert(wordVectorLength &gt; 0)
        val classCount = y[0].size
        assert(classCount &gt; 0)
        return Dimension(
                intArrayOf(exampleCount, wordVectorLength, sentenceLength).toTypedArray(),
                intArrayOf(exampleCount, classCount).toTypedArray()
        )
    }

    data class Fits(val x: INDArray, val y: INDArray)
    private fun fitConversion(x: List&lt;List&lt;Array&lt;Double&gt;&gt;&gt;, y: List&lt;Array&lt;Double&gt;&gt;): Fits {
        val dim = dataDimensions(x, y)
        val xItems = ArrayList&lt;INDArray&gt;()
        for (i in 0..dim.x[0]-1) {
            val itemList = ArrayList&lt;DoubleArray&gt;();
            for (j in 0..dim.x[1]-1) {
                var rowList = ArrayList&lt;Double&gt;()
                for (k in 0..dim.x[2]-1) {
                    rowList.add(x[i][k][j])
                }
                itemList.add(rowList.toTypedArray().toDoubleArray())
            }
            xItems.add(Nd4j.create(itemList.toTypedArray()))
        }
        val xFits = Nd4j.create(xItems, dim.x.toIntArray(), 'c')
        val yItems = ArrayList&lt;DoubleArray&gt;();
        for (i in 0..y.size-1) {
            yItems.add(y[i].toDoubleArray())
        }
        val yFits = Nd4j.create(yItems.toTypedArray())
        return Fits(xFits, yFits)
    }

    private fun error(epoch: Int, x: List&lt;List&lt;Array&lt;Double&gt;&gt;&gt;, y: List&lt;Array&lt;Double&gt;&gt;) {
        var totalDiff = 0.0
        for (i in 0..x.size-1) {
            val source = x[i]
            val result = y[i]
            val realResult = predict(source)
            var diff = 0.0
            for (j in 0..result.size-1) {
                val elementDiff = result[j] - realResult[j]
                diff += Math.pow(elementDiff, 2.0)
            }
            diff = Math.sqrt(diff)
            totalDiff += Math.pow(diff, 2.0)
        }
        totalDiff = Math.sqrt(totalDiff)
        print(""Epoch "")
        print(epoch)
        print("", diff "")
        println(totalDiff)
    }

    fun train(x: List&lt;List&lt;Array&lt;Double&gt;&gt;&gt;, y: List&lt;Array&lt;Double&gt;&gt;) {
        dimensions = dataDimensions(x, y)
        val(xFit, yFit) = fitConversion(x, y)
        for (i in 0..epochs-1) {
            model.input = xFit
            model.labels = yFit
            model.fit()
            error(i+1, x, y)
        }
    }

    fun predict(x: List&lt;Array&lt;Double&gt;&gt;): Array&lt;Double&gt; {
        val xList = ArrayList&lt;DoubleArray&gt;();
        for (i in 0..dimensions.x[1]-1) {
            var row = ArrayList&lt;Double&gt;()
            for (j in 0..dimensions.x[2]-1) {
                row.add(x[j][i])
            }
            xList.add(row.toDoubleArray())
        }
        val xItem = Nd4j.create(xList.toTypedArray())
        val y = model.output(xItem)
        val result = ArrayList&lt;Double&gt;()
        return result.toTypedArray()
    }
}
</code></pre>

<p>upd. Seems like next example have ""near"" task, so later I'll check it and post solution : <a href=""https://github.com/deeplearning4j/dl4j-0.4-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/Word2VecSentimentRNN.java"" rel=""nofollow"">https://github.com/deeplearning4j/dl4j-0.4-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/Word2VecSentimentRNN.java</a></p>
","<deeplearning4j>","2016-07-17 07:23:10","","2","6148139","2017-12-13 05:20:12","3","1","1","","6148139","223","12","0"
"53030107","<p>Matlab and Numpy has <a href=""https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.meshgrid.html"" rel=""nofollow noreferrer"">meshgrid function</a>, which, given N-d space, creates N N-dimensional ndarrays with given coordinate in each.</p>

<p>Is there any equivalent in Nd4j?</p>

<p>I found <a href=""https://deeplearning4j.org/api/latest/org/nd4j/linalg/api/ops/impl/shape/MeshGrid.html"" rel=""nofollow noreferrer"">MeshGrid class</a>, but don't understand it's API, and apidoc is not finished.</p>
","<java><mesh><deeplearning4j><nd4j>","2018-10-28 09:34:34","","0","258483","2018-10-28 09:39:58","0","0","","","258483","13066","1488","43"
"50476137","<p>I have the need to running several (hundreds) of already trained LSTM neural networks with realtime data (on which new time steps are fed very frequently). These LSTM neural networks are implemented using deeplearining4j. In order to run all of these efficiently, I'd like to have them use GPUs to perform their calculations so that I could run hundreds of these with a large stream of realtime data.</p>

<p>I know I can train neural networks using GPUs.</p>

<p>My question is: <strong>can I execute them over realtime data using <code>rnnTimeStep()</code> on GPUs was well?</strong></p>

<p>Any pointers are very appreciated, I spent many hours searching but can't find anything on this. Only material describing training on GPUs.</p>

<p>Don't worry about the GPU overhead, I'm accounting for it, and I know this is an unusual thing to be doing. Just need to know if it's possible and if there are any pointers how to go about it.</p>

<p>Thanks!</p>
","<java><apache-spark><deep-learning><gpu><deeplearning4j>","2018-05-22 20:40:46","","1","489088","2018-06-02 04:01:12","2","0","","","489088","1697","63","2"
"35024675","<p>As shown in the screenshot there is a dependency of the dl4j-0.4-examples project that is unable to be loaded by intellij.  </p>

<blockquote>
  <p>Unable to get dependency information: Unable to read the metadata file
  for artifact 'com.github.jai-imageio-core.jar': Invalid JDK version in
  profile ""java8-and-higher': Unbounded range [1.8</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/DASmm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DASmm.jpg"" alt=""enter image description here""></a></p>

<p>A similar problem resolved in this question </p>

<p><a href=""https://github.com/google/gson/issues/596"" rel=""nofollow noreferrer"">https://github.com/google/gson/issues/596</a></p>

<blockquote>
  <p>as quick fix open the pom file at your local repository and add ')'
  and should  looks like</p>
  
  <p>91         [1.8,)</p>
  
  <p>save and execute again</p>
</blockquote>

<p>But in <em>this</em> case  there is no <code>jdk</code> tag: so that approach can not be used.</p>

<p>Has anyone found a workaround to load this project into intellij?</p>
","<intellij-idea><deeplearning4j>","2016-01-26 21:49:45","","3","1056563","2016-02-28 18:27:59","1","0","","","1056563","23330","2891","85"
"52515309","<p>I cannot find out how to get rid of / solve the below errors, that occur when loading a Neural Network model using the DL4J method <em>restoreMultiLayerNetwork</em> 
 using Android API >= 24:</p>

<p>The errors occur when the following line within the method <em>restoreMultiLayerNetwork</em> is called:</p>

<pre><code>params = Nd4j.read(ins2);
</code></pre>

<p>with</p>

<pre><code>InputStream ins2 = getResources().openRawResource(getResources().getIdentifier(""coefficients"", ""raw"", getPackageName()));
</code></pre>

<p>and</p>

<p>""coefficients.bin"" is the coefficients file, created by the below method from DL4J, when exporting a DL4J neural network:</p>

<pre><code>ModelSerializer.writeModel(model, locationToSave, saveUpdater);
</code></pre>

<p><strong>Errors: Libraries not accessible for ""classloader-namespace""</strong></p>

<blockquote>
  <p>E/linker: library ""/vendor/lib64/libcutils.so"" (""/vendor/lib64/libcutils.so"") needed or dlopened by ""/data/app/com.arai.arai-1/lib/arm64/libjnind4jcpu.so"" is not accessible for the namespace: [name=""classloader-namespace"", ld_library_paths="""", default_library_paths=""/data/app/com.arai.arai-1/lib/arm64:/system/fake-libs64:/data/app/com.arai.arai-1/base.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_dependencies_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_0_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_1_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_2_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_3_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_4_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_5_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_6_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_7_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_8_apk.apk!</p>
  
  <p>library ""/vendor/lib64/libutils.so"" (""/vendor/lib64/libutils.so"") needed or dlopened by
  ""/data/app/com.arai.arai-1/lib/arm64/libjnind4jcpu.so"" is not
  accessible for the namespace: [name=""classloader-namespace"",
  ld_library_paths="""",
  default_library_paths=""/data/app/com.arai.arai-1/lib/arm64:/system/fake-libs64:/data/app/com.arai.arai-1/base.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_dependencies_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_0_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_1_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_2_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_3_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_4_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_5_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_6_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_7_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_8_apk.apk!/l</p>
  
  <p>W/linker: library ""/system/lib64/libbinder.so""
  (""/system/lib64/libbinder.so"") needed or dlopened by
  ""/data/app/com.arai.arai-1/lib/arm64/libjnind4jcpu.so"" is not
  accessible for the namespace ""classloader-namespace"" - the access is
  temporarily granted as a workaround for <a href=""http://b/26394120"" rel=""nofollow noreferrer"">http://b/26394120</a>, note that
  the access will be removed in future releases of Android.</p>
  
  <p>E/linker:
  library ""/vendor/lib64/libnativeloader.so""
  (""/vendor/lib64/libnativeloader.so"") needed or dlopened by
  ""/data/app/com.arai.arai-1/lib/arm64/libjnind4jcpu.so"" is not
  accessible for the namespace: [name=""classloader-namespace"",
  ld_library_paths="""",
  default_library_paths=""/data/app/com.arai.arai-1/lib/arm64:/system/fake-libs64:/data/app/com.arai.arai-1/base.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_dependencies_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_0_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_1_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_2_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_3_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_4_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_5_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_6_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slice_7_apk.apk!/lib/arm64-v8a:/data/app/com.arai.arai-1/split_lib_slic</p>
  
  <p>W/linker: library ""/system/lib64/libandroid_runtime.so""
  (""/system/lib64/libandroid_runtime.so"") needed or dlopened by
  ""/data/app/com.arai.arai-1/lib/arm64/libjnind4jcpu.so"" is not
  accessible for the namespace ""classloader-namespace"" - the access is
  temporarily granted as a workaround for <a href=""http://b/26394120"" rel=""nofollow noreferrer"">http://b/26394120</a>, note that
  the access will be removed in future releases of Android.</p>
</blockquote>

<p><strong>Version of Android: API >= 24</strong></p>

<p><strong>Extract from build.gradle, where ND4J dependencies are set:</strong></p>

<pre><code>    android {
    compileSdkVersion 26
    buildToolsVersion ""26.0.1""

    defaultConfig {
        applicationId ""com.arai.arai""
        minSdkVersion 15
        targetSdkVersion 23
        versionCode 1
        versionName ""1.0""
        multiDexEnabled true

        javaCompileOptions {
        annotationProcessorOptions {
            includeCompileClasspath false
        }
        }


    buildTypes {
        release {
            minifyEnabled true
        shrinkResources true
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }

    buildscript {
    configurations.all {
        resolutionStrategy {
            force 'net.sf.proguard:proguard-gradle:5.3.2'
        }
    }
    }

     packagingOptions {
        exclude 'META-INF/DEPENDENCIES'
        exclude 'META-INF/DEPENDENCIES.txt'
        exclude 'META-INF/LICENSE'
        exclude 'META-INF/LICENSE.txt'
        exclude 'META-INF/license.txt'
        exclude 'META-INF/NOTICE'
        exclude 'META-INF/NOTICE.txt'
        exclude 'META-INF/notice.txt'
        exclude 'META-INF/INDEX.LIST'

    }
}

dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    compile 'com.android.support:appcompat-v7:27.1.0'
    compile 'com.mcxiaoke.volley:library-aar:1.0.0'
    compile 'com.android.support:design:27.1.0'

    compile (group: 'org.deeplearning4j', name: 'deeplearning4j-core', version: '1.0.0-beta2') {
    exclude group: 'org.bytedeco.javacpp-presets', module: 'opencv-platform'
    exclude group: 'org.bytedeco.javacpp-presets', module: 'leptonica-platform'
    exclude group: 'org.bytedeco.javacpp-presets', module: 'hdf5-platform'
    }      
    compile group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta2'
    compile group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta2', classifier: ""android-arm""
    compile group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta2', classifier: ""android-arm64""
    compile group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta2', classifier: ""android-x86""
    compile group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta2', classifier: ""android-x86_64""
    compile group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.0-1.4.2'
    compile group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.0-1.4.2', classifier: ""android-arm""
    compile group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.0-1.4.2', classifier: ""android-arm64""
    compile group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.0-1.4.2', classifier: ""android-x86""
    compile group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.0-1.4.2', classifier: ""android-x86_64""
    compile group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.2-1.4.2'
    compile group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.2-1.4.2', classifier: ""android-arm""
    compile group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.2-1.4.2', classifier: ""android-arm64""
    compile group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.2-1.4.2', classifier: ""android-x86""
    compile group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.2-1.4.2', classifier: ""android-x86_64""
    compile group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.2'
    compile group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.2', classifier: ""android-arm""
    compile group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.2', classifier: ""android-arm64""
    compile group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.2', classifier: ""android-x86""
    compile group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.2', classifier: ""android-x86_64""

    implementation 'com.google.code.gson:gson:2.8.2'
    annotationProcessor 'org.projectlombok:lombok:1.16.16'

    implementation 'com.google.code.findbugs:annotations:3.0.1', {
        exclude module: 'jsr305'
        exclude module: 'jcip-annotations'
    }

    //This corrects for a junit version conflict.
    configurations.all {
        resolutionStrategy.force 'junit:junit:4.12'
    }

}}
</code></pre>
","<java><android><deeplearning4j><dl4j><nd4j>","2018-09-26 10:13:35","","1","10311158","2018-09-26 10:13:35","0","3","","","10311158","60","4","0"
"44053756","<p>I've created a net from EncoderDecoderLSTM example <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/encdec/"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/encdec/</a></p>

<pre><code>NeuralNetConfiguration.Builder builder = new NeuralNetConfiguration.Builder();
    builder.iterations(1).learningRate(LEARNING_RATE).rmsDecay(RMS_DECAY)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).miniBatch(true).updater(Updater.RMSPROP)
            .weightInit(WeightInit.XAVIER).gradientNormalization(GradientNormalization.RenormalizeL2PerLayer);

    GraphBuilder graphBuilder = builder.graphBuilder().pretrain(false).backprop(true).backpropType(BackpropType.Standard)
            .tBPTTBackwardLength(TBPTT_SIZE).tBPTTForwardLength(TBPTT_SIZE);
    graphBuilder.addInputs(""inputLine"", ""decoderInput"")
            .setInputTypes(InputType.recurrent(dict.size()), InputType.recurrent(dict.size()))
            .addLayer(""embeddingEncoder"", new EmbeddingLayer.Builder().nIn(dict.size()).nOut(EMBEDDING_WIDTH).build(), ""inputLine"")
            .addLayer(""encoder"",
                    new GravesLSTM.Builder().nIn(EMBEDDING_WIDTH).nOut(HIDDEN_LAYER_WIDTH).activation(Activation.TANH).build(),
                    ""embeddingEncoder"")
            .addVertex(""thoughtVector"", new LastTimeStepVertex(""inputLine""), ""encoder"")
            .addVertex(""dup"", new DuplicateToTimeSeriesVertex(""decoderInput""), ""thoughtVector"")
            .addVertex(""merge"", new MergeVertex(), ""decoderInput"", ""dup"")
            .addLayer(""decoder"",
                    new GravesLSTM.Builder().nIn(dict.size() + HIDDEN_LAYER_WIDTH).nOut(HIDDEN_LAYER_WIDTH).activation(Activation.TANH)
                            .build(),
                    ""merge"")
            .addLayer(""output"", new RnnOutputLayer.Builder().nIn(HIDDEN_LAYER_WIDTH).nOut(dict.size()).activation(Activation.SOFTMAX)
                    .lossFunction(LossFunctions.LossFunction.MCXENT).build(), ""decoder"")
            .setOutputs(""output"");

    net = new ComputationGraph(graphBuilder.build());
    net.init();
</code></pre>

<p>How I can online train this net with a single examle??</p>

<p>I think it should be something like this, but needed to put ""&lt;go&gt;"" and ""&lt;eos&gt;"" vectors.</p>

<pre><code>private void train(float[] input, float[] prediction){
    float[] decodeArr = new float[dict.size()];
    INDArray decode = Nd4j.create(decodeArr, new int[]{1, dict.size(), 1});
    INDArray in = Nd4j.create(input);
    INDArray pred = Nd4j.create(prediction);
    INDArray predictionMask = Nd4j.ones(dict.size());
    INDArray inputMask = Nd4j.ones(dict.size());
    MultiDataSet data = new MultiDataSet(new INDArray[]{in, decode}, new INDArray[]{pred},
        new INDArray[] { inputMask, predictionMask }, new INDArray[] { predictionMask });
    net.fit(data);
}
</code></pre>
","<java><deeplearning4j>","2017-05-18 16:53:56","","0","7825021","2017-05-18 16:53:56","0","5","","","7825021","8","0","0"
"44009325","<p>I have a question regarding SentenceIterator/DocumentIterator for NLP. Each line in my file represents a short document, which consists of 1 or more sentences.  I would like to pass each line to UIMA nlp processor and receive a List of pos-tagged sentences for this single line (therefore one document), let's say List of PosTaggedSentences.  Is there something similar in the DL4j library that can achieve this purpose? </p>

<pre><code>SentenceIterator iter = UimaSentenceIterator.createWithPath(filePath);
</code></pre>

<p>This code splits all sentences in the file into individual ones, but it doesn't preserve the structure of one document per line.</p>

<p>Any suggestions how to do this in DL4j?</p>
","<uima><deeplearning4j>","2017-05-16 18:48:21","","0","697911","2017-05-19 14:40:17","1","0","","","697911","3339","157","6"
"53503655","<p>I am utilising the Word2Vec module of DeepLearning4j as a method of analysing the distance between words in article abstracts and many papers I have been reading are saying that the utilisation of triangle windows that utilise left leaning context words may be more efficient at calculating distance. When trying to change the window size parameter to a more left leaning window it won't compile, is it possible in DL4J or am I stuck with a rectangular context window? Below is my current code with the 2 rectangular window implemented.</p>

<pre><code>    word2Vec = new Word2Vec.Builder().minWordFrequency(2).iterations(1).layerSize(100).stopWords(fileUtilities.getStopList()).allowParallelTokenization(true).seed(42).windowSize(2)
            .iterate(sentenceIterator).tokenizerFactory(tokenizerFactory).seed(144).build();
</code></pre>
","<java><word2vec><deeplearning4j>","2018-11-27 16:05:56","","0","4618335","2018-11-27 16:05:56","0","0","","","4618335","16","3","0"
"45493612","<p>I have a question
Is there any way to use dl4j on netbeans without maven?
I have many restriction in muy computer, I can't install maven (nothing, by the way, politics of my job, you know...) but I have Netbeans 8.1. 
<a href=""https://i.stack.imgur.com/KxUvf.jpg"" rel=""nofollow noreferrer"">I have these libraries in my project</a>, well I just add them.
I have investigated and i found many examples but using maven and other things.</p>

<p>I can't change of IDE either.</p>

<p>Is it possible to use deeplearning4j on neatbeans without maven?</p>

<p>Greeting!</p>
","<java><netbeans><deeplearning4j>","2017-08-03 20:04:07","","2","8400068","2017-08-14 14:49:33","2","1","","","8400068","9","0","0"
"41482962","<p>I have trained theanets neural network model and I want to use the same model in deeplearning4j, any suggestions?</p>
","<neural-network><deeplearning4j>","2017-01-05 10:45:58","","0","4619335","2017-01-05 11:13:58","1","0","","","4619335","9","0","0"
"53128849","<p>In a Scala project using <a href=""https://deeplearning4j.org/"" rel=""nofollow noreferrer"">DL4J</a> aiming to import a Keras model, my SBT contains the below to DL4J and ND4J dependencies from Sonatype snapshots. I am using Sonatype snapshots and not Maven due to <a href=""https://github.com/deeplearning4j/deeplearning4j/issues/6550"" rel=""nofollow noreferrer"">DL4J Keras Import: 2.2.x/2.3.x compatibility</a> issue.</p>

<pre><code>resolvers += Resolver.sonatypeRepo(""snapshots"")

libraryDependencies ++= Seq(
  // https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-core
  ""org.deeplearning4j"" % ""deeplearning4j-core"" % ""1.0.0-SNAPSHOT"",
  // https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-nn
  ""org.deeplearning4j"" % ""deeplearning4j-nn"" % ""1.0.0-SNAPSHOT"",
  // https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-modelimport
  ""org.deeplearning4j"" % ""deeplearning4j-modelimport"" % ""1.0.0-SNAPSHOT"",
  // https://mvnrepository.com/artifact/org.nd4j/nd4j-cuda-9.0
  ""org.nd4j"" % ""nd4j-cuda-9.0"" % ""1.0.0-SNAPSHOT"",
  // https://mvnrepository.com/artifact/org.nd4j/nd4j-cuda-9.0-platform
  ""org.nd4j"" % ""nd4j-cuda-9.0-platform"" % ""1.0.0-SNAPSHOT"",
  // https://mvnrepository.com/artifact/org.nd4j/nd4j-api
  ""org.nd4j"" % ""nd4j-api"" % ""1.0.0-SNAPSHOT""
)
</code></pre>

<p>Then to the below code snippet:</p>

<pre><code>import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import org.deeplearning4j.nn.modelimport.keras.KerasModelImport

val model = KerasModelImport.importKerasSequentialModelAndWeights(
  """"""{Valid path to a working Keras model saved as an H5 file}"""""",
  true
)
</code></pre>

<p>I get a </p>

<pre><code>Class org.nd4j.evaluation.IEvaluation not found - continuing with a stub.
val model = KerasModelImport.importKerasSequentialModelAndWeights(
</code></pre>

<p>error. Even though I see <code>import org.nd4j.evaluation.IEvaluation;</code> used <a href=""https://github.com/search?q=org%3Adeeplearning4j%20IEvaluation&amp;type=Code"" rel=""nofollow noreferrer"">many times</a> in DL4J GitHub repositories, I don't know what to import to make it available.</p>
","<scala><keras><deeplearning4j><dl4j><nd4j>","2018-11-03 06:04:49","","0","4169924","2018-11-03 06:04:49","0","1","","","4169924","357","249","0"
"53195312","<p>I'm writting sample code for sentence classification using a CNN. I have trained my <code>ComputationGraph</code> and saved it to disk. When I load the network in the future, how would I create an <code>INDArray</code> object from a sentence I want to classify?</p>

<pre><code>ComputationGraph net = ModelSerializer.restoreComputationGraph(""My model"");
String sentence = ""Test sentence"";
// How do I create INDArray?
INDArray array = ????????;
INDArray results = net.outputSingle(array);
</code></pre>
","<deeplearning4j><dl4j>","2018-11-07 18:09:46","","0","1040535","2018-11-08 08:23:37","0","0","","","1040535","78","2","0"
"52472053","<p>Thanks in advance.
I am using Word2Vec in DeepLearning4j.</p>

<p>How do I clear the vocab cache in Word2Vec. This is because I want it to retrain on a new set of word patterns every time I reload Word2Vec. For now, it seems that the vocabulary of the previous set of word patterns persists and I get the same result even though I changed my input training file. </p>

<p>I try to reset the model, but it doesn't work. Codes:-</p>

<p>Word2Vec vec = new Word2Vec.Builder()
                .minWordFrequency(1)
                .iterations(1)
                .layerSize(4)
                .seed(1)
                .windowSize(1)
                .iterate(iter)
                .tokenizerFactory(t)
             .resetModel(true)
             .limitVocabularySize(1)
             .build();</p>

<p>Anyone can help? </p>
","<java><neural-network><word2vec><deeplearning4j>","2018-09-24 02:36:17","","0","10405823","2018-10-03 03:02:30","1","0","","","10405823","3","0","0"
"53692580","<p>I am trying to implement a CNN to correctly detect the edges of a quadrilateral 
in a <strong>322x322</strong> binary image i.e. a white quadrilateral on a black background image e.g.</p>

<p><a href=""https://i.stack.imgur.com/oxJXU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oxJXU.png"" alt=""enter image description here""></a></p>

<p>the network output is designed to represent x and y sandwiched together so for any given
image from the dataset, four(<code>4</code>) corresponding x and y neuron pairs should be fired/excited so that
my labels for the training is a one-hot-label encoding(multi-class classification) for each xy pair of edges for the quardilateral
e.g.say my image is 10x10 if the first quad has the following edges(<strong>x,y</strong>) where possible (x,y)-values are from [<strong>0-9</strong>] 
 => (<code>1</code>,<code>2</code>) (<code>3</code>,<code>8</code>) (<code>4</code>,<code>3</code>) (<code>5</code>,<code>7</code>)
my label is like </p>

<p>[image][<img src=""https://i.stack.imgur.com/bq6Uk.png"" alt=""enter image description here"">]<a href=""https://i.stack.imgur.com/bq6Uk.png"" rel=""nofollow noreferrer"">2</a></p>

<p>for each quad. Here is the configuration for my network in java using deeplearning4j.          </p>

<pre><code>         MultiLayerConfiguration EDGE_PERCEPTRON = new NeuralNetConfiguration.Builder()
                    .seed(seed)
                    .l2(0.0005)
                    .weightInit(WeightInit.XAVIER)
                    .updater(new Nesterovs(new MapSchedule(ScheduleType.ITERATION, lrSchedule)))
                    .list()
                    .layer(0, new ConvolutionLayer.Builder(8, 8)
                            .stride(2, 2)
                            .padding(0, 0)
                            .nIn(channels)
                            .nOut(filtersize10)
                            .activation(Activation.IDENTITY)
                            .build())
                    .layer(1, new ConvolutionLayer.Builder(4, 4)
                            .stride(2, 2)
                            .padding(0, 0)
                            .nOut(filtersize20)
                            .activation(Activation.IDENTITY)
                            .build())
                    .layer(2, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)
                            .kernelSize(2, 2)
                            .stride(1, 1)
                            .padding(0, 0)
                            .build())
                    .layer(3, new DenseLayer.Builder().activation(Activation.SIGMOID)
                            .nOut(outputSize).build())
                    .layer(4, new OutputLayer.Builder(LossFunctions.LossFunction.XENT)
                            .nOut(outputSize)
                            .activation(Activation.SIGMOID)
                            .build())
                    .setInputType(InputType.convolutionalFlat(height, width, channels))
                    .backprop(true).pretrain(false).build();
</code></pre>

<p>Here is it's graphical representation </p>

<p><a href=""https://i.stack.imgur.com/UqonV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UqonV.png"" alt=""enter image description here""></a></p>

<p>Is there anything wrong with my configuration or rather what do you think I am doing wrong. The network doesn't learn anything from the dataset, the predictions  are all uniform and all lesser than 0.1 .
How can I configure the network correctly to give the right predictions?</p>
","<machine-learning><deep-learning><computer-vision><conv-neural-network><deeplearning4j>","2018-12-09 13:02:07","","1","10138416","2018-12-09 14:41:31","0","3","","","10138416","132","40","0"
"53860567","<p>deeplearning4j : How can I store/save a trained model on persistence level and load it back when an ad-hoc request comes to evaluate the deep learning model?</p>

<pre><code>        DataNormalization normalizer = new NormalizerStandardize();
        normalizer.fit(trainingData);           //Collect the statistics (mean/stdev) from the training data. This does not modify the input data
        normalizer.transform(trainingData); 

        //run the model
        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();
        model.setListeners(new ScoreIterationListener(100));

        for( int i=0; i&lt;epochs; i++ ) {
            model.fit(trainingData);
        }
</code></pre>

<p><strong>I need to store the trained model. How can I do this? With which Api?</strong></p>

<pre><code>        //evaluate the model on the test set
        Evaluation eval = new Evaluation(3);
        INDArray output = model.output(testData.getFeatures());

        eval.eval(testData.getLabels(), output);
        log.info(eval.stats());    
</code></pre>
","<deeplearning4j>","2018-12-19 23:26:42","","1","9277462","2019-01-14 12:07:25","1","0","","","9277462","6","0","0"
"54159900","<p>Env: Windows 7, GeForce GTX 750, CUDA 10.0, cuDNN 7.4</p>

<p>Maven dependencies:</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
        &lt;artifactId&gt;nd4j-cuda-10.0&lt;/artifactId&gt;
        &lt;version&gt;1.0.0-beta3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-cuda-10.0&lt;/artifactId&gt;
        &lt;version&gt;1.0.0-beta3&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>Every 10 minibatches I'm checking performance on the test test. I used to call net.evaluate(), but that gave me this error:</p>

<pre><code>Exception in thread ""AMDSI prefetch thread"" java.lang.RuntimeException: java.lang.RuntimeException: Failed to allocate 637074016 bytes from DEVICE [0] memory
    at org.deeplearning4j.datasets.iterator.AsyncMultiDataSetIterator$AsyncPrefetchThread.run(AsyncMultiDataSetIterator.java:396)
Caused by: java.lang.RuntimeException: Failed to allocate 637074016 bytes from DEVICE [0] memory
    at org.nd4j.jita.memory.CudaMemoryManager.allocate(CudaMemoryManager.java:76)
    at org.nd4j.jita.workspace.CudaWorkspace.init(CudaWorkspace.java:88)
    at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.initializeWorkspace(Nd4jWorkspace.java:508)
    at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.close(Nd4jWorkspace.java:651)
    at org.deeplearning4j.datasets.iterator.AsyncMultiDataSetIterator$AsyncPrefetchThread.run(AsyncMultiDataSetIterator.java:372)
</code></pre>

<p>Then I switched from net.evaluate() to net.output() with training = false and reduced the size of test set from 100 to just 20.
This worked without errors. I tried to increase the number of records to 30, it showed this warning, but kept working:</p>

<pre><code>2019-01-12 14:47:44 WARN  org.deeplearning4j.nn.layers.BaseCudnnHelper Cannot allocate 300000000 bytes of device memory (CUDA error = 2), proceeding with host memory
</code></pre>

<p>I can understand that there is not enough memory on the video card (<a href=""https://www.geforce.com/hardware/desktop-gpus/geforce-gtx-750/specifications"" rel=""nofollow noreferrer"">GeForce GTX 750 Spec</a> shows memory of 1G), but 
since it can use host memory I increased test set size back to 100, and got a permanent failure with this error:</p>

<pre><code>2019-01-12 14:59:29 WARN  org.deeplearning4j.nn.layers.BaseCudnnHelper Cannot allocate 1000000000 bytes of device memory (CUDA error = 2), proceeding with host memory
Exception in thread ""main"" 2019-01-12 14:59:39 ERROR org.deeplearning4j.util.CrashReportingUtil &gt;&gt;&gt; Out of Memory Exception Detected. Memory crash dump written to: C:\DATA\Projects\dl4j-language-model\dl4j-memory-crash-dump-1547294372940_1.txt
java.lang.OutOfMemoryError: Failed to allocate memory within limits: totalBytes (470M + 7629M) &gt; maxBytes (7851M)
2019-01-12 14:59:39 WARN  org.deeplearning4j.util.CrashReportingUtil Memory crash dump reporting can be disabled with CrashUtil.crashDumpsEnabled(false) or using system property -Dorg.deeplearning4j.crash.reporting.enabled=false
    at org.bytedeco.javacpp.Pointer.deallocator(Pointer.java:580)
        at org.deeplearning4j.nn.layers.BaseCudnnHelper$DataCache.&lt;init&gt;(BaseCudnnHelper.java:119)
2019-01-12 14:59:39 WARN  org.deeplearning4j.util.CrashReportingUtil Memory crash dump reporting output location can be set with CrashUtil.crashDumpOutputDirectory(File) or using system property -Dorg.deeplearning4j.crash.reporting.directory=&lt;path&gt;
        at org.deeplearning4j.nn.layers.recurrent.CudnnLSTMHelper.activate(CudnnLSTMHelper.java:509)
</code></pre>

<p>Now, I'm assuming <code>maxBytes (7851M)</code> refers to heap size (JVM runs with -Xmx8G -Xms8G), yet I also output <code>Runtime</code> <code>freeMemory()</code> and <code>totalMemory()</code> and it showed the following right before crashing, which is more than enough of free memory:</p>

<pre><code>2019-01-12 15:29:20 INFO  Free memory: 7722607976/8232370176
</code></pre>

<p><strong>So my question is, where do <code>totalBytes (470M + 7629M)</code> numbers come from and why required 1G cannot be allocated if there is free memory available inside JVM?</strong></p>

<p>Below is the memory crash report:</p>

<pre><code>Deeplearning4j OOM Exception Encountered for ComputationGraph
Timestamp:                              2019-01-12 14:59:32.940
Thread ID                               1
Thread Name                             main


Stack Trace:
java.lang.OutOfMemoryError: Failed to allocate memory within limits: totalBytes (470M + 7629M) &gt; maxBytes (7851M)
    at org.bytedeco.javacpp.Pointer.deallocator(Pointer.java:580)
    at org.deeplearning4j.nn.layers.BaseCudnnHelper$DataCache.&lt;init&gt;(BaseCudnnHelper.java:119)
    at org.deeplearning4j.nn.layers.recurrent.CudnnLSTMHelper.activate(CudnnLSTMHelper.java:509)
    at org.deeplearning4j.nn.layers.recurrent.LSTMHelpers.activateHelper(LSTMHelpers.java:205)
    at org.deeplearning4j.nn.layers.recurrent.LSTM.activateHelper(LSTM.java:163)
    at org.deeplearning4j.nn.layers.recurrent.LSTM.activate(LSTM.java:140)
    at org.deeplearning4j.nn.graph.vertex.impl.LayerVertex.doForward(LayerVertex.java:110)
    at org.deeplearning4j.nn.graph.ComputationGraph.outputOfLayersDetached(ComputationGraph.java:2316)
    at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1727)
    at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1686)
    at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1672)
    at org.lungen.deeplearning.net.predictor.CharacterSequenceValuePredictorNet.testOutputAndScore(CharacterSequenceValuePredictorNet.java:195)
    at org.lungen.deeplearning.net.predictor.CharacterSequenceValuePredictorNet.train(CharacterSequenceValuePredictorNet.java:166)
    at org.lungen.deeplearning.net.predictor.CharacterSequenceValuePredictorNet.main(CharacterSequenceValuePredictorNet.java:283)


========== Memory Information ==========
----- Version Information -----
Deeplearning4j Version                  1.0.0-beta3
Deeplearning4j CUDA                     deeplearning4j-cuda-10.0

----- System Information -----
Operating System                        Microsoft Windows 7 SP1
CPU                                     Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz
CPU Cores - Physical                    4
CPU Cores - Logical                     8
Total System Memory                       15.97 GB (17144102912)
Number of GPUs Detected                 1
  Name                           CC                Total Memory              Used Memory              Free Memory
  GeForce GTX 750                5.0          2 GB (2147483648)     1.67 GB (1795002368)    336.15 MB (352481280)

----- ND4J Environment Information -----
Data Type                               FLOAT
backend                                 CUDA
blas.vendor                             CUBLAS
os                                      Windows 7

----- Memory Configuration -----
JVM Memory: XMX                            7.67 GB (8232370176)
JVM Memory: current                        7.67 GB (8232370176)
JavaCPP Memory: Max Bytes                  7.67 GB (8232370176)
JavaCPP Memory: Max Physical              15.33 GB (16464740352)
JavaCPP Memory: Current Bytes            470.26 MB (493106209)
JavaCPP Memory: Current Physical           3.35 GB (3601498112)
Periodic GC Enabled                     true
Periodic GC Frequency                   100 ms

----- Workspace Information -----
Workspaces: # for current thread        4
Current thread workspaces:
  Name                      State       Size                          # Cycles            
  WS_LAYER_WORKING_MEM      CLOSED       117.40 MB (123100000)        6802                
  WS_ALL_LAYERS_ACT         CLOSED        19.41 MB (20349840)         2400                
  WS_LAYER_ACT_0            CLOSED         6.23 MB (6528000)          1601                
  WS_LAYER_ACT_1            CLOSED       381.47 MB (400000000)        1601                
Workspaces total size                    524.50 MB (549977840)
Helper Workspaces
  CUDNN_WORKSPACE                            7.06 MB (7408000)

----- Network Information -----
Network # Parameters                    1432106
Parameter Memory                           5.46 MB (5728424)
Parameter Gradients Memory                 5.46 MB (5728424)
Updater Number of Elements              2862812
Updater Memory                            10.92 MB (11451248)
Updater Classes:
  org.nd4j.linalg.learning.AdamUpdater
  org.nd4j.linalg.learning.NoOpUpdater
Params + Gradient + Updater Memory        16.38 MB (17179672)
Iteration Count                         400
Epoch Count                             0
Backprop Type                           TruncatedBPTT
TBPTT Length                            50/50
Workspace Mode: Training                ENABLED
Workspace Mode: Inference               ENABLED
Number of Layers                        7
Layer Counts
  BatchNormalization                      2
  DenseLayer                              1
  LSTM                                    3
  OutputLayer                             1
Layer Parameter Breakdown
  Idx Name                 Layer Type           Layer # Parameters   Layer Parameter Memory
  1   lstm-1               LSTM                 403000                  1.54 MB (1612000)
  2   lstm-2               LSTM                 501000                  1.91 MB (2004000)
  3   lstm-3               LSTM                 501000                  1.91 MB (2004000)
  5   norm-1               BatchNormalization   1000                    3.91 KB (4000)   
  6   dense-1              DenseLayer           25100                  98.05 KB (100400) 
  7   norm-2               BatchNormalization   400                     1.56 KB (1600)   
  8   output               OutputLayer          606                     2.37 KB (2424)   

----- Layer Helpers - Memory Use -----
#   Layer Name           Layer Class               Helper Class                   Total Memory Memory Breakdown
5   norm-1               BatchNormalization        CudnnBatchNormalizationHelper     1.95 KB (2000) {meanCache=1000, varCache=1000}
7   norm-2               BatchNormalization        CudnnBatchNormalizationHelper       800 B   {meanCache=400, varCache=400}
Total Helper Count                      2
Helper Count w/ Memory                  2
Total Helper Persistent Memory Use         2.73 KB (2800)

----- Network Activations: Inferred Activation Shapes -----
Current Minibatch Size                  100
Current Input Shape (Input 0)           [100, 152, 2000]
Idx Name                 Layer Type           Activations Type                           Activations Shape    # Elements   Memory      
0   recurrentInput       InputVertex          InputTypeRecurrent(152,timeSeriesLength=2000) [100, 152, 2000]     30400000      115.97 MB (121600000)
1   lstm-1               LSTM                 InputTypeRecurrent(250,timeSeriesLength=2000) [100, 250, 2000]     50000000      190.73 MB (200000000)
2   lstm-2               LSTM                 InputTypeRecurrent(250,timeSeriesLength=2000) [100, 250, 2000]     50000000      190.73 MB (200000000)
3   lstm-3               LSTM                 InputTypeRecurrent(250,timeSeriesLength=2000) [100, 250, 2000]     50000000      190.73 MB (200000000)
4   thoughtVector        LastTimeStepVertex   InputTypeFeedForward(250)                  [100, 250]           25000          97.66 KB (100000)
5   norm-1               BatchNormalization   InputTypeFeedForward(250)                  [100, 250]           25000          97.66 KB (100000)
6   dense-1              DenseLayer           InputTypeFeedForward(100)                  [100, 100]           10000          39.06 KB (40000)
7   norm-2               BatchNormalization   InputTypeFeedForward(100)                  [100, 100]           10000          39.06 KB (40000)
8   output               OutputLayer          InputTypeFeedForward(6)                    [100, 6]             600             2.34 KB (2400)
Total Activations Memory                 688.44 MB (721882400)
Total Activation Gradient Memory         688.44 MB (721880000)

----- Network Training Listeners -----
Number of Listeners                     3
Listener 0                              org.x.deeplearning.listener.ScorePrintListener@7b78ed6a
Listener 1                              ScoreIterationListener(10)
Listener 2                              org.x.deeplearning.listener.UIStatsListener@6fca5907
</code></pre>
","<gpu><cudnn><deeplearning4j>","2019-01-12 13:10:19","","0","4461907","2019-01-26 16:20:37","1","2","","","4461907","72","9","0"
"45603658","<p>I believe that deeplearning4j and R with exactly the same parameters should perform the same, comparable MSE. But I am not sure how to achieve that.</p>

<p>I have a csv file with the following format, which contains <code>46</code> variables and <code>2</code> outputs. Totally there are <code>1,0000</code> samples. All the data is normalized and the model is for regression analysis. </p>

<pre><code>S1  |  S2  |  ...  |  S46  |  X  |  Y
</code></pre>

<p>In R, I use <code>neuralnet</code> package, and the code is:</p>

<pre><code>rn &lt;- colnames(traindata)
f &lt;- as.formula(paste(""X + Y ~"", paste(rn[1:(length(rn)-2)], collapse=""+"")))
nn &lt;- neuralnet(f, 
                rep=1,
                data=traindata, 
                hidden=c(10), 
                linear.output=T,
                threshold = 0.5)
</code></pre>

<p>which is quite straightforward.</p>

<p>As I want to integrate the algorithm into Java project, so I consider dl4j to train the model. The trainset is exactly the same as that in R code. The test set is randomly selected. THe dl4j code is:</p>

<pre><code>MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(rngSeed) //include a random seed for reproducibility
            // use stochastic gradient descent as an optimization algorithm
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .iterations(100)
            .learningRate(0.0001) //specify the learning rate
            .updater(Updater.NESTEROVS).momentum(0.9) //specify the rate of change of the learning rate.
            .regularization(true).l2(0.0001)
            .list()
            .layer(0, new DenseLayer.Builder() //create the first, input layer with xavier initialization
                    .nIn(46)
                    .nOut(10)
                    .activation(Activation.TANH)
                    .weightInit(WeightInit.XAVIER)
                    .build())
            .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.MSE) //create hidden layer
                    .nIn(10)
                    .nOut(outputNum)
                    .activation(Activation.IDENTITY)
                    .build())
            .pretrain(false).backprop(true) //use backpropagation to adjust weights
            .build();
</code></pre>

<p>The number of epoch is 10 and batchsize is 128. </p>

<p>Using the test set, the performance of R is
<a href=""https://i.stack.imgur.com/o8CYV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o8CYV.png"" alt=""enter image description here""></a></p>

<p>and the performance of dl4j is the following, I think it does not work out its full potential.
<a href=""https://i.stack.imgur.com/lQqm7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQqm7.png"" alt=""""></a></p>

<p>The mornitor of dl4j is</p>

<p><a href=""https://i.stack.imgur.com/ykMXc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ykMXc.png"" alt=""""></a></p>

<p>As there is much more parameters in dl4j such as <code>updater</code>, <code>regulization</code> and <code>weightInit</code>. So I think some of the parameters are not properly set. BTW, why there are periodic thorns in the mornitor graph.</p>

<p>Can any one help?</p>
","<deeplearning4j>","2017-08-10 02:52:59","","1","7817561","2017-08-10 03:29:15","1","0","","","7817561","53","3","0"
"35502161","<p>I have a sentiment analysis program to predict whether a given movie review is positive or negative using recurrent neutral network. I'm using Deeplearning4j deep learning library for that program. Now I need to add that program to apache spark pipeline.</p>

<p>When doing it, I have a class <code>MovieReviewClassifier</code> which extends <code>org.apache.spark.ml.classification.ProbabilisticClassifier</code> and I have to add an instance of that class to the pipeline. The features which are needed to build the model are entered to the program using <code>setFeaturesCol(String s)</code> method. The features I add are in <code>String</code> format since they are a set of strings used for sentiment analysis. But the features should be in the form <code>org.apache.spark.mllib.linalg.VectorUDT</code>. Is there a way to convert the strings to Vector UDT?</p>

<p>I have attached my code for pipeline implementation below:</p>

<pre><code>public class RNNPipeline {
    final static String RESPONSE_VARIABLE =  ""s"";
    final static String INDEXED_RESPONSE_VARIABLE =  ""indexedClass"";
    final static String FEATURES = ""features"";
    final static String PREDICTION = ""prediction"";
    final static String PREDICTION_LABEL = ""predictionLabel"";

    public static void main(String[] args) {

        SparkConf sparkConf = new SparkConf();
        sparkConf.setAppName(""test-client"").setMaster(""local[2]"");
        sparkConf.set(""spark.driver.allowMultipleContexts"", ""true"");
        JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf);
        SQLContext sqlContext = new SQLContext(javaSparkContext);

        // ======================== Import data ====================================
        DataFrame dataFrame =    sqlContext.read().format(""com.databricks.spark.csv"")
                .option(""inferSchema"", ""true"")
                .option(""header"", ""true"")
                .load(""/home/RNN3/WordVec/training.csv"");

        // Split in to train/test data
        double [] dataSplitWeights = {0.7,0.3};
        DataFrame[] data = dataFrame.randomSplit(dataSplitWeights);



        // ======================== Preprocess ===========================



        // Encode labels
        StringIndexerModel labelIndexer = new StringIndexer().setInputCol(RESPONSE_VARIABLE)
                .setOutputCol(INDEXED_RESPONSE_VARIABLE)
                .fit(data[0]);


        // Convert indexed labels back to original labels (decode labels).
        IndexToString labelConverter = new IndexToString().setInputCol(PREDICTION)
                .setOutputCol(PREDICTION_LABEL)
                .setLabels(labelIndexer.labels());


        // ======================== Train ========================



        MovieReviewClassifier mrClassifier = new MovieReviewClassifier().setLabelCol(INDEXED_RESPONSE_VARIABLE).setFeaturesCol(""Review"");



        // Fit the pipeline for training..setLabelCol.setLabelCol.setLabelCol.setLabelCol
        Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] { labelIndexer, mrClassifier, labelConverter});
        PipelineModel pipelineModel = pipeline.fit(data[0]);

        }

  }
</code></pre>

<p>Review is the feature column which contains strings to be predicted as positive or negative. </p>

<p>I get the following error when I execute the code:</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: requirement failed: Column Review must be of type org.apache.spark.mllib.linalg.VectorUDT@f71b0bce but was actually StringType.
    at scala.Predef$.require(Predef.scala:233)
    at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
    at org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:50)
    at org.apache.spark.ml.Predictor.validateAndTransformSchema(Predictor.scala:71)
    at org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:116)
    at org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:167)
    at org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:167)
    at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
    at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
    at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108)
    at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:167)
    at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:62)
    at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:121)
    at RNNPipeline.main(RNNPipeline.java:82)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
</code></pre>
","<java><apache-spark><machine-learning><deeplearning4j>","2016-02-19 09:47:52","","1","5950143","2016-03-23 06:38:36","2","1","1","","5950143","131","40","0"
"44612333","<p><strong>i am try to run the deeplearning for java ""main"" java.lang.UnsatisfiedLinkError: no jniopenblas in java.library.path</strong></p>

<pre><code> Class Test{
    protected static final String[] allowedExtensions = 
    BaseImageLoader.ALLOWED_FORMATS;
    protected static int height = 20;
    protected static int width = 20;
    protected static int channels = 1;
    protected static int outputNum = 2;
    protected static final long seed = 123;
    protected static double rate = 0.006;





    protected static int epochs = 10;
    public static final Random randNumGen = new Random();
    private static Logger log = LoggerFactory.getLogger(Text2Saved.class);

    public static void main(String[] args) {

        File parentDir = new File(""./data/text2_test"");
        String modelfile = ""./data/text2-goodmodel.model"";
        System.out.println(modelfile);
            ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator();
        BalancedPathFilter pathFilter = new BalancedPathFilter(randNumGen, allowedExtensions, labelMaker);
        FileSplit filesInDir = new FileSplit(parentDir, allowedExtensions, randNumGen);

    // Split the image files into train and test.
        InputSplit[] filesInDirSplit = filesInDir.sample(pathFilter);
        InputSplit testData = filesInDirSplit[0];
        ImageRecordReader testReader = new ImageRecordReader(height, width, channels, labelMaker);
        System.out.println(""Number of records in Test: "" + testData.length());
        try {
            testReader.initialize(testData);

            MultiLayerNetwork model = ModelSerializer.restoreMultiLayerNetwork(new File(modelfile));

            DataSetIterator testIter = new RecordReaderDataSetIterator(testReader, 20, 1, outputNum);
            Evaluation eval = new Evaluation(outputNum);
            while (testIter.hasNext()) {
                DataSet next = testIter.next();
                INDArray output = model.output(next.getFeatureMatrix(), false);
                eval.eval(next.getLabels(), output);
            }
            System.out.println(eval.stats());
            log.info(eval.stats());

        } catch (IOException e) {
            e.printStackTrace();
        }
    }
</code></pre>

<p><strong>Exception details is</strong></p>

<blockquote>
  <p>Exception in thread ""main"" java.lang.UnsatisfiedLinkError: no jniopenblas in java.library.path</p>
</blockquote>

<pre><code>at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
    at java.lang.Runtime.loadLibrary0(Runtime.java:870)
    at java.lang.System.loadLibrary(System.java:1122)
    at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:945)
    at org.bytedeco.javacpp.Loader.load(Loader.java:750)
    at org.bytedeco.javacpp.Loader.load(Loader.java:657)
    at org.bytedeco.javacpp.openblas.&lt;clinit&gt;(openblas.java:10)
    at org.nd4j.linalg.cpu.nativecpu.blas.CpuBlas.setMaxThreads(CpuBlas.java:87)
    at org.nd4j.nativeblas.Nd4jBlas.&lt;init&gt;(Nd4jBlas.java:36)
    at org.nd4j.linalg.cpu.nativecpu.blas.CpuBlas.&lt;init&gt;(CpuBlas.java:11)
    at org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.createBlas(CpuNDArrayFactory.java:79)
    at org.nd4j.linalg.factory.BaseNDArrayFactory.blas(BaseNDArrayFactory.java:71)
    at org.nd4j.linalg.cpu.nativecpu.blas.CpuLevel3.&lt;init&gt;(CpuLevel3.java:26)
    at org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.createLevel3(CpuNDArrayFactory.java:94)
    at org.nd4j.linalg.factory.BaseNDArrayFactory.level3(BaseNDArrayFactory.java:92)
    at org.nd4j.linalg.factory.BaseBlasWrapper.level3(BaseBlasWrapper.java:42)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.mmuli(BaseNDArray.java:2849)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.mmul(BaseNDArray.java:2643)
    at org.deeplearning4j.nn.layers.BaseLayer.preOutput(BaseLayer.java:373)
    at org.deeplearning4j.nn.layers.BaseLayer.activate(BaseLayer.java:384)
    at org.deeplearning4j.nn.layers.BaseLayer.activate(BaseLayer.java:405)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:590)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:713)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:667)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:658)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.output(MultiLayerNetwork.java:1541)
    at org.woolfel.robottag.Text2Saved.main(Text2Saved.java:60)
</code></pre>
","<java><deeplearning4j>","2017-06-18 06:27:42","","0","7875196","2017-06-18 06:27:42","0","6","","","7875196","15","52","0"
"42343922","<p>I have trained <a href=""https://github.com/shelhamer/fcn.berkeleyvision.org"" rel=""nofollow noreferrer"">FCN32</a> for semantic segmentation from scratch for my data, and I got the following output:
<a href=""https://github.com/shelhamer/fcn.berkeleyvision.org"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NHvKi.png"" alt=""enter image description here""></a></p>

<p>As it can be seen, this is not a good learning curve showing an improper training on data. 
<code>solver</code> is as follows: </p>

<pre><code>net: ""train_val.prototxt""
#test_net: ""val.prototxt""
test_iter: 5105 #736
# make test net, but don't invoke it from the solver itself
test_interval: 1000000 #20000
display: 50
average_loss: 50
lr_policy: ""step"" #""fixed""
stepsize: 50000 #+
gamma: 0.1  #+
# lr for unnormalized softmax
base_lr: 1e-10 
# high momentum
momentum: 0.99
# no gradient accumulation
iter_size: 1
max_iter: 600000
weight_decay: 0.0005
snapshot: 30000
snapshot_prefix: ""snapshot/FCN32s_CNN1""
test_initialization: false
solver_mode: GPU
</code></pre>

<p>after changing the learning rate to 0.001, it became worse:
<a href=""https://i.stack.imgur.com/wllM3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wllM3.png"" alt=""enter image description here""></a>
I am wondering what can I do for improving the training? Thanks </p>
","<deep-learning><caffe><pycaffe><deeplearning4j>","2017-02-20 11:55:42","","0","6494707","2017-02-21 02:56:23","1","0","","","6494707","715","63","1"
"44255411","<p>what is the difference between R-CNN, fast R-CNN, faster R-CNN and YOLO in terms of the following:
(1) Precision on same image set
(2) Given SAME IMAGE SIZE, the run time
(3) Support for android porting</p>

<p>Considering these three criteria which is the best object localization technique? </p>
","<deep-learning><deeplearning4j>","2017-05-30 06:51:23","","4","6680821","2017-11-09 14:01:29","2","2","1","","6680821","352","29","1"
"54019421","<p>Is it possible to ""freeze"" one or several specific neurons, or - even better for me - one or several specific weights? I assume that I have to use <a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-nn-transfer-learning"" rel=""nofollow noreferrer"">Transfer Learning</a> functionality for this purpose, but I only understood how to freeze the whole layer and not a specific neuron or weight in some layer...</p>
","<java><deeplearning4j>","2019-01-03 09:24:57","","0","1520291","2019-01-03 09:24:57","0","0","","","1520291","1145","184","29"
"43927433","<p>I'm trying to propose a patch to deeplearning4j, but first I need to be able to build the project. I'm able to build it from maven using the manual instructions, but IntelliJ (2016.3.6) is finding errors, and when I look at the source code, I don't blame it.</p>

<p>The source file I'm specifically stumped by is <a href=""https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/word2vec/StaticWord2Vec.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/word2vec/StaticWord2Vec.java</a>, which has a couple references to a variable <code>log</code> that's not declared in this file. </p>

<pre><code>package org.deeplearning4j.models.word2vec;

import lombok.extern.slf4j.Slf4j;
import org.deeplearning4j.models.embeddings.WeightLookupTable;
import org.deeplearning4j.models.embeddings.reader.ModelUtils;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.deeplearning4j.models.word2vec.wordstore.VocabCache;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.compression.AbstractStorage;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

/**
 * This is special limited Word2Vec implementation, suited for serving as lookup table in concurrent multi-gpu environment
 * This implementation DOES NOT load all vectors onto any of gpus, instead of that it holds vectors in, optionally, compressed state in host memory.
 * This implementation DOES NOT provide some of original Word2Vec methods, such as wordsNearest or wordsNearestSum.
 *
 * @author raver119@gmail.com
 */
@Slf4j
public class StaticWord2Vec implements WordVectors {
    private List&lt;Map&lt;Integer, INDArray&gt;&gt; cacheWrtDevice = new ArrayList&lt;&gt;();
    private AbstractStorage&lt;Integer&gt; storage;
    private long cachePerDevice = 0L;
    private VocabCache&lt;VocabWord&gt; vocabCache;
    private String unk = null;
 ... snipped
</code></pre>

<p>The class extends an interface, but does not explicitly extend a parent class. Inspecting the class file generated by Maven using javap, I see:</p>

<pre><code>Compiled from ""StaticWord2Vec.java""
public class org.deeplearning4j.models.word2vec.StaticWord2Vec 
implements org.deeplearning4j.models.embeddings.wordvectors.WordVectors {
private static final org.slf4j.Logger log;
... snipped
</code></pre>
","<java><intellij-idea><slf4j><lombok><deeplearning4j>","2017-05-12 00:07:40","","1","634551","2017-05-12 18:20:35","1","2","","","634551","370","364","4"
"54020352","<p>I am building a recurrent neural network with deeplearning4j and I need to create the training and test data sets.</p>

<p>All the examples provided in <a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-nn-recurrent"" rel=""nofollow noreferrer"">the documentation</a> and the <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/seqclassification/UCISequenceClassificationExample.java"" rel=""nofollow noreferrer"">example code</a>, use a <code>CSVSequenceRecordReader</code> to read CSV files.</p>

<p>Then a <code>DataSetIterator</code> is created with the <code>SequenceRecordReaderDataSetIterator</code> constructor and fed into the <code>MultiLayerNetwork.fit()</code> or the <code>MultiLayerNetwork.evaluate()</code> method (depending if it's a training or test data set iterator).</p>

<p>However, in my case, the data set I have is not stored in a CSV file. I access it online through a third-party library, pre-process it to obtain a <code>List&lt;Data&gt;</code> and a <code>List&lt;Labels&gt;</code> objects.</p>

<p>How can I:</p>

<p>1) create the <code>DataSetIterator</code> from my two lists?</p>

<p>2) split the <code>DataSetIterator</code> in a training set and a test set?</p>

<p><strong>Edit</strong>:</p>

<p>I think my question is too broad. Let me try to narrow it down.</p>

<p>I have started to read <a href=""https://progur.com/2017/06/how-to-create-lstm-rnn-deeplearning4j.html"" rel=""nofollow noreferrer"">this article</a> which uses a very simple approach to create a data set:</p>

<p>It creates two INDArrays and builds a DataSet from them using the <code>DataSet(INDArray first, INDArray second)</code> constructor.</p>

<p>Training the data works using <code>network.fit(dataSet);</code>, but I can't evaluate it while training, as the method <code>evaluate</code> requires an data set iterator, not a data set.</p>

<p>Moreover, from what I understand, using this approach also means that there is only one huge data set, no mini batches.</p>

<p>I also guess that I could create mini batches from this big data set by using the   <code>batchBy(int num)</code> method. But this method returns a list of data sets, and not an data set iterator... iterateWithMiniBatches() does return a data set iterator but when I looked at the source file, it returns null and is deprecated. Then I tried to see if there is an implementation of the DataSetIterator I could use, but there are a lot of them. I tried the BaseDataSetIterator but it does not take a DataSet as constructor parameter but a DataSetFetcher... Yet another layer.</p>

<p>Is there somewhere an example that shows how to create a data set without using the default record readers? Or should I just create my how implementation of a record reader?</p>
","<deeplearning4j>","2019-01-03 10:23:17","","1","674552","2019-01-11 16:56:18","1","0","","","674552","1805","121","4"
"53256447","<p>For the past week or so, I have been trying to get a neural network to function using RGB images, but no matter what I do it seems to only be predicting one class.
I have read all the links I could find with people encountering this problem and experimented with a lot of different things, but it always ends up predicting only one out of the two output classes. I have checked the batches going in to the model, I have increased the size of the dataset, I have increased the original pixel size(28*28) to 56*56, increased epochs, done a lot of model tuning and I have even tried a simple non-convolutional neural network as well as dumbing down my own CNN model, yet it changes nothing.</p>

<p>I have also checked into the structure of how the data is passed in for the training set(specifically imageRecordReader), but this input structure(in terms of folder structure and how the data is passed into the training set) works perfectly when given gray-scale images(as it originally was created with a 99% accuracy on the MNIST dataset). </p>

<p>Some context: I use the following folder names as my labels, i.e folder(0), folder(1) for both training and testing data as there will only be two output classes. The training set contains 320 images of class 0 and 240 images of class 1, whereas the testing set is made up of 79 and 80 images respectively. </p>

<p>Code below:</p>

<pre><code>private static final Logger log = LoggerFactory.getLogger(MnistClassifier.class);
private static final String basePath = System.getProperty(""java.io.tmpdir"") + ""/ISIC-Images"";

public static void main(String[] args) throws Exception {
    int height = 56;
    int width = 56;
    int channels = 3; // RGB Images
    int outputNum = 2; // 2 digit classification
    int batchSize = 1;
    int nEpochs = 1;
    int iterations = 1;
    int seed = 1234;
    Random randNumGen = new Random(seed);

    // vectorization of training data
    File trainData = new File(basePath + ""/Training"");
    FileSplit trainSplit = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, randNumGen);
    ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator(); // parent path as the image label
    ImageRecordReader trainRR = new ImageRecordReader(height, width, channels, labelMaker);
    trainRR.initialize(trainSplit);
    DataSetIterator trainIter = new RecordReaderDataSetIterator(trainRR, batchSize, 1, outputNum);

    // vectorization of testing data
    File testData = new File(basePath + ""/Testing"");
    FileSplit testSplit = new FileSplit(testData, NativeImageLoader.ALLOWED_FORMATS, randNumGen);
    ImageRecordReader testRR = new ImageRecordReader(height, width, channels, labelMaker);
    testRR.initialize(testSplit);
    DataSetIterator testIter = new RecordReaderDataSetIterator(testRR, batchSize, 1, outputNum);

    log.info(""Network configuration and training..."");
    Map&lt;Integer, Double&gt; lrSchedule = new HashMap&lt;&gt;();
    lrSchedule.put(0, 0.06); // iteration #, learning rate
    lrSchedule.put(200, 0.05);
    lrSchedule.put(600, 0.028);
    lrSchedule.put(800, 0.0060);
    lrSchedule.put(1000, 0.001);

    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
        .seed(seed)
        .l2(0.0008)
        .updater(new Nesterovs(new MapSchedule(ScheduleType.ITERATION, lrSchedule)))
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .weightInit(WeightInit.XAVIER)
        .list()
        .layer(0, new ConvolutionLayer.Builder(5, 5)
            .nIn(channels)
            .stride(1, 1)
            .nOut(20)
            .activation(Activation.IDENTITY)
            .build())
        .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)
            .kernelSize(2, 2)
            .stride(2, 2)
            .build())
        .layer(2, new ConvolutionLayer.Builder(5, 5)
            .stride(1, 1)
            .nOut(50)
            .activation(Activation.IDENTITY)
            .build())
        .layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)
            .kernelSize(2, 2)
            .stride(2, 2)
            .build())
        .layer(4, new DenseLayer.Builder().activation(Activation.RELU)
            .nOut(500).build())
        .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.SQUARED_LOSS)
            .nOut(outputNum)
            .activation(Activation.SOFTMAX)
            .build())
        .setInputType(InputType.convolutionalFlat(56, 56, 3)) // InputType.convolutional for normal image
        .backprop(true).pretrain(false).build();

    MultiLayerNetwork net = new MultiLayerNetwork(conf);
    net.init();
    net.setListeners(new ScoreIterationListener(10));
    log.debug(""Total num of params: {}"", net.numParams());

    // evaluation while training (the score should go down)
    for (int i = 0; i &lt; nEpochs; i++) {
        net.fit(trainIter);
        log.info(""Completed epoch {}"", i);
        Evaluation eval = net.evaluate(testIter);
        log.info(eval.stats());
        trainIter.reset();
        testIter.reset();
    }
    ModelSerializer.writeModel(net, new File(basePath + ""/Isic.model.zip""), true);
}
</code></pre>

<p>Output from running the model:</p>

<p><a href=""https://i.stack.imgur.com/haYCh.png"" rel=""nofollow noreferrer"">Odd iteration scores</a></p>

<p><a href=""https://i.stack.imgur.com/aoZzh.png"" rel=""nofollow noreferrer"">Evaluation metrics</a></p>

<p>Any insight would be much appreciated.</p>
","<java><deep-learning><conv-neural-network><rgb><deeplearning4j>","2018-11-12 05:39:33","","0","6427828","2018-12-30 19:59:16","3","0","","","6427828","6","0","0"
"53166784","<p>Recently i started to learn DeepLearning4j library , so i cloned a dl4j project and executed in eclipse.</p>

<p>After executing , it showed following error:</p>

<pre><code>Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.
</code></pre>

<p>I checked my windows path variables and i could not find any variable of name  MKL.</p>

<p>Following are the dependency that i used in the pom.xml :</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
        &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
        &lt;version&gt;0.8.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
        &lt;version&gt;0.8.0&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
","<java><deeplearning4j><dl4j>","2018-11-06 06:32:29","","0","4120631","2018-11-06 07:08:36","1","0","","","4120631","3764","54","1"
"35689755","<p>I'm using the ParagraphVector tool in DeepLearning4j framework. What I'm doing is training a model on a set of text documents and then calculating the similarity between those documents.</p>

<p>Now, as the reference page (<a href=""http://deeplearning4j.org/word2vec"" rel=""nofollow"">http://deeplearning4j.org/word2vec</a>) says, the metric used by the tool to calculate similarity is cosine similarity, which should be included between 0 and 1. However, for some pair of documents, I get negative scores. </p>

<p>Can anybody tell why is that?</p>

<p>Thank you in advance.</p>
","<machine-learning><information-retrieval><cosine-similarity><word2vec><deeplearning4j>","2016-02-28 23:41:58","","0","5995017","2016-03-15 13:16:37","1","0","","","5995017","21","1","0"
"53695888","<p>first of all thank you for taking the time to help me. My name is Matt and I've been trying to learn java, as well as creating mc plugins. I've been trying to make a spigot plugin(for MC) that has deep learning in it. I am trying to use deeplearning4j. I know I could put this question on the spigotmc website but I don't think they would be very helpful, as from looking at previous posts on spigotmc it doesn't look like many of them have a lot of knowledge on dl4j. So what I have done so far is use the quickstart guide on dl4j's website to setup maven and Intellij and create a maven project within intellij and add the dependencies for deeplearning4j, here is my POM.xml:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0""
     xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
     xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0         http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

&lt;groupId&gt;com.gmail.mateo.lack&lt;/groupId&gt;
&lt;artifactId&gt;SNAC5&lt;/artifactId&gt;
&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;spigot-repo&lt;/id&gt;
        &lt;url&gt;https://hub.spigotmc.org/nexus/content/repositories/snapshots/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
&lt;properties&gt;

    &lt;dl4j.version&gt;1.0.0-beta3&lt;/dl4j.version&gt;

    &lt;nd4j.version&gt;1.0.0-beta3&lt;/nd4j.version&gt;

    &lt;logback.version&gt;1.2.3&lt;/logback.version&gt;

    &lt;java.version&gt;1.8&lt;/java.version&gt;

    &lt;maven-shade-plugin.version&gt;2.4.3&lt;/maven-shade-plugin.version&gt;

&lt;/properties&gt;
&lt;dependencies&gt;
    &lt;!--Spigot API--&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.gmail.mateo.lack&lt;/groupId&gt;
        &lt;artifactId&gt;spigot&lt;/artifactId&gt;
        &lt;version&gt;1.0&lt;/version&gt;
        &lt;scope&gt;system&lt;/scope&gt;
        &lt;systemPath&gt;C:/Users/mlavatar/Downloads/spigot-1.8.8-R0.1-SNAPSHOT-latest.jar&lt;/systemPath&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
        &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
        &lt;version&gt;1.0.0-beta3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
        &lt;version&gt;1.0.0-beta3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.datavec&lt;/groupId&gt;
        &lt;artifactId&gt;datavec-hadoop&lt;/artifactId&gt;
        &lt;version&gt;1.0.0-beta3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.datavec&lt;/groupId&gt;
        &lt;artifactId&gt;datavec-api&lt;/artifactId&gt;
        &lt;version&gt;1.0.0-beta3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.gmail.mateo.lack&lt;/groupId&gt;
        &lt;artifactId&gt;ShatteredStaff&lt;/artifactId&gt;
        &lt;version&gt;1.0&lt;/version&gt;
        &lt;scope&gt;system&lt;/scope&gt;
        &lt;systemPath&gt;C:/Users/mlavatar/Desktop/MyPlugins/ShatteredStaff.jar&lt;/systemPath&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
&lt;build&gt;

    &lt;plugins&gt;

        &lt;!-- Maven compiler plugin: compile for Java 8 --&gt;

        &lt;plugin&gt;

            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;

            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;

            &lt;version&gt;3.5.1&lt;/version&gt;

            &lt;configuration&gt;

                &lt;source&gt;1.8&lt;/source&gt;

                &lt;target&gt;1.8&lt;/target&gt;

            &lt;/configuration&gt;

        &lt;/plugin&gt;





        &lt;!--

        Maven shade plugin configuration: this is required so that if you build a single JAR file (an ""uber-jar"")

        it will contain all the required native libraries, and the backends will work correctly.

        Used for example when running the following commants



        mvn package

        cd target

        java -cp deeplearning4j-examples-1.0.0-beta-bin.jar org.deeplearning4j.LenetMnistExample

        --&gt;

        &lt;plugin&gt;

            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;

            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;

            &lt;version&gt;2.4.3&lt;/version&gt;

            &lt;configuration&gt;

                &lt;shadedArtifactAttached&gt;true&lt;/shadedArtifactAttached&gt;

                &lt;shadedClassifierName&gt;bin&lt;/shadedClassifierName&gt;

                &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt;

                &lt;filters&gt;

                    &lt;filter&gt;

                        &lt;artifact&gt;*:*&lt;/artifact&gt;

                        &lt;excludes&gt;

                            &lt;exclude&gt;org/datanucleus/**&lt;/exclude&gt;

                            &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;

                            &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;

                            &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;

                        &lt;/excludes&gt;

                    &lt;/filter&gt;

                &lt;/filters&gt;

            &lt;/configuration&gt;



            &lt;executions&gt;

                &lt;execution&gt;

                    &lt;phase&gt;package&lt;/phase&gt;

                    &lt;goals&gt;

                        &lt;goal&gt;shade&lt;/goal&gt;

                    &lt;/goals&gt;

                    &lt;configuration&gt;

                        &lt;transformers&gt;

                            &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer""&gt;

                                &lt;resource&gt;reference.conf&lt;/resource&gt;

                            &lt;/transformer&gt;

                            &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.ServicesResourceTransformer""/&gt;

                            &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer""&gt;

                            &lt;/transformer&gt;

                        &lt;/transformers&gt;

                    &lt;/configuration&gt;

                &lt;/execution&gt;

            &lt;/executions&gt;

        &lt;/plugin&gt;

    &lt;/plugins&gt;

&lt;/build&gt;
&lt;/project&gt;
</code></pre>

<p>However, the problem is, once I export it (by going to right, maven, lifecycle, package) and I try out the jar in the plugins folder I get this error:</p>

<pre><code>09.12 13:58:02 [Server] ERROR Could not load 'plugins/SAC.jar' in folder 'plugins'
09.12 13:58:02 [Server] INFO org.bukkit.plugin.InvalidPluginException: java.lang.NoClassDefFoundError: org/nd4j/linalg/schedule/ISchedule
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.java.JavaPluginLoader.loadPlugin(JavaPluginLoader.java:133) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.SimplePluginManager.loadPlugin(SimplePluginManager.java:331) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.SimplePluginManager.loadPlugins(SimplePluginManager.java:254) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at org.bukkit.craftbukkit.v1_8_R3.CraftServer.loadPlugins(CraftServer.java:293) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at net.minecraft.server.v1_8_R3.DedicatedServer.init(DedicatedServer.java:202) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at net.minecraft.server.v1_8_R3.MinecraftServer.run(MinecraftServer.java:563) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]
09.12 13:58:02 [Server] INFO Caused by: java.lang.NoClassDefFoundError: org/nd4j/linalg/schedule/ISchedule
09.12 13:58:02 [Server] INFO at SAC.sac.&lt;init&gt;(sac.java:48) ~[?:?]
09.12 13:58:02 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.java.PluginClassLoader.&lt;init&gt;(PluginClassLoader.java:77) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.java.JavaPluginLoader.loadPlugin(JavaPluginLoader.java:129) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO ... 6 more
09.12 13:58:02 [Server] INFO Caused by: java.lang.ClassNotFoundException: org.nd4j.linalg.schedule.ISchedule
09.12 13:58:02 [Server] INFO at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.java.PluginClassLoader.findClass(PluginClassLoader.java:102) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.java.PluginClassLoader.findClass(PluginClassLoader.java:87) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at SAC.sac.&lt;init&gt;(sac.java:48) ~[?:?]
09.12 13:58:02 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_191]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.java.PluginClassLoader.&lt;init&gt;(PluginClassLoader.java:77) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 13:58:02 [Server] INFO at org.bukkit.plugin.java.JavaPluginLoader.loadPlugin(JavaPluginLoader.java:129) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
</code></pre>

<p>I have tried to google this error and nothing useful has come up, I've searched this site for answers, and most of them point me back to the quickstart guide which I followed in the first place. Help would be very much appreciated.</p>

<p>EDIT: <strong>After using an Uberjar I get this error:</strong></p>

<pre><code>ERROR Could not load 'plugins/SAC.jar' in folder 'plugins'
09.12 18:17:31 [Server] INFO org.bukkit.plugin.InvalidPluginException: java.lang.ExceptionInInitializerError
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.java.JavaPluginLoader.loadPlugin(JavaPluginLoader.java:133) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.SimplePluginManager.loadPlugin(SimplePluginManager.java:331) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.SimplePluginManager.loadPlugins(SimplePluginManager.java:254) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at org.bukkit.craftbukkit.v1_8_R3.CraftServer.loadPlugins(CraftServer.java:293) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at net.minecraft.server.v1_8_R3.DedicatedServer.init(DedicatedServer.java:202) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at net.minecraft.server.v1_8_R3.MinecraftServer.run(MinecraftServer.java:563) [paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at java.lang.Thread.run(Thread.java:748) [?:1.8.0_191]
09.12 18:17:31 [Server] INFO Caused by: java.lang.ExceptionInInitializerError
09.12 18:17:31 [Server] INFO at org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.seed(NeuralNetConfiguration.java:683) ~[?:?]
09.12 18:17:31 [Server] INFO at KillAura.kMain.&lt;init&gt;(kMain.java:118) ~[?:?]
09.12 18:17:31 [Server] INFO at SAC.sac.&lt;init&gt;(sac.java:48) ~[?:?]
09.12 18:17:31 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.java.PluginClassLoader.&lt;init&gt;(PluginClassLoader.java:77) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.java.JavaPluginLoader.loadPlugin(JavaPluginLoader.java:129) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO ... 6 more
09.12 18:17:31 [Server] INFO Caused by: java.lang.RuntimeException: org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: Please ensure that you have an nd4j backend on your classpath. Please see: http://nd4j.org/getstarted.html
09.12 18:17:31 [Server] INFO at org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:5484) ~[?:?]
09.12 18:17:31 [Server] INFO at org.nd4j.linalg.factory.Nd4j.&lt;clinit&gt;(Nd4j.java:215) ~[?:?]
09.12 18:17:31 [Server] INFO at org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.seed(NeuralNetConfiguration.java:683) ~[?:?]
09.12 18:17:31 [Server] INFO at KillAura.kMain.&lt;init&gt;(kMain.java:118) ~[?:?]
09.12 18:17:31 [Server] INFO at SAC.sac.&lt;init&gt;(sac.java:48) ~[?:?]
09.12 18:17:31 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.java.PluginClassLoader.&lt;init&gt;(PluginClassLoader.java:77) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.java.JavaPluginLoader.loadPlugin(JavaPluginLoader.java:129) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO ... 6 more
09.12 18:17:31 [Server] INFO Caused by: org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: Please ensure that you have an nd4j backend on your classpath. Please see: http://nd4j.org/getstarted.html
09.12 18:17:31 [Server] INFO at org.nd4j.linalg.factory.Nd4jBackend.load(Nd4jBackend.java:213) ~[?:?]
09.12 18:17:31 [Server] INFO at org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:5481) ~[?:?]
09.12 18:17:31 [Server] INFO at org.nd4j.linalg.factory.Nd4j.&lt;clinit&gt;(Nd4j.java:215) ~[?:?]
09.12 18:17:31 [Server] INFO at org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.seed(NeuralNetConfiguration.java:683) ~[?:?]
09.12 18:17:31 [Server] INFO at KillAura.kMain.&lt;init&gt;(kMain.java:118) ~[?:?]
09.12 18:17:31 [Server] INFO at SAC.sac.&lt;init&gt;(sac.java:48) ~[?:?]
09.12 18:17:31 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at java.lang.Class.newInstance(Class.java:442) ~[?:1.8.0_191]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.java.PluginClassLoader.&lt;init&gt;(PluginClassLoader.java:77) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d""]
09.12 18:17:31 [Server] INFO at org.bukkit.plugin.java.JavaPluginLoader.loadPlugin(JavaPluginLoader.java:129) ~[paper-1.8.8.jar:git-PaperSpigot-""4c7641d
</code></pre>

<p>EDIT: <strong>Log after building</strong></p>

<p>x86_64:3.4.3-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica-platform:jar:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:android-arm:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:android-arm64:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:android-x86:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:android-x86_64:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:linux-x86:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:linux-x86_64:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:linux-armhf:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:linux-ppc64le:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:macosx-x86_64:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:windows-x86:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.bytedeco.javacpp-presets:leptonica:jar:windows-x86_64:1.76.0-1.4.3 in the shaded jar.
    [INFO] Including org.deeplearning4j:deeplearning4j-ui-components:jar:1.0.0-beta3 in the shaded jar.
    [INFO] Including org.datavec:datavec-hadoop:jar:1.0.0-beta3 in the shaded jar.
    [INFO] Including com.sun.xml.bind:jaxb-core:jar:2.2.11 in the shaded jar.
    [INFO] Including com.sun.xml.bind:jaxb-impl:jar:2.2.11 in the shaded jar.
    [INFO] Including io.netty:netty:jar:3.10.4.Final in the shaded jar.
    [INFO] Including org.apache.zookeeper:zookeeper:jar:3.4.6 in the shaded jar.
    [INFO] Including jline:jline:jar:0.9.94 in the shaded jar.
    [INFO] Including junit:junit:jar:3.8.1 in the shaded jar.
    [INFO] Including org.datavec:datavec-api:jar:1.0.0-beta3 in the shaded jar.
    [INFO] Including org.jetbrains:annotations:jar:13.0 in the shaded jar.
    [INFO] Including commons-codec:commons-codec:jar:1.10 in the shaded jar.
    [INFO] Including joda-time:joda-time:jar:2.2 in the shaded jar.
    [INFO] Including org.yaml:snakeyaml:jar:1.12 in the shaded jar.
    [INFO] Including org.freemarker:freemarker:jar:2.3.23 in the shaded jar.
    [INFO] Including org.nd4j:nd4j-common:jar:1.0.0-beta3 in the shaded jar.
    [INFO] Including com.clearspring.analytics:stream:jar:2.7.0 in the shaded jar.
    [INFO] Including net.sf.opencsv:opencsv:jar:2.3 in the shaded jar.
    [INFO] Including com.tdunning:t-digest:jar:3.2 in the shaded jar.
    [INFO] Including it.unimi.dsi:fastutil:jar:6.5.7 in the shaded jar.
    [INFO] Including ch.qos.logback:logback-classic:jar:1.2.3 in the shaded jar.
    [INFO] Including ch.qos.logback:logback-core:jar:1.2.3 in the shaded jar.
    [WARNING] nd4j-base64-1.0.0-beta3.jar, nd4j-api-1.0.0-beta3.jar define 1 overlapping classes: 
    [WARNING]   - org.nd4j.serde.base64.Nd4jBase64
    [WARNING] maven-shade-plugin has detected that some class files are
    [WARNING] present in two or more JARs. When this happens, only one
    [WARNING] single version of the class is copied to the uber jar.
    [WARNING] Usually this is not harmful and you can skip these warnings,
    [WARNING] otherwise try to manually exclude artifacts based on
    [WARNING] mvn dependency:tree -Ddetail=true and the above output.
    [WARNING] See <a href=""http://maven.apache.org/plugins/maven-shade-plugin/"" rel=""nofollow noreferrer"">http://maven.apache.org/plugins/maven-shade-plugin/</a>
    [INFO] Attaching shaded artifact.
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 01:44 min
    [INFO] Finished at: 2018-12-10T21:13:33-05:00
    [INFO] Final Memory: 43M/447M
    [INFO] ------------------------------------------------------------------------</p>

<p>Theres a little more at the beginning but I can't fit it in here.</p>
","<java><deeplearning4j>","2018-12-09 19:28:15","","1","10214280","2018-12-11 02:52:12","1","0","","","10214280","6","0","0"
"35545819","<p>I've been trying to run the example code provided by DL4J for the stacked denoising autoencoder. However, my results have been really bad:</p>

<pre><code>Warning: class 0 was never predicted by the model. This class was excluded    from the average precision
Warning: class 1 was never predicted by the model. This class was excluded from the average precision
Warning: class 2 was never predicted by the model. This class was excluded from the average precision
Warning: class 3 was never predicted by the model. This class was excluded from the average precision
Warning: class 4 was never predicted by the model. This class was excluded from the average precision
Warning: class 5 was never predicted by the model. This class was excluded from the average precision
Warning: class 6 was never predicted by the model. This class was excluded from the average precision
Warning: class 7 was never predicted by the model. This class was excluded from the average precision
Warning: class 9 was never predicted by the model. This class was excluded from the average precision

==========================Scores========================================
 Accuracy:  0.0944
 Precision: 0.0944
 Recall:    0.1
 F1 Score:  0.0971
</code></pre>

<p>I'm not exactly sure what could be causing the results to be that bad. I'm using the MNIST dataset, and the code for the stacked denoising autoencoder was provided by DL4J. My code is below:</p>

<pre><code>/**
 * Created by chris on 1/31/16.
 * Import statements above
 */
public class stackedDenoisingAutoencoderExample {

    private static Logger log = LoggerFactory.getLogger(stackedDenoisingAutoencoderExample.class);

    public static void main(String[] args) throws Exception {
        final int numRows = 28;
        final int numColumns = 28;
        int outputNum = 10;
        int numSamples = 60000;
        int batchSize = 100;
        int iterations = 10;
        int seed = 123;
        int listenerFreq = batchSize / 5;

        log.info(""Load data...."");
        DataSetIterator iter = new MnistDataSetIterator(batchSize,numSamples,true);

        log.info(""Build model...."");

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(seed)
  .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)
            .gradientNormalizationThreshold(1.0)
            .iterations(iterations)
            .momentum(0.5)
            .momentumAfter(Collections.singletonMap(3, 0.9))
            .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)
            .list(4)
            .layer(0, new AutoEncoder.Builder().nIn(numRows * numColumns).nOut(500)
                    .weightInit(WeightInit.XAVIER).lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                    .corruptionLevel(0.3)
                    .build())
            .layer(1, new AutoEncoder.Builder().nIn(500).nOut(250)
                    .weightInit(WeightInit.XAVIER).lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                    .corruptionLevel(0.3)

                    .build())
            .layer(2, new AutoEncoder.Builder().nIn(250).nOut(200)
                    .weightInit(WeightInit.XAVIER).lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                    .corruptionLevel(0.3)
                    .build())
            .layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).activation(""softmax"")
                    .nIn(200).nOut(outputNum).build())
            .pretrain(true).backprop(false)
            .build();

        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();
        model.setListeners(Arrays.asList((IterationListener) new ScoreIterationListener(listenerFreq)));

        log.info(""Train model...."");
        model.fit(iter); // achieves end to end pre-training

        log.info(""Evaluate model...."");
        Evaluation eval = new Evaluation(outputNum);

        DataSetIterator testIter = new MnistDataSetIterator(100,10000);
        while(testIter.hasNext()) {
            DataSet testMnist = testIter.next();
            INDArray predict2 = model.output(testMnist.getFeatureMatrix());
            eval.eval(testMnist.getLabels(), predict2);
        }

        log.info(eval.stats());
        log.info(""****************Example finished********************"");

    }
}
</code></pre>

<p>Thanks!</p>
","<java><machine-learning><neural-network><deep-learning><deeplearning4j>","2016-02-22 04:26:58","","0","4688258","2018-06-11 10:59:26","1","1","","","4688258","1","0","0"
"53606149","<p>I am trying to make a Convolutional Neural Network using Weka's DeepLearning4j package. This is to recognize 1-D numerical vectors with nominal labels at the end. It represents network traffic for cyber attacks.
I get this input error message<br><br><a href=""https://i.stack.imgur.com/VoGsD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VoGsD.png"" alt=""this input error message""></a></p>

<p>I know there is a way to handle this in Java using .setInputTypes(InputType.convolutional(60, 160, 1)) but I don't know how to set the input type in the Weka GUI.</p>

<p>I am loading 1-D numerical vectors with nominal labels at the end. I am loading them with the DefaultInstanceIterator.</p>

<p>Any help would be greatly appreciated.</p>

<p>Sincerely,</p>

<p>Ahsen Husain</p>
","<conv-neural-network><weka><deeplearning4j>","2018-12-04 05:21:30","","0","9811630","2018-12-04 07:36:34","0","0","","","9811630","1","0","0"
"41779503","<p>I have just followed the steps given on: <a href=""https://deeplearning4j.org/quickstart"" rel=""nofollow noreferrer"">https://deeplearning4j.org/quickstart</a> (cloning it from github, executing mvn clean install, import the project into IntelliJ) and I was able to run the examples in IntelliJ.</p>

<p>However, I am a bit unsure if these steps are enough for me to be able to use DL4J libraries outside of the examples (And How I would be able to do it?). </p>

<p>If I would want to use DL4J libraries in my own project, would it be enough just to create a new maven project and add the dependency for DL4J in the POM.XML file? If that is the case, could you please give me an example?</p>

<p>Or the guide I followed above is just for the examples and not for the actual libraries?</p>

<p>If that is the case, if anyone has the time, could you give me a detailed explanation on how I can install the DL4J libraries and everything that is required to run it smoothly (and how I would be able to use the library in the future with an example?)?</p>

<p>I am running Windows 10, JDK 1.8.0.65 64 bit, Maven 3.3.9, IntelliJ.</p>

<p>I'm sorry if my question seems stupid and basic, but I got to struggle with the installation.</p>

<p>Thank you for your time to read my issue!</p>
","<java><git><maven><deeplearning4j><dl4j>","2017-01-21 12:43:44","","3","6504741","2018-06-29 13:23:39","1","3","","","6504741","23","1","0"
"42708534","<p>I'm working on a project using DeepLearning4J, Maven, and IntelliJ.  I added a user interface class to my package.  However, whenever I try to import a class from the same package into my UserInterface.java class, I receive the following error:</p>

<pre><code>Information:java: Errors occurred while compiling module 'stock-analyzer'

Information:javac 1.8.0_121 was used to compile java sources

Information:3/9/17, 8:16 PM - Compilation completed with 2 errors and 0 warnings in 2s 143ms

/Users/raulie/CAP5600/stock-analyzer/src/main/java/UserInterface.java

Error:(5, 17) java: package main.java does not exist
Error:(9, 13) java: cannot find symbol
  symbol:   class StockAnalyzerBasicForUI
  location: class UserInterface
</code></pre>

<p>I believe this is a dependency issue with maven, but have been unable to resolve.  I tried ""mvn compile"" and ""mvn clean install"" in the project directory (where the pom.xml file is), but receive a similar error like the one shown above.</p>

<p>Below is the pom.xml file:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0""
         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;artifactId&gt;stock-analyzer&lt;/artifactId&gt;

    &lt;parent&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-examples-parent&lt;/artifactId&gt;
        &lt;version&gt;0.7-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;

    &lt;name&gt;Stock Analyzer&lt;/name&gt;

    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;snapshots-repo&lt;/id&gt;
            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;
            &lt;releases&gt;
                &lt;enabled&gt;false&lt;/enabled&gt;
            &lt;/releases&gt;
            &lt;snapshots&gt;
                &lt;enabled&gt;true&lt;/enabled&gt;
            &lt;/snapshots&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;

    &lt;distributionManagement&gt;
        &lt;snapshotRepository&gt;
            &lt;id&gt;sonatype-nexus-snapshots&lt;/id&gt;
            &lt;name&gt;Sonatype Nexus snapshot repository&lt;/name&gt;
            &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;
        &lt;/snapshotRepository&gt;
        &lt;repository&gt;
            &lt;id&gt;nexus-releases&lt;/id&gt;
            &lt;name&gt;Nexus Release Repository&lt;/name&gt;
            &lt;url&gt;http://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/distributionManagement&gt;

    &lt;dependencyManagement&gt;
        &lt;dependencies&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
                &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
                &lt;version&gt;${nd4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
                &lt;artifactId&gt;nd4j-cuda-7.5-platform&lt;/artifactId&gt;
                &lt;version&gt;${nd4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
            &lt;dependency&gt;
                &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
                &lt;artifactId&gt;nd4j-cuda-8.0-platform&lt;/artifactId&gt;
                &lt;version&gt;${nd4j.version}&lt;/version&gt;
            &lt;/dependency&gt;
       &lt;/dependencies&gt;
    &lt;/dependencyManagement&gt;

    &lt;dependencies&gt;
        &lt;!-- ND4J backend. You need one in every DL4J project. Normally define artifactId as either ""nd4j-native-platform"" or ""nd4j-cuda-7.5-platform"" --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;${nd4j.backend}&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;!-- Core DL4J functionality --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
            &lt;version&gt;${dl4j.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;
            &lt;version&gt;${dl4j.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- deeplearning4j-ui is used for HistogramIterationListener + visualization: see http://deeplearning4j.org/visualization --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-ui_2.10&lt;/artifactId&gt;
            &lt;version&gt;${dl4j.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- Force guava versions for using UI/HistogramIterationListener --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;${guava.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- datavec-data-codec: used only in video example for loading video data --&gt;
        &lt;dependency&gt;
            &lt;artifactId&gt;datavec-data-codec&lt;/artifactId&gt;
            &lt;groupId&gt;org.datavec&lt;/groupId&gt;
            &lt;version&gt;${datavec.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- Used in the feedforward/classification/MLP* and feedforward/regression/RegressionMathFunctions example --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;jfree&lt;/groupId&gt;
            &lt;artifactId&gt;jfreechart&lt;/artifactId&gt;
            &lt;version&gt;${jfreechart.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.jfree&lt;/groupId&gt;
            &lt;artifactId&gt;jcommon&lt;/artifactId&gt;
            &lt;version&gt;${jcommon.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- Used for downloading data in some of the examples --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;
            &lt;artifactId&gt;httpclient&lt;/artifactId&gt;
            &lt;version&gt;4.3.5&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
                &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;${exec-maven-plugin.version}&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;exec&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
                &lt;configuration&gt;
                    &lt;executable&gt;java&lt;/executable&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;version&gt;${maven-shade-plugin.version}&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;shadedArtifactAttached&gt;true&lt;/shadedArtifactAttached&gt;
                    &lt;shadedClassifierName&gt;${shadedClassifier}&lt;/shadedClassifierName&gt;
                    &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt;
                    &lt;filters&gt;
                        &lt;filter&gt;
                            &lt;artifact&gt;*:*&lt;/artifact&gt;
                            &lt;excludes&gt;
                                &lt;exclude&gt;org/datanucleus/**&lt;/exclude&gt;
                                &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;
                                &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;
                                &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;
                            &lt;/excludes&gt;
                        &lt;/filter&gt;
                    &lt;/filters&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;transformers&gt;
                                &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.AppendingTransformer""&gt;
                                    &lt;resource&gt;reference.conf&lt;/resource&gt;
                                &lt;/transformer&gt;
                                &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.ServicesResourceTransformer""/&gt;
                                &lt;transformer implementation=""org.apache.maven.plugins.shade.resource.ManifestResourceTransformer""&gt;
                                &lt;/transformer&gt;
                            &lt;/transformers&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.5.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;${java.version}&lt;/source&gt;
                    &lt;target&gt;${java.version}&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>

<p>Any insight you can provide into what I'm doing wrong would be greatly appreciated.  Thanks!</p>
","<java><maven><intellij-idea><deeplearning4j>","2017-03-10 01:24:35","","0","6383802","2017-03-10 01:27:25","1","0","","","6383802","1","0","0"
"50273031","<p>I recently started learning Deeplearning4j and I fail to understand how the concept of epochs and iterations is actually implemented.
In the online documentation it says:</p>

<blockquote>
  <p>an epoch is a complete pass through a given dataset ...<br>
  Not to be confused with an iteration, which is simply one 
  update of the neural net model’s parameters.</p>
</blockquote>

<p>I ran a training using a MultipleEpochsIterator, but for the first run I set 1 epoch, miniBatchSize = 1 and a dataset of 1000 samples, so I expected the training to finish after 1 epoch and 1000 iterations, but after more than 100.000 iterations it was still running.</p>

<pre><code>int nEpochs = 1;
int miniBatchSize = 1;

MyDataSetFetcher fetcher = new MyDataSetFetcher(xDataDir, tDataDir, xSamples, tSamples);
//The same batch size set here was set in the model
BaseDatasetIterator baseIterator = new BaseDatasetIterator(miniBatchSize, sampleSize, fetcher);

MultipleEpochsIterator iterator = new MultipleEpochsIterator(nEpochs, baseIterator);
model.fit(iterator)
</code></pre>

<p>Then I did more tests changing the batch size, but that didn't change the frequency of the log lines printed by the IterationListener. I mean that I thought that if I increase the batch size to 100 then with 1000 samples I would have just 10 updates of the parameters an therefore just 10 iterations, but the logs and the timestamp intervals are more or less the same.</p>

<p>BTW. There is a similar question, but the answer does not actually answer my question, I would like to understand better the actual details:
<a href=""https://stackoverflow.com/questions/41637150/deeplearning4j-iterations-epochs-and-scoreiterationlistener"">Deeplearning4j: Iterations, Epochs, and ScoreIterationListener</a></p>
","<deep-learning><deeplearning4j>","2018-05-10 12:29:07","","1","9770698","2018-05-15 00:29:22","1","1","","","9770698","48","0","0"
"43794279","<p>I am trying to implement the <a href=""https://arxiv.org/abs/1509.02971"" rel=""nofollow noreferrer"">deep deterministic policy gradient algorithm</a> in Java using the <a href=""https://deeplearning4j.org/"" rel=""nofollow noreferrer"">DeepLearning4j</a> library, but I am having some trouble with implementing the policy gradient efficiently. </p>

<p>In many tensorflow implementations the policy gradient is calculated using the following simple command</p>

<pre><code>tf.gradients(output, network_params, -action_gradient)
</code></pre>

<p>Can this be done in an easy way using the DeepLearning4J library as well?</p>
","<deep-learning><gradient-descent><reinforcement-learning><deeplearning4j>","2017-05-04 23:31:27","","1","3178181","2017-05-29 20:59:30","1","2","","","3178181","6","0","0"
"42842839","<p>I was wondering why there is a method .nOut() that needs to be specified at each convolutional layer of a neural network in DeepLearning4j. Isn't the number of outputs dependent on the size of the kernel, the stride and the size of inputs? It also only takes one parameter, what if I want to specify both the width and length of my output at each convolutional layer, how do I do that? When I try not using this method, it makes my output 0 by default which throws an InvalidConfigurationException at runtime.</p>

<pre><code>.layer(1, new ConvolutionLayer.Builder(kernelHeight, kernelWidth)
    .stride(verticalStride, horizontalStride)
    .nOut(numberOfOutputs)        //This is what I don't understand
    .activation(Activation.IDENTITY)
    .build()) 
</code></pre>
","<conv-neural-network><deeplearning4j>","2017-03-16 19:07:33","","1","6215389","2017-03-16 19:12:55","0","1","","","6215389","35","0","0"
"34605430","<p>I am trying to replicate the paper <a href=""http://arxiv.org/abs/1103.0398"" rel=""nofollow"">NLP (almost) from scratch</a> using deeplearning4j. I have done the following steps:</p>

<ol>
<li>load the SENNA word vectors</li>
<li>write a iterator for the CoNLL'03 dataset: for each word, I form a word feature vector by concatenating the word vectors of its neighbour words (with window size = 5)</li>
<li>use the above dataset iterator to train a simple regression layer, for example:</li>
</ol>

<pre class=""lang-java prettyprint-override""><code>MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .seed(seed).iterations(iterations)
    .learningRate(1e-8f)
    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
    .list(2)
    .layer(0, new DenseLayer.Builder()
        .nIn(wordVecLayers * windowSize).nOut(hiddenSize)
        .activation(""relu"")
        .weightInit(WeightInit.DISTRIBUTION)
        .dist(new UniformDistribution(-2.83 / Math.sqrt(hiddenSize), 2.83 / Math.sqrt(hiddenSize)))
        .biasInit(0.0f).build())
    .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD)
        .nIn(hiddenSize).nOut(types.size())
        .activation(""softmax"").weightInit(WeightInit.DISTRIBUTION)
        .dist(new UniformDistribution(-2.83 / Math.sqrt(hiddenSize), 2.83 / Math.sqrt(hiddenSize)))
        .biasInit(0.0f).build())
    .backprop(true).pretrain(false)
    .build();
</code></pre>

<p>I have tried many different configurations but none of them worked for me. The model keep predicting all words with the 'O'-tag. I would appreciate if you can point out what's wrong with my approach? And what steps I should do next? Thank you!</p>
","<java><deep-learning><deeplearning4j>","2016-01-05 06:24:42","","3","1716303","2016-07-25 12:06:30","1","1","3","","1716303","98","5","0"
"51028267","<p>I'm trying to get a maven project built with an nd4j dependency on a linux ARM system.
Here's my current dependency in my pom.xml file</p>

<pre><code>&lt;dependency&gt;
     &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
     &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
     &lt;version&gt;0.9.1&lt;/version&gt;
     &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>

<p>Here is the error I'm getting when trying to do a maven install on my ARM system</p>

<blockquote>
  <p>Could not resolve dependencies for project com.test.test:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.nd4j:nd4j-native:jar:linux-arm:0.9.1, org.bytedeco.javacpp-presets:openblas:jar:linux-arm:0.2.19-1.3: Failure to find org.nd4j:nd4j-native:jar:linux-arm:0.9.1 in <a href=""https://repo.maven.apache.org/maven2"" rel=""nofollow noreferrer"">https://repo.maven.apache.org/maven2</a> was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced</p>
</blockquote>
","<java><maven><artificial-intelligence><raspbian><deeplearning4j>","2018-06-25 16:44:16","","1","7435912","2018-06-25 16:44:16","0","0","","","7435912","6","0","0"
"52425268","<p>I'm looking for the ways to generate a spectrogram image in an Android app.  I've found <a href=""https://github.com/deeplearning4j/DataVec/blob/master/datavec-data/datavec-data-audio/src/main/java/org/datavec/audio/extension/Spectrogram.java"" rel=""nofollow noreferrer"">this project</a> which appears to do half the work required: it loads the audio-file and creates a 2d array of intensities at a given time &amp; frequency.  However, now I'm a bit lost: how does one generate a human-viewable picture from these data?</p>

<p>As far as I understand, it will involve mapping the intensity values from [-1,1] float range to pixel colors.  But being a noob in audio processing, I don't know how other applications do this.</p>

<p>I'm not looking for exact code: I'd appreciate just the description of a general approach.</p>
","<java><spectrogram><deeplearning4j>","2018-09-20 12:32:19","","1","832474","2019-04-18 19:37:35","1","0","","","832474","373","8","0"
"36117502","<p>I'm trying to train an xor network with deeplearning4j, but i think i didn't really get how to use the dataset.</p>

<p>I wanted to create a NN with two inputs, two hidden neurons and one output neuron. </p>

<p>here is what i have:</p>

<pre><code>package org.deeplearning4j.examples.xor;

import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.Layer;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.distribution.UniformDistribution;
import org.deeplearning4j.nn.conf.layers.GravesLSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;

public class XorExample {
    public static void main(String[] args) {

        INDArray input = Nd4j.zeros(4, 2);
        INDArray labels = Nd4j.zeros(4, 1);

        input.putScalar(new int[] { 0, 0 }, 0);
        input.putScalar(new int[] { 0, 1 }, 0);

        input.putScalar(new int[] { 1, 0 }, 1);
        input.putScalar(new int[] { 1, 1 }, 0);

        input.putScalar(new int[] { 2, 0 }, 0);
        input.putScalar(new int[] { 2, 1 }, 1);

        input.putScalar(new int[] { 3, 0 }, 1);
        input.putScalar(new int[] { 3, 1 }, 1);

        labels.putScalar(new int[] { 0, 0 }, 0);
        labels.putScalar(new int[] { 1, 0 }, 1);
        labels.putScalar(new int[] { 2, 0 }, 1);
        labels.putScalar(new int[] { 3, 0 }, 0);

        DataSet ds = new DataSet(input,labels);

        //Set up network configuration:
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)
            .learningRate(0.1)
            .list(2)
            .layer(0, new GravesLSTM.Builder().nIn(2).nOut(2)
                    .updater(Updater.RMSPROP)
                    .activation(""tanh"").weightInit(WeightInit.DISTRIBUTION)
                    .dist(new UniformDistribution(-0.08, 0.08)).build())
            .layer(1, new RnnOutputLayer.Builder(LossFunction.MCXENT).activation(""softmax"")        //MCXENT + softmax for classification
                    .updater(Updater.RMSPROP)
                    .nIn(2).nOut(1).weightInit(WeightInit.DISTRIBUTION)
                    .dist(new UniformDistribution(-0.08, 0.08)).build())
            .pretrain(false).backprop(true)
            .build();

            MultiLayerNetwork net = new MultiLayerNetwork(conf);
            net.init();
            net.setListeners(new ScoreIterationListener(1));

            //Print the  number of parameters in the network (and for each layer)
            Layer[] layers = net.getLayers();
            int totalNumParams = 0;
            for( int i=0; i&lt;layers.length; i++ ){
                int nParams = layers[i].numParams();
                System.out.println(""Number of parameters in layer "" + i + "": "" + nParams);
                totalNumParams += nParams;
            }
            System.out.println(""Total number of network parameters: "" + totalNumParams);

            net.fit(ds);


            Evaluation eval = new Evaluation(3);
            INDArray output = net.output(ds.getFeatureMatrix());
            eval.eval(ds.getLabels(), output);
            System.out.println(eval.stats());

    }
}
</code></pre>

<p>the output looks like that</p>

<pre><code>Mär 20, 2016 7:03:06 PM com.github.fommil.jni.JniLoader liberalLoad
INFORMATION: successfully loaded C:\Users\LuckyPC\AppData\Local\Temp\jniloader5209513403648831212netlib-native_system-win-x86_64.dll
Number of parameters in layer 0: 46
Number of parameters in layer 1: 3
Total number of network parameters: 49
o.d.o.s.BaseOptimizer - Objective function automatically set to minimize. Set stepFunction in neural net configuration to change default settings.
o.d.o.l.ScoreIterationListener - Score at iteration 0 is 0.6931495070457458
Exception in thread ""main"" java.lang.IllegalArgumentException: Unable to getFloat row of non 2d matrix
    at org.nd4j.linalg.api.ndarray.BaseNDArray.getRow(BaseNDArray.java:3640)
    at org.deeplearning4j.eval.Evaluation.eval(Evaluation.java:107)
    at org.deeplearning4j.examples.xor.XorExample.main(XorExample.java:80)
</code></pre>
","<deeplearning4j>","2016-03-20 18:05:31","","5","1131857","2016-04-23 23:58:15","2","0","2","","1131857","7458","813","17"
"45893850","<p>I'm very new to NN and have started playing around with Deeplearning4j. I have tried as an exercise the idea of reducing ISO noise from a photo. To do this I took 2 photos identically framed, but one shot at ISO 100 and one shot at ISO25600.</p>

<p>I made a simple NN with 2 hidden layers. The input is the noisy, high ISO image (3 channels * 100px * 100px = 30000) and the output should be the clean image (same size as the input)</p>

<p>Here's how my NN looks like:</p>

<pre><code>final int numHiddenNodes = 50;
    return new NeuralNetConfiguration.Builder()
            .seed(seed)
            .iterations(iterations)
            .optimizationAlgo(
                    OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .learningRate(learningRate)
            .weightInit(WeightInit.XAVIER)
            .updater(Updater.NESTEROVS)
            .optimizationAlgo(
                        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .list()
            .layer(0,
                    new DenseLayer.Builder().nIn(numInputs)
                            .nOut(numHiddenNodes)
                            .activation(Activation.TANH).build())
            .layer(1,
                    new DenseLayer.Builder().nIn(numHiddenNodes)
                            .nOut(numHiddenNodes)
                            .activation(Activation.TANH).build())
            .layer(2,
                    new OutputLayer.Builder(LossFunctions.LossFunction.MSE)
                            .activation(Activation.RELU)
                            .nIn(numHiddenNodes).nOut(numOutputs).build())
            .pretrain(false).backprop(true).build();
</code></pre>

<p>I've trained my network for several thousands epochs, and the score I get is around 130 (that's probably the Mean Squared Error), but the result I'm getting is not what I expected.</p>

<p>Input:
<a href=""https://i.stack.imgur.com/yxZu4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yxZu4.jpg"" alt=""enter image description here""></a></p>

<p>Ouptut:
<a href=""https://i.stack.imgur.com/VaPRS.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VaPRS.jpg"" alt=""enter image description here""></a></p>

<p>Am I going about this the wrong way?</p>
","<image-processing><neural-network><artificial-intelligence><linear-regression><deeplearning4j>","2017-08-26 09:16:15","","0","2587784","2017-08-28 17:15:38","0","5","","","2587784","370","5","5"
"54307802","<p>I want to train a simple LSTM network but I got the exception</p>

<pre><code>java.lang.IllegalStateException: C (result) array is not F order or is a view. Nd4j.gemm requires the result array to be F order and not a view. C (result) array: [Rank: 2,Offset: 0 Order: f Shape: [10,1],  stride: [1,10]]
</code></pre>

<p>I'm training a simple NN with a single LSTM cell and a single output cell for regression.</p>

<p>I created a training dataset of just 10 samples with variable sequence length (from 5 to 10) in csv files, each sample consists of just one value for the input and one value for the output.</p>

<p>I created a <code>SequenceRecordReaderDataSetIterator</code> from a <code>CSVSequenceRecordReader</code>.
When I train my network the code throws the exception.</p>

<p>I tried generating random dataset coding the dataset iterator directly with 'f shape' INDarray and the code runs without error.</p>

<p>So the problem seems to be the shape of tensors created by <code>CSVSequenceRecordReader</code>.</p>

<p>Does anyone have this problems?</p>

<h3>SingleFileTimeSeriesDataReader.java</h3>

<pre><code>package org.mmarini.lstmtest;

import java.io.IOException;

import org.datavec.api.records.reader.SequenceRecordReader;
import org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader;
import org.datavec.api.split.NumberedFileInputSplit;
import org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;

/**
 *
 */
public class SingleFileTimeSeriesDataReader {

    private final int miniBatchSize;
    private final int numPossibleLabels;
    private final boolean regression;
    private final String filePattern;
    private final int maxFileIdx;
    private final int minFileIdx;
    private final int numInputs;

    /**
     * 
     * @param filePattern
     * @param minFileIdx
     * @param maxFileIdx
     * @param numInputs
     * @param numPossibleLabels
     * @param miniBatchSize
     * @param regression
     */
    public SingleFileTimeSeriesDataReader(final String filePattern, final int minFileIdx, final int maxFileIdx,
            final int numInputs, final int numPossibleLabels, final int miniBatchSize, final boolean regression) {
        this.miniBatchSize = miniBatchSize;
        this.numPossibleLabels = numPossibleLabels;
        this.regression = regression;
        this.filePattern = filePattern;
        this.maxFileIdx = maxFileIdx;
        this.minFileIdx = minFileIdx;
        this.numInputs = numInputs;
    }

    /**
     *
     * @return
     * @throws IOException
     * @throws InterruptedException
     */
    public DataSetIterator apply() throws IOException, InterruptedException {
        final SequenceRecordReader reader = new CSVSequenceRecordReader(0, "","");
        reader.initialize(new NumberedFileInputSplit(filePattern, minFileIdx, maxFileIdx));
        final DataSetIterator iter = new SequenceRecordReaderDataSetIterator(reader, miniBatchSize, numPossibleLabels,
                numInputs, regression);
        return iter;
    }
}
</code></pre>

<h3>TestConfBuilder.java</h3>

<pre><code>/**
 *
 */
package org.mmarini.lstmtest;

import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.LSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.weights.WeightInit;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;

/**
 * @author mmarini
 *
 */
public class TestConfBuilder {

    private final int noInputUnits;
    private final int noOutputUnits;
    private final int noLstmUnits;

    /**
     *
     * @param noInputUnits
     * @param noOutputUnits
     * @param noLstmUnits
     */
    public TestConfBuilder(final int noInputUnits, final int noOutputUnits, final int noLstmUnits) {
        super();
        this.noInputUnits = noInputUnits;
        this.noOutputUnits = noOutputUnits;
        this.noLstmUnits = noLstmUnits;
    }

    /**
     *
     * @return
     */
    public MultiLayerConfiguration build() {
        final NeuralNetConfiguration.Builder builder = new NeuralNetConfiguration.Builder()
                .weightInit(WeightInit.XAVIER).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);
        final LSTM lstmLayer = new LSTM.Builder().units(noLstmUnits).nIn(noInputUnits).activation(Activation.TANH)
                .build();
        final RnnOutputLayer outLayer = new RnnOutputLayer.Builder(LossFunction.MEAN_SQUARED_LOGARITHMIC_ERROR)
                .activation(Activation.IDENTITY).nOut(noOutputUnits).nIn(noLstmUnits).build();
        final MultiLayerConfiguration conf = builder.list(lstmLayer, outLayer).build();
        return conf;
    }
}
</code></pre>

<h3>TestTrainingTest .java</h3>

<pre><code>package org.mmarini.lstmtest;

import static org.hamcrest.CoreMatchers.equalTo;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.junit.jupiter.api.Assertions.assertNotNull;

import java.io.File;
import java.io.IOException;
import java.util.Arrays;

import org.deeplearning4j.datasets.iterator.INDArrayDataSetIterator;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.junit.jupiter.api.Test;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.primitives.Pair;
import org.nd4j.linalg.util.ArrayUtil;

class TestTrainingTest {

    private static final int MINI_BATCH_SIZE = 10;
    private static final int NUM_LABELS = 1;
    private static final boolean REGRESSION = true;
    private static final String SAMPLES_FILE = ""src/test/resources/datatest/sample_%d.csv"";
    private static final int MIN_INPUTS_FILE_IDX = 0;
    private static final int MAX_INPUTS_FILE_IDX = 9;
    private static final int NUM_INPUTS_COLUMN = 1;
    private static final int NUM_HIDDEN_UNITS = 1;

    DataSetIterator createData() {
        final double[][][] featuresAry = new double[][][] { { { 0.5, 0.2, 0.5 } }, { { 0.5, 1.0, 0.0 } } };
        final double[] featuresData = ArrayUtil.flattenDoubleArray(featuresAry);
        final int[] featuresShape = new int[] { 2, 1, 3 };
        final INDArray features = Nd4j.create(featuresData, featuresShape, 'c');

        final double[][][] labelsAry = new double[][][] { { { 1.0, -1.0, 1.0 }, { 1.0, -1.0, -1.0 } } };
        final double[] labelsData = ArrayUtil.flattenDoubleArray(labelsAry);
        final int[] labelsShape = new int[] { 2, 1, 3 };
        final INDArray labels = Nd4j.create(labelsData, labelsShape, 'c');

        final INDArrayDataSetIterator iter = new INDArrayDataSetIterator(
                Arrays.asList(new Pair&lt;INDArray, INDArray&gt;(features, labels)), 2);
        System.out.println(iter.inputColumns());
        return iter;
    }

    private String file(String template) {
        return new File(""."", template).getAbsolutePath();
    }

    @Test
    void testBuild() throws IOException, InterruptedException {
        final SingleFileTimeSeriesDataReader reader = new SingleFileTimeSeriesDataReader(file(SAMPLES_FILE),
                MIN_INPUTS_FILE_IDX, MAX_INPUTS_FILE_IDX, NUM_INPUTS_COLUMN, NUM_LABELS, MINI_BATCH_SIZE, REGRESSION);

        final DataSetIterator data = reader.apply();

        assertThat(data.inputColumns(), equalTo(NUM_INPUTS_COLUMN));
        assertThat(data.totalOutcomes(), equalTo(NUM_LABELS));

        final TestConfBuilder builder = new TestConfBuilder(NUM_INPUTS_COLUMN, NUM_LABELS, NUM_HIDDEN_UNITS);
        final MultiLayerConfiguration conf = builder.build();
        final MultiLayerNetwork net = new MultiLayerNetwork(conf);
        assertNotNull(net);
        net.init();
        net.fit(data);
    }

}
</code></pre>

<p>I expect not to throw any exception but I got the following exception:</p>

<pre><code>java.lang.IllegalStateException: C (result) array is not F order or is a view. Nd4j.gemm requires the result array to be F order and not a view. C (result) array: [Rank: 2,Offset: 0 Order: f Shape: [10,1],  stride: [1,10]]
    at org.nd4j.base.Preconditions.throwStateEx(Preconditions.java:641)
    at org.nd4j.base.Preconditions.checkState(Preconditions.java:304)
    at org.nd4j.linalg.factory.Nd4j.gemm(Nd4j.java:980)
    at org.deeplearning4j.nn.layers.recurrent.LSTMHelpers.backpropGradientHelper(LSTMHelpers.java:696)
    at org.deeplearning4j.nn.layers.recurrent.LSTM.backpropGradientHelper(LSTM.java:122)
    at org.deeplearning4j.nn.layers.recurrent.LSTM.backpropGradient(LSTM.java:93)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.calcBackpropGradients(MultiLayerNetwork.java:1826)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2644)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2587)
    at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:160)
    at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:63)
    at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fitHelper(MultiLayerNetwork.java:1602)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1521)
    at org.mmarini.lstmtest.TestTrainingTest.testBuild(TestTrainingTest.java:77)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:532)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:171)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:167)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:114)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:59)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$4(NodeTestTask.java:108)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:98)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:74)
    at java.util.ArrayList.forEach(ArrayList.java:1257)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$4(NodeTestTask.java:112)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:98)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:74)
    at java.util.ArrayList.forEach(ArrayList.java:1257)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$4(NodeTestTask.java:112)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:98)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:74)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
    at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
    at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
    at org.eclipse.jdt.internal.junit5.runner.JUnit5TestReference.run(JUnit5TestReference.java:89)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:41)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:541)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:763)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:463)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:209)

</code></pre>
","<deeplearning4j>","2019-01-22 11:56:13","","1","3119185","2019-02-20 05:37:53","1","0","","","3119185","67","0","0"
"53623860","<p>I've been running a DL4J project on my laptop using my mobile gpu. It runs fine, the gpu is detected and my models get trained. But after switching to a desktop with discrete graphics, the same program attempts to use the cpu!
Both machines are running ubuntu 18.04 and have cuda 9.0 installed.</p>

<p>When the program starts, I get this output from the laptop:</p>

<blockquote>
  <p>INFO: Loaded [JCublasBackend] backend Dec 05, 2018 3:26:33 AM
  org.nd4j.nativeblas.NativeOpsHolder  INFO: Number of threads
  used for NativeOps: 32 Dec 05, 2018 3:26:33 AM
  org.nd4j.nativeblas.Nd4jBlas  INFO: Number of threads used for
  BLAS: 0 Dec 05, 2018 3:26:33 AM
  org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner
  printEnvironmentInformation INFO: Backend used: [CUDA]; OS: [Linux]
  Dec 05, 2018 3:26:33 AM
  org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner
  printEnvironmentInformation INFO: Cores: [8]; Memory: [3.4GB]; Dec 05,
  2018 3:26:33 AM
  org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner
  printEnvironmentInformation INFO: Blas vendor: [CUBLAS] Dec 05, 2018
  3:26:33 AM org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner
  printEnvironmentInformation INFO: Device Name: [GeForce GTX 960M]; CC:
  [5.0]; Total/free memory: [4242604032]</p>
</blockquote>

<p>And this from the desktop:</p>

<blockquote>
  <p>03:29:44.843 [main] INFO  org.nd4j.linalg.factory.Nd4jBackend - Loaded
  [CpuBackend] backend</p>
</blockquote>

<p>A crash happens immediately after since my dependencies include nd4j-cuda-9.0-platform and not nd4j-native-platform.</p>

<p>I'm pretty sure cuda is working on the desktop, I've run tensorflow-gpu programs on it without any issues.</p>

<p>I use gradle, here is the dependencies section of my build.gradle:</p>

<pre><code>compile group: 'org.deeplearning4j', name: 'deeplearning4j-core', version: '1.0.0-beta'
compile group: 'org.nd4j', name: 'nd4j-cuda-9.0-platform', version: '1.0.0-beta'
compile group: 'org.datavec', name: 'datavec-api', version: '1.0.0-beta'
compile group: 'org.deeplearning4j', name: 'rl4j-core', version: '1.0.0-beta'
compile group: 'org.deeplearning4j', name: 'rl4j-api', version: '1.0.0-beta'
compile group: 'org.deeplearning4j', name: 'deeplearning4j-cuda-9.0', version: '1.0.0-beta'
compile group: 'org.deeplearning4j', name: 'deeplearning4j-ui_2.10', version: '1.0.0-beta'
</code></pre>

<p>I could be missing something basic here. Any help is greatly appreciated.</p>
","<java><deeplearning4j><nd4j>","2018-12-05 01:27:34","","1","1744811","2018-12-05 06:51:53","0","2","","","1744811","11","0","0"
"53849603","<p>I am getting into Deeplearning4j and am at a stage where I need to refine my LSTM training but have run into a flew blockers.</p>

<p>So far I have based my work on various time-series prediction examples in Deeplearning4j. I am able to train an LSTM using a DataSet and predict future steps with some degree of accuracy but am unsure how to ensure my configurations are correct i.e. that I am not over-fitting my data. </p>

<p>I have been using the Deeplearing4j UI to help understand the training process but I am a bit confused by some of the metrics. I was wondering if anyone could clarify what the difference between the <strong>Summary</strong> and <strong>Score</strong> lines represent in the chart on the top left hard corner of the overview page? I have provided an image of this below. Do these correlate to training and validation errors which would identify over-fitting / under-fitting of data? </p>

<p><a href=""https://i.stack.imgur.com/vTi0O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vTi0O.png"" alt=""Model Score vs. Iteration""></a></p>

<p>If not, what is the best way to extract this information and diagnose any issues with my LSTM? I have seen uses of the Evaluation class but am unsure if this is already done by the ui plugin? Furthermore, if the Evaluation class is required, is there any way to integrate this into the UI plugin so it is easily displayed?</p>

<p>I am also using a DataSet instead of a DataSetIterator which may cause issues when using the Evaluator class as I can't see a way to evaluate the net using a plain DataSet.</p>

<p>My final question is a bit more of a general time-series training one. I understand to achieve useful evaluation of my net, a split of the training data is used. For example, 70% is used for training and 30% for evaluation. In the case of time-series data, could this be problematic? If not all data is used in the training process, it could mean the LSTM does not learn the most recent and relevant trends which allow it to do its job well. How is this normally solved when using Depplearing4j for time-series prediction? </p>

<p>Any help would be greatly appreciated!</p>
","<deeplearning4j>","2018-12-19 10:48:32","","0","10094119","2018-12-19 21:00:31","0","0","","","10094119","38","2","0"
"53711426","<p>I'm trying to train a model using deep learning in java, when I start training the train data it gives an error </p>

<pre><code>Invalid classification data: expect label value (at label index column = 0) to be in range 0 to 1 inclusive (0 to numClasses-1, with numClasses=2); got label value of 2
</code></pre>

<p>I didn't understand the error since I am a beginner in deep learning 4j. I am using a data set which views relationship between two people (if there is a relationship between two people then the class label is going to be 1 otherwise 0).</p>

<p>The Java code </p>

<pre><code>public class SNA {
private static Logger log = LoggerFactory.getLogger(SNA.class);

public static void main(String[] args) throws Exception {
    int seed = 123;
    double learningRate = 0.01;
    int batchSize = 50;
    int nEpochs = 30;
    int numInputs = 2;
    int numOutputs = 2;
    int numHiddenNodes = 20;

    //load the training data
    RecordReader rr = new CSVRecordReader(0,"","");
    rr.initialize(new FileSplit(new File(""C:\\Users\\GTS\\Desktop\\SNA project\\experiments\\First experiment\\train\\slashdotTrain.csv"")));
    DataSetIterator trainIter = new RecordReaderDataSetIterator(rr, batchSize,0, 2);

    // load test data
    RecordReader rrTest = new CSVRecordReader();
    rr.initialize(new FileSplit(new File(""C:\\Users\\GTS\\Desktop\\SNA project\\experiments\\First experiment\\test\\slashdotTest.csv"")));
    DataSetIterator testIter = new RecordReaderDataSetIterator(rrTest, batchSize,0, 2);

    log.info(""**** Building Model ****"");
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(seed)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .iterations(1)
            .learningRate(learningRate)
            .updater(Updater.NESTEROVS).momentum(0.9)
            .list()
            .layer(0, new DenseLayer.Builder()
                    .nIn(numInputs)
                    .nOut(numHiddenNodes)
                    .activation(""relu"")
                    .weightInit(WeightInit.XAVIER)
                    .build())
            .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                    .activation(""softmax"")
                    .weightInit(WeightInit.XAVIER)
                    .nIn(numHiddenNodes)
                    .nOut(numOutputs)
                    .build())
            .pretrain(false).backprop(true)
            .build();

    MultiLayerNetwork model = new MultiLayerNetwork(conf);
    model.init();

    // Listener to show how the network is training in the log
    model.setListeners(new ScoreIterationListener(10));

    log.info("" **** Train Model **** "");
    for (int i = 0; i &lt; nEpochs; i++) {
        model.fit(trainIter);
    }

    System.out.println(""**** Evaluate Model ****"");
    Evaluation evaluation = new Evaluation(numOutputs);
    while (testIter.hasNext()) {
        DataSet t = testIter.next();
        INDArray feature = t.getFeatureMatrix();
        INDArray labels = t.getLabels();
        INDArray predicted = model.output(feature, false);
        evaluation.eval(labels, predicted);
    }

    System.out.println(evaluation.stats());
}
</code></pre>

<p>}</p>

<p>Any help Please?
Thanks A lot </p>
","<java><neural-network><deeplearning4j>","2018-12-10 18:17:46","","0","10666466","2018-12-11 10:54:28","1","0","","","10666466","12","0","0"
"36280892","<p>I'm trying to develop some intuition for machine learning. I looked over examples from <a href=""https://github.com/deeplearning4j/dl4j-0.4-examples"" rel=""nofollow"">https://github.com/deeplearning4j/dl4j-0.4-examples</a> and I wanted to develop my own example. Basically I just took a simple function:  a * a + b * b + c * c - a * b * c + a + b + c and generated 10000 outputs for random a,b,c and tried to train my network on 90% of the inputs. The thing is no matter what I done my network never gets to predict the rest of the examples.</p>

<p>Here is my code: </p>

<pre><code>public class BasicFunctionNN {

    private static Logger log = LoggerFactory.getLogger(MlPredict.class);

    public static DataSetIterator generateFunctionDataSet() {
        Collection&lt;DataSet&gt; list = new ArrayList&lt;&gt;();
        for (int i = 0; i &lt; 100000; i++) {
            double a = Math.random();
            double b = Math.random();
            double c = Math.random();

            double output = a * a + b * b + c * c - a * b * c + a + b + c;
            INDArray in = Nd4j.create(new double[]{a, b, c});
            INDArray out = Nd4j.create(new double[]{output});
            list.add(new DataSet(in, out));
        }
        return new ListDataSetIterator(list, list.size());
    }

    public static void main(String[] args) throws Exception {
        DataSetIterator iterator = generateFunctionDataSet();

        Nd4j.MAX_SLICES_TO_PRINT = 10;
        Nd4j.MAX_ELEMENTS_PER_SLICE = 10;

        final int numInputs = 3;
        int outputNum = 1;
        int iterations = 100;

        log.info(""Build model...."");
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .iterations(iterations).weightInit(WeightInit.XAVIER).updater(Updater.SGD).dropOut(0.5)
                .learningRate(.8).regularization(true)
                .l1(1e-1).l2(2e-4)
                .optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT)
                .list(3)
                .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(8)
                        .activation(""identity"")
                        .build())
                .layer(1, new DenseLayer.Builder().nIn(8).nOut(8)
                        .activation(""identity"")
                        .build())
                .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.RMSE_XENT)//LossFunctions.LossFunction.RMSE_XENT)
                        .activation(""identity"")
                        .weightInit(WeightInit.XAVIER)
                        .nIn(8).nOut(outputNum).build())
                .backprop(true).pretrain(false)
                .build();


        //run the model
        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();
        model.setListeners(Collections.singletonList((IterationListener) new ScoreIterationListener(iterations)));

        //get the dataset using the record reader. The datasetiterator handles vectorization
        DataSet next = iterator.next();
        SplitTestAndTrain testAndTrain = next.splitTestAndTrain(0.9);
        System.out.println(testAndTrain.getTrain());

        model.fit(testAndTrain.getTrain());

        //evaluate the model
        Evaluation eval = new Evaluation(10);
        DataSet test = testAndTrain.getTest();
        INDArray output = model.output(test.getFeatureMatrix());
        eval.eval(test.getLabels(), output);
        log.info(""&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"");
        log.info(eval.stats());

    }
}
</code></pre>

<p>I also played with the learning rate, and it happens many time that score doesn't improve:</p>

<pre><code>10:48:51.404 [main] DEBUG o.d.o.solvers.BackTrackLineSearch - Exited line search after maxIterations termination condition; score did not improve (bestScore=0.8522868127536543, scoreAtStart=0.8522868127536543). Resetting parameters
</code></pre>

<p>As an activation function I also tried relu</p>
","<machine-learning><deep-learning><deeplearning4j>","2016-03-29 09:52:11","","0","1995187","2016-03-30 19:17:33","1","0","","","1995187","289","55","1"
"35063068","<p>Deeplearning4j canova example not working.I am getting the output of eval.stats as NaN (accuracy).I</p>

<pre><code>import org.slf4j.LoggerFactory;


public class ImageClassifierExample {

    public static void main(String[] args) throws IOException, InterruptedException {


        // Path to the labeled images
        String labeledPath = System.getProperty(""user.home"")+""/lfw"";
         List&lt;String&gt; labels = new ArrayList&lt;&gt;();
        for(File f : new File(labeledPath).listFiles()) {
            labels.add(f.getName());
        }
        // Instantiating a RecordReader pointing to the data path with the specified
        // height and width for each image.
        RecordReader recordReader = new ImageRecordReader(28, 28, true,labels);
        recordReader.initialize(new FileSplit(new File(labeledPath)));

        // Canova to Dl4j
        DataSetIterator iter = new RecordReaderDataSetIterator(recordReader, 784,labels.size());

        // Creating configuration for the neural net.
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)
                .constrainGradientToUnitNorm(true)
                .weightInit(WeightInit.DISTRIBUTION)
                .dist(new NormalDistribution(1,1e-5))
                .iterations(100).learningRate(1e-3)
                .nIn(784).nOut(labels.size())
                .visibleUnit(org.deeplearning4j.nn.conf.layers.RBM.VisibleUnit.GAUSSIAN)
                .hiddenUnit(org.deeplearning4j.nn.conf.layers.RBM.HiddenUnit.RECTIFIED)
                .layer(new org.deeplearning4j.nn.conf.layers.RBM())
                .list(4).hiddenLayerSizes(600, 250, 100).override(3, new ConfOverride() {
                    @Override
                    public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
                        if (i == 3) {
                            builder.layer(new org.deeplearning4j.nn.conf.layers.OutputLayer());
                            builder.activationFunction(""softmax"");
                            builder.lossFunction(LossFunctions.LossFunction.MCXENT);

                        }
                    }
                }).build();

        MultiLayerNetwork network = new MultiLayerNetwork(conf);
        network.setListeners(Arrays.&lt;IterationListener&gt;asList(new ScoreIterationListener(10)));

        // Training
        while(iter.hasNext()){
            DataSet next = iter.next();
            network.fit(next);
        }

        // Testing -- We're not doing split test and train
        // Using the same training data as test.
        iter.reset();
        Evaluation eval = new Evaluation();
        while(iter.hasNext()){
            DataSet next = iter.next();
            INDArray predict2 = network.output(next.getFeatureMatrix());
            eval.eval(next.getLabels(), predict2);
        }

        System.out.println(eval.stats());
    }
}
</code></pre>
","<java><deeplearning4j>","2016-01-28 13:39:22","","1","3651739","2016-01-29 22:03:14","1","2","","","3651739","851","569","55"
"43793217","<p>I am working with some fixed length time series data . This data is of the form headers #0-->time series trace#0-->header#1-->time series trace #1 -->..->header#P-->time series trace #P. Through the use of some Apis meant to deal with such data I was finally able to convert the data into an INDArray of shape [P, length of time series] . If my aim is to remove noise from the data, what should I be using as my DNN ?
I have worked with stacked Denoised Autoencoders so far, but am not sure if the way I have implemented it is correct.</p>

<pre><code>  INDArray sinput=Nd4j.zeros(n1,n2);           //creating a 2D matrix of rows=samples, columns=traceNumbers


    float[] data = new float[n1];           //for the length of n1 samples


    for(int j=0;j&lt;n2;j++){                  //for each trace
        input.read(data);

        for(int k=0;k&lt; n1;k++){
            System.out.println(""reading ""+k+"",""+j+"" value: ""+data[k] );
            sinput.putScalar(new int[]{k,j},data[k]);


        }
    }



    Activation act=Activation.RELU;
    LearningRatePolicy lrPol=LearningRatePolicy.None;
    MultiLayerConfiguration configuration= new NeuralNetConfiguration.Builder()
            .seed(13)
            .iterations(20000)
            .learningRate(0.003)
            .learningRateDecayPolicy(lrPol)
            .lrPolicyDecayRate(1e-5)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .l1(1e-5)
            .regularization(true)
            .l2(1e-3)
            .list()

            .layer(0,new AutoEncoder.Builder().nIn(n2).nOut(FIRST_HIDDEN_LAYER_WIDTH).activation(act).corruptionLevel(0.3).weightInit(WeightInit.XAVIER).build())
            .layer(1,new AutoEncoder.Builder().nIn(FIRST_HIDDEN_LAYER_WIDTH).nOut(SECOND_HIDDEN_LAYER_WIDTH).activation(act).weightInit(WeightInit.XAVIER).build())
            .layer(2,new AutoEncoder.Builder().nIn(SECOND_HIDDEN_LAYER_WIDTH).nOut(THIRD_HIDDEN_LAYER_WIDTH).activation(act).weightInit(WeightInit.XAVIER).build())
            .layer(3,new AutoEncoder.Builder().nIn(THIRD_HIDDEN_LAYER_WIDTH).nOut(FOURTH_HIDDEN_LAYER_WIDTH).activation(act).weightInit(WeightInit.XAVIER).build())



            .layer(4,new AutoEncoder.Builder().nIn(FOURTH_HIDDEN_LAYER_WIDTH).nOut(THIRD_HIDDEN_LAYER_WIDTH).activation(act).weightInit(WeightInit.XAVIER).corruptionLevel(0.3).build())
            .layer(5,new AutoEncoder.Builder().nIn(THIRD_HIDDEN_LAYER_WIDTH).nOut(SECOND_HIDDEN_LAYER_WIDTH).activation(act).weightInit(WeightInit.XAVIER).corruptionLevel(0.3).build())
            .layer(6,new AutoEncoder.Builder().nIn(SECOND_HIDDEN_LAYER_WIDTH).nOut(FIRST_HIDDEN_LAYER_WIDTH).activation(act).weightInit(WeightInit.XAVIER).corruptionLevel(0.3).build())
            .layer(7,new AutoEncoder.Builder().nIn(FIRST_HIDDEN_LAYER_WIDTH).nOut(n2).activation(act).weightInit(WeightInit.XAVIER).corruptionLevel(0.3).build())
            //  .layer(1,new AutoEncoder.Builder().nIn(SECOND_HIDDEN_LAYER_WIDTH).nOut(SECOND_HIDDEN_LAYER_WIDTH).activation(Activation.SIGMOID).corruptionLevel(0.3)momentum(0.3)..build())
            .layer(8,new OutputLayer.Builder(LossFunctions.LossFunction.SQUARED_LOSS).activation(Activation.IDENTITY).nIn(n2).nOut(n2).build())
            .pretrain(false).backprop(true)
            .build();








    MultiLayerNetwork network = new MultiLayerNetwork(configuration);
    network.init();

    DataSet dataSet=new DataSet(sinput,sinput);



    network.init();
    network.fit(new DataSet(dataSet.getFeatureMatrix(), dataSet.getFeatureMatrix()));             //training
    //network.fit(seisInput,seisInput);
    System.out.println(""Layer 0 has ""+network.getLayer(0).numParams()+"" and Params: "");
    System.out.println(network.getLayer(0).params());
    System.out.println();
 //DataSet testDataSet=new DataSet(sinput,sinput);
   // DataSetIterator iterator=new INDArrayDataSetIterator()

    INDArray outputFromNet=network.output(testDataSet.getFeatureMatrix());
    INDArray reconstructedFromNet=network.reconstruct(outputFromNet,1); 
</code></pre>
","<neural-network><time-series><deep-learning><autoencoder><deeplearning4j>","2017-05-04 21:41:03","","1","7835192","2017-05-06 12:01:05","0","0","","","7835192","6","0","0"
"40978565","<p>I'm trying to classify some data using knime with knime-labs deep learning plugin.</p>

<p>I have about 16.000 products in my DB, but I have about 700 of then that I know its category.</p>

<p>I'm trying to classify as much as possible using some DM (data mining) technique. I've downloaded some plugins to knime, now I have some deep learning tools as some text tools.</p>

<p>Here is my workflow, I'll use it to explain what I'm doing:</p>

<p><a href=""https://i.stack.imgur.com/fQF9k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fQF9k.png"" alt=""Basic Workflow""></a></p>

<p>I'm transforming the <em>product</em> name into vector, than applying into it. 
After I train a <strong>DL4J</strong> learner with <strong>DeepMLP</strong>. (I'm not really understand it all, it was the one that I thought I got the best results). Than I try to apply the model in the same data set.</p>

<p>I thought I would get the result with the predicted classes. But I'm getting a column with output_activations that looks that gets a pair of doubles. when sorting this column I get some related date close to each other. But I was expecting to get the classes.</p>

<p>Here is a print of the result table, here you can see the output with the input.</p>

<p><a href=""https://i.stack.imgur.com/YT8kw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YT8kw.png"" alt=""Output table""></a></p>

<p>In columns selection it's getting just the converted_document and selected des_categoria as Label Column (learning node config). And in Predictor node I checked the ""Append SoftMax Predicted Label?""</p>

<p>The <em>nom_produto</em> is the text column that I'm trying to use to predict the <em>des_categoria</em> column that it the product category.</p>

<p>I'm really newbie about DM and DL. If you could get me some help to solve what I'm trying to do would be awesome. Also be free to suggest some learning material about what attempting to achieve </p>

<p>PS: I also tried to apply it into the unclassified data (17,000 products), but I got the same result.</p>
","<machine-learning><classification><deep-learning><knime><deeplearning4j>","2016-12-05 16:13:54","","0","1864883","2017-12-06 10:19:43","1","0","","","1864883","508","570","6"
"44836606","<p>I have a few general questions regarding using pre-trained image classification models in mobile. </p>

<ol>
<li>How big is a typical pre-trained model?</li>
<li>If it is too big for mobile, what is the best strategy from there? </li>
<li>I checked out the documentation of DeepLearning for Java, anywhere to download pre-trained model?</li>
</ol>

<p>Thanks in advance.</p>
","<neural-network><deeplearning4j>","2017-06-29 23:42:19","","0","3830834","2017-06-30 00:08:38","1","0","","","3830834","131","39","0"
"37306003","<p>so I'm trying to implement the free deeplearning library for Java called deeplearning4j to tackle a classification task in nlp.</p>

<pre><code>public static void Learn(String labelledDataFileName) throws Exception {
    ParagraphVectors paragraphVectors = new ParagraphVectors();
    InMemoryLookupCache cache = new InMemoryLookupCache();
    LabelleDataIterator iterator = new LabelleDataIterator(new File(labelledDataFileName));
    TokenizerFactory t = new DefaultTokenizerFactory();
    t.setTokenPreProcessor(new CommonPreprocessor());
    paragraphVectors = new ParagraphVectors.Builder()
            .minWordFrequency(1)
            .iterations(3)
            .learningRate(0.025)
            .minLearningRate(0.001)
            .layerSize(400)
            .batchSize(1000)
            .epochs(1)
            .iterate(iterator)
            .trainWordVectors(true)
            .vocabCache(cache)
            .tokenizerFactory(t)
            .build();
    paragraphVectors.fit();
    WordVectorSerializer.writeFullModel(paragraphVectors, MODEL_FILE_NAME);
}
</code></pre>

<p>pretty standard, not much different from the sample provided on the net. The trained model after being fitted is then saved to a text file with the method writeFullModel. Then it can be loaded with this method</p>

<pre><code>WordVectorSerializer.loadFullModel(MODEL_FILE_NAME);
</code></pre>

<p>The problem is, it doesn't seem to work when the model gets big. For a model file of size 120Mb, I keep getting this </p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal slice 7151
at org.nd4j.linalg.api.ndarray.BaseNDArray.slice(BaseNDArray.java:2852)
at org.nd4j.linalg.api.ndarray.BaseNDArray.tensorAlongDimension(BaseNDArray.java:753)
at org.nd4j.linalg.api.ndarray.BaseNDArray.vectorAlongDimension(BaseNDArray.java:830)
at org.nd4j.linalg.api.ndarray.BaseNDArray.getRow(BaseNDArray.java:3628)
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.loadFullModel(WordVectorSerializer.java:523)
</code></pre>

<p>It loasd fine with a small model file though.
Any help would be appreciated, thank you so much.</p>
","<java><nlp><deep-learning><deeplearning4j>","2016-05-18 17:04:54","","2","6352251","2016-05-29 12:07:03","1","1","","","6352251","11","0","0"
"55210579","<p>I first started using Deeplearning4j this weekend. I looked at the page at <a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-quickstart"" rel=""nofollow noreferrer"">https://deeplearning4j.org/docs/latest/deeplearning4j-quickstart</a> and saw the option for ""Eclipse setup without Maven"". Since I was used to using Eclipse and had never heard of Maven before, I decided to follow those instructions.</p>

<p>After I set it up, I found a tutorial on YouTube but when I go to run the project, I get this error:</p>

<pre><code>Exception in thread ""main"" java.lang.UnsatisfiedLinkError: no jniopenblas in java.library.path
at java.lang.ClassLoader.loadLibrary(Unknown Source)
at java.lang.Runtime.loadLibrary0(Unknown Source)
at java.lang.System.loadLibrary(Unknown Source)
at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:945)
at org.bytedeco.javacpp.Loader.load(Loader.java:750)
at org.bytedeco.javacpp.Loader.load(Loader.java:657)
at org.bytedeco.javacpp.openblas.&lt;clinit&gt;(openblas.java:10)
at org.nd4j.linalg.cpu.nativecpu.blas.CpuBlas.setMaxThreads(CpuBlas.java:87)
at org.nd4j.nativeblas.Nd4jBlas.&lt;init&gt;(Nd4jBlas.java:36)
at org.nd4j.linalg.cpu.nativecpu.blas.CpuBlas.&lt;init&gt;(CpuBlas.java:11)
at org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.createBlas(CpuNDArrayFactory.java:79)
at org.nd4j.linalg.factory.BaseNDArrayFactory.blas(BaseNDArrayFactory.java:71)
at org.nd4j.linalg.cpu.nativecpu.blas.CpuLevel3.&lt;init&gt;(CpuLevel3.java:26)
at org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.createLevel3(CpuNDArrayFactory.java:94)
at org.nd4j.linalg.factory.BaseNDArrayFactory.level3(BaseNDArrayFactory.java:92)
at org.nd4j.linalg.factory.BaseBlasWrapper.level3(BaseBlasWrapper.java:42)
at org.nd4j.linalg.api.ndarray.BaseNDArray.mmuli(BaseNDArray.java:2849)
at org.nd4j.linalg.api.ndarray.BaseNDArray.mmul(BaseNDArray.java:2643)
at org.deeplearning4j.nn.layers.BaseLayer.preOutput(BaseLayer.java:373)
at org.deeplearning4j.nn.layers.BaseLayer.activate(BaseLayer.java:384)
at org.deeplearning4j.nn.layers.BaseLayer.activate(BaseLayer.java:405)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:590)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:713)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:1821)
at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:151)
at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:54)
at org.deeplearning4j.optimize.Solver.optimize(Solver.java:51)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1031)
at Tester.main(Tester.java:96)
Caused by: java.lang.UnsatisfiedLinkError: no openblas in java.library.path
at java.lang.ClassLoader.loadLibrary(Unknown Source)
at java.lang.Runtime.loadLibrary0(Unknown Source)
at java.lang.System.loadLibrary(Unknown Source)
at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:945)
at org.bytedeco.javacpp.Loader.load(Loader.java:738)
... 24 more
</code></pre>
","<java><deeplearning4j><dl4j>","2019-03-17 18:38:26","","0","11217239","2019-03-17 20:09:26","1","2","","","11217239","1","0","0"
"43927079","<p>Regularly, a simple neural network to solve XOR should have 2 inputs, 2 neurons in hidden layer, 1 neuron in output layer.</p>

<p>However, the following example implementation has 2 output neurons, and I don't get it:</p>

<p><a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/feedforward/xor/XorExample.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/feedforward/xor/XorExample.java</a></p>

<p>Why did the author put 2 output neurons in there?</p>

<p><strong>Edit:
Author of the example noted that he is using 4 neurons in hidden layer, 2 neurons in output layer. But I still don't get it why, why a shape of {4,2} instead of {2,1}?</strong></p>
","<neural-network><artificial-intelligence><deep-learning><xor><deeplearning4j>","2017-05-11 23:22:46","","2","5581893","2019-03-30 07:28:04","3","3","2","","5581893","1070","4789","3"
"45472557","<p>I am trying to find a GRU implementation within DeepLearning4J but cannot seem to find one.  Does anyone know if GRU's are implemented within DL4J?  If so can you please direct me to an example.  If not, is this on their roadmap anywhere?</p>

<p>Thanks</p>
","<deep-learning><deeplearning4j><rnn><gated-recurrent-unit>","2017-08-02 23:32:29","","1","59535","2017-08-03 07:12:40","1","1","","","59535","1949","107","2"
"44032588","<p>I have been trying to find if there is a way to capture and save(as json) weights of each layer of neural network while training using DL4J(deeplearning4j on spark).</p>

<p>Please let me know if someone has any idea.</p>
","<apache-spark><deep-learning><deeplearning4j><dl4j>","2017-05-17 18:42:41","","-2","5469218","2017-05-18 00:45:43","1","0","","","5469218","202","8","0"
"54391916","<p>The score for my model is dropping far too fast. For a huge dataset it's declining from > 5 to 0 well before the first epoch is complete.</p>

<p>I suspect I may have misconfigured it somehow and perhaps each batch contains the same data.</p>

<p>Is there any way to print out the inputs and outputs for each row during training so I can test this theory?</p>
","<deeplearning4j><dl4j>","2019-01-27 19:13:39","","1","654128","2019-01-27 19:13:39","0","0","","","654128","812","146","3"
"41481521","<p>The official guides for Deeplearning4j show how to use .csv files, but I want to know how to use my custom models with it. I tried looking for an appropriate DataSet implementation but can't seem to find any. Even if it would take the contents (in string format) of a normal .csv it would be good enough. I tried doing it like this:</p>

<p>Model:</p>

<pre><code>package com.example.kamil.deeplearningandroid;

public class Job implements LearnableModel {
private int type;
private int salary;
private int choice;

public Job(String type, int salary, boolean choice) {
    this.type = encodeType(type);
    this.salary = salary;
    this.choice = encodeChoice(choice);
}

private int encodeType(String job) {
    switch (job) {
        case ""Mechanic"": return 0;
        case ""Programmer"": return 1;
        case ""Teacher"": return 2;
        case ""Driver"": return 3;
        case ""Cook"": return 4;
        default: return 5;
    }
}

private int encodeChoice(boolean choice) {
    return choice ? 1: 0;
}

@Override
public String toString() {
    return type + SEPARATOR + salary + SEPARATOR + choice + ""\n"";
}
}
</code></pre>

<p>and in JobClassifier:</p>

<pre><code> private DataSet readStringDataset(List&lt;LearnableModel&gt; data, int batchSize, int labelIndex, int numClasses) throws IOException, InterruptedException {
    RecordReader rr = new LineRecordReader();
    rr.initialize(new StringSplit(modelToString(data)));
    DataSetIterator iterator = new RecordReaderDataSetIterator(rr,batchSize,labelIndex,numClasses);
    return iterator.next();
}

private String modelToString(List&lt;LearnableModel&gt; list) {
    StringBuilder sb = new StringBuilder();
    for (LearnableModel model: list) {
        sb.append(model.toString());
    }
    return sb.toString();
}
</code></pre>

<p>With all this I'm getting:</p>

<pre><code>W/System.err: java.lang.NumberFormatException: Invalid double: ""1,10,0
W/System.err: 1,15,1
W/System.err: 4,7,0
W/System.err: 5,10,1
W/System.err: 3,10,0
W/System.err: 3,20,0
W/System.err: 4,5,0
W/System.err: 4,12,1
W/System.err: 2,20,1
W/System.err: 2,4,0
W/System.err: 5,12,1
W/System.err: 0,10,0
W/System.err: 5,5,0
W/System.err: 1,10,0
W/System.err: 2,16,1
W/System.err: 3,30,1
W/System.err: 4,16,1
W/System.err: 5,19,1
W/System.err: 5,6,0
W/System.err: 1,11,0""
W/System.err:     at java.lang.StringToReal.invalidReal(StringToReal.java:63)
W/System.err:     at java.lang.StringToReal.initialParse(StringToReal.java:164)
W/System.err:     at java.lang.StringToReal.parseDouble(StringToReal.java:282)
W/System.err:     at java.lang.Double.parseDouble(Double.java:301)
W/System.err:     at org.datavec.api.writable.Text.toDouble(Text.java:601)
W/System.err:     at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.getDataSet(RecordReaderDataSetIterator.java:271)
W/System.err:     at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:177)
W/System.err:     at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:372)
W/System.err:     at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:52)
W/System.err:     at com.example.kamil.deeplearningandroid.JobClassifier.readStringDataset(JobClassifier.java:185)
W/System.err:     at com.example.kamil.deeplearningandroid.JobClassifier.classify(JobClassifier.java:65)
W/System.err:     at com.example.kamil.deeplearningandroid.MainActivity.onCreate(MainActivity.java:23)
W/System.err:     at android.app.Activity.performCreate(Activity.java:6251)
W/System.err:     at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1107)
W/System.err:     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2369)
W/System.err:     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2476)
W/System.err:     at android.app.ActivityThread.-wrap11(ActivityThread.java)
W/System.err:     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1344)
W/System.err:     at android.os.Handler.dispatchMessage(Handler.java:102)
W/System.err:     at android.os.Looper.loop(Looper.java:148)
W/System.err:     at android.app.ActivityThread.main(ActivityThread.java:5417)
W/System.err:     at java.lang.reflect.Method.invoke(Native Method)
W/System.err:     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)
W/System.err:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)
</code></pre>
","<java><android><csv><deep-learning><deeplearning4j>","2017-01-05 09:37:07","","0","4881418","2017-01-05 09:59:42","1","0","","","4881418","161","9","0"
"44570542","<p>I want to submit my jar on cluster using master=yarn-cluster , but get error.</p>

<pre><code>java.lang.NoClassDefFoundError: org/nd4j/Nd4jRegistrator
        at Main.main(Main.java:51)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:552)
Caused by: java.lang.ClassNotFoundException: org.nd4j.Nd4jRegistrator
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
</code></pre>

<p>I set sparkConf like this, using krio serializer, because i had exceptions when i run without it</p>

<pre><code>SparkConf sparkConf = new SparkConf().setMaster(master).setAppName(""DL4J Spark Example"");
sparkConf.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"");
sparkConf.set(""spark.kryo.registrator"", ""org.nd4j.Nd4jRegistrator"");
JavaSparkContext sc = new JavaSparkContext(sparkConf);
</code></pre>

<p>dependencies in pom file:</p>

<pre><code>&lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
            &lt;version&gt;1.6.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
            &lt;version&gt;0.8.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;dl4j-spark_2.10&lt;/artifactId&gt;
            &lt;version&gt;0.8.0_spark_1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
            &lt;version&gt;0.8.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;nd4j-kryo_2.10&lt;/artifactId&gt;
            &lt;version&gt;0.8.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt;
            &lt;artifactId&gt;kryo&lt;/artifactId&gt;
            &lt;version&gt;4.0.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
</code></pre>

<p>it can't find Nd4jRegistrator class, but why?
When i run local this works correctly.</p>
","<java><maven><apache-spark><deeplearning4j>","2017-06-15 14:46:54","","0","8113890","2017-06-15 23:43:09","1","0","1","","8113890","1","0","0"
"48441911","<p>I am trying to extract the layer activations to save them locally as features. 
I'm still new to CNNs so I'd like to show what I did and I'd like to know if what I'm doing is correct:</p>

<pre><code>public static void main(String[] args) throws IOException {
    ComputationGraph vgg16transfer = getComputationGraph();

    for (File file : new File(ImageClassifier.class.getClassLoader().getResource(""mydirectory"").getFile()).listFiles()) {
        Map&lt;String, INDArray&gt; stringINDArrayMap = extractTwo(file, vgg16transfer);
        //Extract the features from the last fully connected layers
        saveCompressed(file,stringINDArrayMap.get(""fc2""));
    }
}

/**
 * Retrieves the VGG16 computation graph
 * @return ComputationGraph from the pretrained VGG16
 * @throws IOException
 */
public static ComputationGraph getComputationGraph() throws IOException {
    ZooModel zooModel = new VGG16();
    return (ComputationGraph) zooModel.initPretrained(PretrainedType.IMAGENET);
}

/**
 * Compresses the input INDArray and writes it to file
 * @param imageFile the original image file
 * @param array INDArray to be saved (features)
 * @throws IOException
 */
private static void saveCompressed(File imageFile, INDArray array) throws IOException {
    INDArray compress = BasicNDArrayCompressor.getInstance().compress(array);
    Nd4j.write(compress,new DataOutputStream(new FileOutputStream(new File(""features/"" + imageFile.getName()+ ""feat""))));
}

/**
 * Given an input image and a ComputationGraph it calls the feedForward method after rescaling the image.
 * @param imageFile the image whose features need to be extracted 
 * @param vgg16 the ComputationGraph to be used.
 * @return a map of activations for each layer
 * @throws IOException
 */
public static Map&lt;String, INDArray&gt; extractTwo(File imageFile, ComputationGraph vgg16) throws IOException {
    // Convert file to INDArray
    NativeImageLoader loader = new NativeImageLoader(224, 224, 3);
    INDArray image = loader.asMatrix(imageFile);

    // Mean subtraction pre-processing step for VGG
    DataNormalization scaler = new VGG16ImagePreProcessor();
    scaler.transform(image);

    //Call the feedForward method to get a map of activations for each layer
    return vgg16.feedForward(image, false);
}
</code></pre>

<p>So basically I am calling the feedForward method and obtaining the activations from the fc2 layer.</p>

<p>I have several questions about this: </p>

<p>1) Is the code I wrote indeed extracting features that can be saved and stored for further usages?</p>

<p>2) How would I go about doing PCA/Whitening on the extracted features?</p>

<p>3) Is there any way I could encode this to VLAD as proposed but such paper: <a href=""https://arxiv.org/pdf/1707.00058.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1707.00058.pdf</a></p>

<p>4) I then would like to compare the saved features, I did so using a simple Euclidean distance and it seems to be working although results are not the best. Is there some kind of pre-processing I should do or are the saved features directly comparable?</p>

<p>Thanks.</p>
","<java><multidimensional-array><neural-network><deep-learning><deeplearning4j>","2018-01-25 11:35:19","","1","2026366","2018-01-26 00:55:22","1","1","","","2026366","577","63","21"
"51320032","<p>My goal is to have an autoencoding network where I can train the identity function and then do forward passes yielding a reconstruction of the input.</p>

<p>For this, I'm trying to use <code>VariationalAutoencoder</code>, e.g. something like:</p>

<pre class=""lang-java prettyprint-override""><code>MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(77147718)
                .trainingWorkspaceMode(WorkspaceMode.NONE)
                .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)
                .gradientNormalizationThreshold(1.0)
                .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)
                .list()
                .layer(0, new VariationalAutoencoder.Builder()
                        .activation(Activation.LEAKYRELU)
                        .nIn(100).nOut(15)
                        .encoderLayerSizes(120, 60, 30)
                        .decoderLayerSizes(30, 60, 120)
                        .pzxActivationFunction(Activation.IDENTITY)
                        .reconstructionDistribution(new BernoulliReconstructionDistribution(Activation.SIGMOID.getActivationFunction()))
                        .build())
                .pretrain(true).backprop(false)
                .build();
</code></pre>

<p>However, <code>VariationalAutoencoder</code> seems to be designed for training (and providing) mappings from an input to an encoded version, i.e. a vector of size 100 to a vector of size 15 in above example configuration.</p>

<p>However, I'm not particularly interested in the encoded version, but would like to train a mapping of a 100-vector to itself. Then, I'd like to run a other 100-vectors through it and get back their reconstructed versions.</p>

<p>But even when looking at the API of of the <code>VariationalAutoencoder</code> (or <code>AutoEncoder</code> too), I can't figure out how to do this. Or are those layers not designed for this kind of ""end-to-end usage"" and I would have to manually construct an autoencoding network?</p>
","<autoencoder><deeplearning4j><dl4j>","2018-07-13 07:40:26","","0","1090166","2018-07-19 02:49:35","1","0","","","1090166","5175","1111","141"
"40921213","<p>I need to use <strong>deeplearning4j</strong> library in a new java project.</p>

<p>I downloaded .jar (in particoular deeplearning4j-core and ndj4-api-platform) from maven library.</p>

<p>I imported that in eclipse</p>

<p>I receive an error because <em>org.ndj4.api.complex.IComplexNumber</em> is not found.</p>

<p>Where may I find a <em>.jar</em> with that <strong>classes?</strong></p>
","<java><eclipse><maven><deeplearning4j>","2016-12-01 22:03:01","","-1","6774731","2016-12-02 01:15:42","1","4","","","6774731","35","4","0"
"45897616","<p>I'm currently trying to get dl4j (deeplearning4j) to import my model that I trained in keras 1.2.</p>

<p>This is my code:</p>

<pre><code>public static void main( String[] args )
{
    try {
        MultiLayerNetwork network = KerasModelImport.importKerasSequentialModelAndWeights(
                ""C:\\Users\\A\\Documents\\GitHub\\DevanagriRecognizer\\model_keras1.h5"");
        System.out.println( ""Hello World!"" );
    } catch (IOException e) {
        e.printStackTrace();
    } catch (InvalidKerasConfigurationException e) {
        e.printStackTrace();
    } catch (UnsupportedKerasConfigurationException e) {
        e.printStackTrace();
    }
}
</code></pre>

<p>I'm using Maven to handle the dependencies, and this is my first time using it. (That might be relevant)</p>

<p>When I run the above code I get a ExceptionInInitializerError caused by UnsatisfiedLinkError: no jnind4jcpu in java.library.path.</p>

<p>It looks like a missing dependency, but I have no idea how to fix it.</p>

<p>This is my pom.xml: <a href=""https://pastebin.com/FzAMwA0z"" rel=""nofollow noreferrer"">https://pastebin.com/FzAMwA0z</a></p>

<p>And this is my full stacktrace: <a href=""https://pastebin.com/a2kyUtch"" rel=""nofollow noreferrer"">https://pastebin.com/a2kyUtch</a></p>

<p>By the way, I'm using IntelliJ with Java 1.8u101 on 64-bit Windows 10</p>
","<java><maven><deeplearning4j><dl4j>","2017-08-26 16:44:52","","0","6202029","2017-08-30 19:20:11","3","0","","","6202029","390","111","7"
"49748734","<p>I am triying to configure an RNN neural netwwork in order to predict 5 different types of text entities. I am using the next configuration:</p>

<pre><code>    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(seed)
            .iterations(100)
            .updater(Updater.ADAM)  //To configure: .updater(Adam.builder().beta1(0.9).beta2(0.999).build())
            .regularization(true).l2(1e-5)
            .weightInit(WeightInit.XAVIER)
            .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue).gradientNormalizationThreshold(1.0)
            .learningRate(2e-2)
            .trainingWorkspaceMode(WorkspaceMode.SEPARATE).inferenceWorkspaceMode(WorkspaceMode.SEPARATE)   //https://deeplearning4j.org/workspaces
            .list()
            .layer(0, new GravesLSTM.Builder().nIn(500).nOut(3)
                    .activation(Activation.TANH).build())
            .layer(1, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX)        //MCXENT + softmax for classification
                    .nIn(3).nOut(5).build())
            .pretrain(false).backprop(true).build();
  MultiLayerNetwork net = new MultiLayerNetwork(conf);
  net.init();
</code></pre>

<p>I train it and then I evaluate it. It works. Nevertheless when I use:</p>

<pre><code> int[] prediction = net.predict(features);
</code></pre>

<p>Sometimes it retuns and unexpected predictions. It returns correct predictions as 1,2....5 but sometimes it returns numbers as 9,14,12... This numbers not corresponds to an recognised prediction/label.</p>

<p>Why this configuration return unexpected outputs?</p>
","<java><deep-learning><rnn><deeplearning4j>","2018-04-10 08:21:08","","5","1708774","2018-06-27 15:48:59","1","4","2","2018-06-29 22:09:24","1708774","609","505","12"
"48549651","<p>I am just going through basic tutorials for DL4J. And I am planning to compare similarity between two unseen sentences.
I used a simple example to compare 2 words once W2V is done using <code>GoogleNews-vectors-negative300.bin.gz</code>.
When I tried using GoogleNews-vectors-negative300.bin.gz for sentence comparision like below:</p>

<pre><code>File gModel = new File(""GoogleNews-vectors-negative300.bin.gz"");
Word2Vec vecGoogle = WordVectorSerializer.readWord2VecModel(gModel);

ParagraphVectors vecGoogleForSentences = new ParagraphVectors.Builder()
.useExistingWordVectors(vecGoogle)
.build();

System.out.println(Transforms.cosineSim(vecGoogleForSentences.inferVector(""I like bananas and mangoes""), vecGoogleForSentences.inferVector(""I like mangoes"")));
</code></pre>

<p>I get error:  </p>

<blockquote>
  <p>org.nd4j.linalg.exception.ND4JIllegalStateException: Model being
  passed as existing has no syn1/syn1Neg available</p>
</blockquote>

<p>Can someone please explain what I am hitting here or how I can compare 2 unseen sentences semantically using vector GoogleNews-vectors-negative300.bin.gz?
What I am trying is based on suggestion given in <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/paragraphvectors/ParagraphVectorsInferenceExample.java"" rel=""nofollow noreferrer"">DL4J demo code</a>:</p>

<p>Much thanks in Advance !</p>
","<nlp><deeplearning4j><dl4j>","2018-01-31 19:05:40","","1","2702249","2018-02-01 06:18:40","1","1","","","2702249","1501","105","11"
"42806761","<p>I'm trying to implement something like this <a href=""https://www.youtube.com/watch?v=Fp9kzoAxsA4"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=Fp9kzoAxsA4</a> which is a GANN (Genetic Algorithm Neural Network) using DL4J library.</p>

<p>Genetic learning variables:</p>

<ul>
<li><strong>Genes</strong>: Creature Neural Network weights</li>
<li><strong>Fitness</strong>: Total distance moved.</li>
</ul>

<p>Neural network layers for every creature:</p>

<ul>
<li><strong>input layer</strong>: 5 sensors that either <code>1</code> if there's a wall in the sensor direction or <code>0</code> if not.<a href=""https://i.stack.imgur.com/YXD58.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YXD58.png"" alt=""enter image description here""></a></li>
<li><strong>output layer</strong>: Linear output that maps to the angle of the creature.</li>
</ul>

<p>This is my <code>createBrain</code> method for the creature object:</p>

<pre><code>private void createBrain() {
    Layer inputLayer = new DenseLayer.Builder()
            // 5 eye sensors
            .nIn(5)
            .nOut(5)
            // How do I initialize custom weights using creature genes (this.genes)?
            // .weightInit(WeightInit.ZERO)
            .activation(Activation.RELU)
            .build();

    Layer outputLayer = new OutputLayer.Builder()
            .nIn(5)
            .nOut(1)
            .activation(Activation.IDENTITY)
            .lossFunction(LossFunctions.LossFunction.MSE)
            .build();

    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(6)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .iterations(1)
            .learningRate(0.006)
            .updater(Updater.NESTEROVS).momentum(0.9)
            .list()
            .layer(0,inputLayer)
            .layer(1, outputLayer)
            .pretrain(false).backprop(true)
            .build();

    this.brain = new MultiLayerNetwork(conf);
    this.brain.init();
}
</code></pre>

<p>If it might help I have pushed to this repo
<a href=""https://github.com/kareem3d/GeneticNeuralNetwork"" rel=""nofollow noreferrer"">https://github.com/kareem3d/GeneticNeuralNetwork</a></p>

<p>And this is the Creature class
<a href=""https://github.com/kareem3d/GeneticNeuralNetwork/blob/master/src/main/java/com/mycompany/gaan/Creature.java"" rel=""nofollow noreferrer"">https://github.com/kareem3d/GeneticNeuralNetwork/blob/master/src/main/java/com/mycompany/gaan/Creature.java</a></p>

<p>I'm a machine learning student so if you see any obvious mistakes please let me know, thanks :)</p>
","<machine-learning><deeplearning4j><dl4j>","2017-03-15 10:16:32","","1","1460518","2019-02-02 10:35:23","2","0","2","","1460518","113","0","0"
"53575216","<p>I have a project which taking the small .wav file and process them. I'm trying to use dl4j, but the examples didn't help much and I don't know where to start.</p>

<p>My first idea, converting the .wav file to byte array, then normalizing the data, and finally giving data to dl4j. But it takes too much time. And I'm not sure this method gonna work or not.</p>
","<deeplearning4j><dl4j><nd4j>","2018-12-01 21:28:24","","0","6841566","2018-12-01 21:28:24","0","0","","","6841566","26","1","0"
"41989197","<p>I am trying a time prediction model in deeplearning4j for text processing which takes no of words,sentences,char as input features and produces time as output.But while modelling input data to output i am having difficulties to transform these values and how to tell the network for these values of input these are respective output values.</p>

<p>Also should i reduce dimensionality from just having x1 and y.instead of x1-x4?</p>

<p>training-data.csv has the below columns with 100 values.
x1,x2,x3,x4(inputs) y(output)</p>

<p>I tried using SequenceRecorder and Iterator which can capture variant inputs.
below is my code</p>

<pre><code>public static void main(String[] args) throws Exception
{
    // Initlizing parametres
    final Logger log = LoggerFactory.getLogger(MainExpert.class);
    final int seed =123;
    final int numInput = 4;
    final int numOutput = 1;
    final int numHidden = 20;
    final double learningRate = 0.015;
    final int batchSize =30;
    final int nEpochs =30;
    //final int inputFeatures =4;

    //Constructing Training data

    final File baseFolder =new File(""/home/aj/my/samples/corpus"");
    final File testFolder = new File(""/home/aj/my/samples/corpus/train_data_0.csv"");
    SequenceRecordReader trainReader = new CSVSequenceRecordReader(0,"","");
    trainReader.initialize(new NumberedFileInputSplit(baseFolder.getAbsolutePath() + ""/train_data_%d.csv"",0,0));
    DataSetIterator trainIterator = new SequenceRecordReaderDataSetIterator(trainReader,batchSize,-1,4,true);


    SequenceRecordReader testReader = new CSVSequenceRecordReader(0,"","");
    testReader.initialize(new NumberedFileInputSplit(baseFolder.getAbsolutePath() + ""/test_data_%d.csv"",0,0));
    DataSetIterator testIterator = new SequenceRecordReaderDataSetIterator(testReader,batchSize,-1,4,true);

    DataSet trainData = trainIterator.next();
    System.out.println(trainData);
    DataSet testData = testIterator.next();

    NormalizerMinMaxScaler normalizer = new NormalizerMinMaxScaler(0, 1);
    normalizer.fitLabel(true);
    normalizer.fit(trainData);              
    normalizer.transform(trainData);
    normalizer.transform(testData);

    //Configuring Network
    log.info(""Building Model"");
    MultiLayerConfiguration config = new NeuralNetConfiguration.Builder()
            .seed(seed)
            .iterations(1)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .learningRate(learningRate)
            .updater(Updater.NESTEROVS).momentum(0.9)
            .list()
            .layer(0, new DenseLayer.Builder()
                    .nIn(numInput)
                    .nOut(numHidden)
                    .weightInit(WeightInit.XAVIER).
                    activation(Activation.RELU)
                    .build())
            .layer(1, new DenseLayer.Builder()
                    .nIn(numHidden)
                    .nOut(numHidden)
                    .weightInit(WeightInit.XAVIER)
                    .activation(Activation.RELU)
                    .build())
            .layer(2, new OutputLayer.Builder(LossFunction.MSE)
                    .nIn(numHidden)
                    .nOut(numOutput)
                    .weightInit(WeightInit.XAVIER)
                    .activation(Activation.IDENTITY)
                    .build())
            .pretrain(false).backprop(true).build();

    //Initializing network
    log.info(""initlizing model"");
    MultiLayerNetwork model = new MultiLayerNetwork(config);
    model.init();
    model.setListeners(new ScoreIterationListener(1));

    log.info(""Training Model"");
    for(int i=0;i&lt;nEpochs;i++)
    {
        model.fit(trainData);
    }
    //Evaluation
    RegressionEvaluation reval=new RegressionEvaluation(1);

        while(testIterator.hasNext())
        {
    INDArray feat =testData.getFeatureMatrix();
    INDArray labels =testData.getLabels();
    INDArray prediction =model.output(feat);
    reval.eval(labels, prediction);
}
    System.out.println(reval.stats());
}
</code></pre>

<p>}</p>

<p>my data has four input values and one output values.
But i get an exception
 <code>org.deeplearning4j.exception.DL4JInvalidInputException: Input that is not a matrix; expected matrix (rank 2), got rank 3 array with shape [1, 4, 107]</code></p>
","<neural-network><deeplearning4j>","2017-02-01 20:32:00","","0","7070240","2017-02-02 12:37:22","1","0","","","7070240","31","0","0"
"43292890","<p>Im new to deeplearning4J. I already experimented with its word2vec functionality and everything was fine. But now I am little bit confused regarding image classification. I was playing with this example: </p>

<p><a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/AnimalsClassification.java"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/AnimalsClassification.java</a></p>

<p>I changed the ""save"" flag to true and my model is stored into model.bin file. 
Now comes the problematic part (I am sorry if this sounds as silly question, maybe I am missing something really obvious here)</p>

<p>I created separate class called AnimalClassifier and its purpose is to load model from model.bin file, restore neural network from it and then classify single image using restored network. For this single image I created ""temp"" folder -> dl4j-examples/src/main/resources/animals/temp/ where I put picture of polar bear that was previously used in training process in AnimalsClassification.java (I wanted to be sure that image would be classified correctly - therefore I reused picture from ""bear"" folder).</p>

<p>This my code trying to classify polar bear:</p>

<pre><code>protected static int height = 100;
    protected static int width = 100;
    protected static int channels = 3;
    protected static int numExamples = 1;
    protected static int numLabels = 1;
    protected static int batchSize = 10;

    protected static long seed = 42;
    protected static Random rng = new Random(seed);
    protected static int listenerFreq = 1;
    protected static int iterations = 1;
    protected static int epochs = 7;
    protected static double splitTrainTest = 0.8;
    protected static int nCores = 2;
    protected static boolean save = true;

    protected static String modelType = ""AlexNet""; //

    public static void main(String[] args) throws Exception {

        String basePath = FilenameUtils.concat(System.getProperty(""user.dir""), ""dl4j-examples/src/main/resources/"");
        MultiLayerNetwork multiLayerNetwork = ModelSerializer.restoreMultiLayerNetwork(basePath + ""model.bin"", true);

        ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator();
        File mainPath = new File(System.getProperty(""user.dir""), ""dl4j-examples/src/main/resources/animals/temp/"");
        FileSplit fileSplit = new FileSplit(mainPath, NativeImageLoader.ALLOWED_FORMATS, rng);
        BalancedPathFilter pathFilter = new BalancedPathFilter(rng, labelMaker, numExamples, numLabels, batchSize);


        InputSplit[] inputSplit = fileSplit.sample(pathFilter, 1);
        InputSplit analysedData = inputSplit[0];


        ImageRecordReader recordReader = new ImageRecordReader(height, width, channels);
        recordReader.initialize(analysedData);
        DataSetIterator dataIter = new RecordReaderDataSetIterator(recordReader, batchSize, 0, 4);
        while (dataIter.hasNext()) {
            DataSet testDataSet = dataIter.next();

            String expectedResult = testDataSet.getLabelName(0);
            List&lt;String&gt; predict = multiLayerNetwork.predict(testDataSet);
            String modelResult = predict.get(0);
            System.out.println(""\nFor example that is labeled "" + expectedResult + "" the model predicted "" + modelResult + ""\n\n"");
        }
    }
</code></pre>

<p>After running this, I get error: </p>

<p>java.lang.UnsupportedOperationException
    at org.datavec.api.writable.ArrayWritable.toInt(ArrayWritable.java:47)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.getDataSet(RecordReaderDataSetIterator.java:275)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:186)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:389)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:52)
    at org.deeplearning4j.examples.convolution.AnimalClassifier.main(AnimalClassifier.java:66)
Disconnected from the target VM, address: '127.0.0.1:63967', transport: 'socket'
Exception in thread ""main"" java.lang.IllegalStateException: Label names are not defined on this dataset. Add label names in order to use getLabelName with an id.
    at org.nd4j.linalg.dataset.DataSet.getLabelName(DataSet.java:1106)
    at org.deeplearning4j.examples.convolution.AnimalClassifier.main(AnimalClassifier.java:68)</p>

<p>I can see there is a method public void setLabels(INDArray labels) in MultiLayerNetwork.java but I don't get how to use (especially when it takes as argument INDArray).</p>

<p>I am also confused why I have to specify number of possible labels in constructor of RecordReaderDataSetIterator. I would expect that model already knows which labels to use (should not it use labels that were used during training automatically?). I guess, maybe I am loading the picture in completely wrong way...</p>

<p>So to summarize, I would like to achieve simply following:</p>

<ol>
<li>restore network from model (this is working)</li>
<li>load image to be classified (also working)</li>
<li>classify this image using the same labels that were used during training (bear, deer, duck, turtle) (tricky part)</li>
</ol>

<p>Thank you in advance for your help or any hints !</p>
","<deeplearning4j>","2017-04-08 10:29:29","","0","2352206","2017-04-09 21:14:56","1","3","","","2352206","18","1","0"
"40585416","<p>I'm getting below error while running spark program using spark-submit.</p>

<p>My spark-cluster is of version 2.0.0 and I use sbt to compile my code and below is my sbt dependencies. </p>

<pre><code>libraryDependencies ++= Seq(
  ""commons-io"" % ""commons-io"" % ""2.4"",
  ""com.google.guava"" % ""guava"" % ""19.0"",
  ""jfree"" % ""jfreechart"" % ""1.0.13"",
  (""org.deeplearning4j"" % ""deeplearning4j-core"" % ""0.5.0"").exclude(""org.slf4j"", ""slf4j-log4j12""),
  ""org.jblas"" % ""jblas"" % ""1.2.4"",
  ""org.nd4j"" % ""canova-nd4j-codec"" % ""0.0.0.15"",
  ""org.nd4j"" % ""nd4j-native"" % ""0.5.0"" classifier """" classifier ""linux-x86_64"",
  ""org.deeplearning4j"" % ""dl4j-spark"" % ""0.4-rc3.6"" ,
  ""org.apache.spark"" % ""spark-sql_2.10"" % ""1.3.1"", 
  ""org.apache.spark"" % ""spark-hive_2.10"" % ""1.3.1"",
  ""org.apache.hive"" % ""hive-serde"" % ""0.14.0"", 
  (""org.deeplearning4j"" % ""arbiter-deeplearning4j"" % ""0.5.0""))



16/11/14 22:57:03 INFO hive.HiveSharedState: Warehouse path is 'file:/home/hduser/spark-warehouse'.
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.spark.sql.hive.HiveContext.sql(Ljava/lang/String;)Lorg/apache/spark/sql/DataFrame;
    at poc.common.utilities.StockData$.fetchStockData(StockData.scala:15)
    at poc.analaticsEngine.AnalaticsStockWorkBench.fetchTrainingDataSet(AnalaticsStockWorkBench.scala:69)
    at poc.analaticsEngine.AnalaticsStockWorkBench.trainModel(AnalaticsStockWorkBench.scala:79)
    at test.poc.analatics.StockPrediction$.testTrainSaveModel(StockPrediction.scala:21)
    at test.poc.analatics.StockPrediction$.main(StockPrediction.scala:10)
    at test.poc.analatics.StockPrediction.main(StockPrediction.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/11/14 22:57:03 INFO spark.SparkContext: Invoking stop() from shutdown hook
</code></pre>
","<apache-spark><deeplearning4j><hivecontext>","2016-11-14 09:27:53","","0","6602033","2019-02-28 14:22:18","2","1","","","6602033","36","1","0"
"39604806","<p>As a beginner in <em>deep learning</em> I am currently practicing using the project called <a href=""http://deeplearning4j.org/"" rel=""nofollow"">deep learning 4 j</a>.</p>

<p>I am getting really good at the ""training"" and ""evaluating"" part of deep learning, but how am I supposed to deploy my finished network in an application? It seems that there are no good example of using a network ( a.k.a prediction) in the <a href=""https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples"" rel=""nofollow"">examples for dl4j</a>.</p>

<p>So to be 100% concrete; in a <em>java context</em>, what is the leanest way to deploy a network/model created in <strong>dl4j</strong> for use in an application such as a mobile phone app (<strong>Android</strong>)?</p>

<p>Are there any <strong>maven</strong>/<strong>gradle</strong> lines that will get me just the dependencies needed for prediction (as opposed to the whole toolset)?</p>

<p>What is a minimal source code example for using my network?</p>
","<java><maven><deployment><deep-learning><deeplearning4j>","2016-09-20 22:38:55","","3","1035897","2017-06-20 02:41:30","2","0","1","","1035897","4873","811","18"
"36825287","<p>I am trying to implement an image search engine using AlexNethttps://github.com/akrizhevsky/cuda-convnet2</p>

<p>The idea is to implement an image search engine by training a neural net to classify images and then using the code from the net's last hidden layer as a similarity measure.</p>

<p>I am trying to figure out how to train the CNN on a new set of images to classify them. Does anyone know how to get started with this?</p>

<p>Thanks</p>
","<deep-learning><deeplearning4j>","2016-04-24 15:37:51","","0","6247894","2016-04-25 09:26:24","1","1","","","6247894","1","0","0"
"52875029","<p>I am going to use DL4J for finding a good model against a conditional matrix.  I have prepared the CSV-like dataset (sample as at below) and after fine tuned the hyperparamaters and trained the model for many times, I still cannot get an reasonable Precision, Recall and F1 results.  May I ask if I have implemented anything wrongly?</p>

<p><strong>Sample dataset:</strong></p>

<p><em>## Basically each column defines whether a condition did exist (1) or not (0) for each sample. The first column is the label class only have 2 output, i.e. 1/0</em></p>

<pre><code>[1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1]
[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]
...........
</code></pre>

<p><strong>DataVec portion:</strong></p>

<pre><code>int OUTPUT_NEURONS = 2;  // Only 2 classes for output
int CLASS_INDEX = 0;     // First column is the label
int FILE_SIZE = 0;       // FILE_SIZE will be calculated while preparing the datavecRecords below

List&lt;List&lt;Writable&gt;&gt; datavecRecords = new ArrayList&lt;&gt;();

......
Prepare the datavecRecords using above csv data 
......

CollectionRecordReader crr = new CollectionRecordReader(datavecRecords);
RecordReaderDataSetIterator iter = new RecordReaderDataSetIterator(crr, FILE_SIZE, CLASS_INDEX, OUTPUT_NEURONS);
allData = iter.next();

SplitTestAndTrain testAndTrain = allData.splitTestAndTrain(0.6);
DataSet trainingData = testAndTrain.getTrain();
DataSet testData = testAndTrain.getTest();

DataNormalization normalizer = new NormalizerStandardize();
normalizer.fit(trainingData);
normalizer.transform(trainingData);
normalizer.transform(testData);

// For early escaping use
DataSetIterator trainSetIterator = new ListDataSetIterator(trainingData.asList()); 
DataSetIterator testSetIterator = new ListDataSetIterator(testData.asList()); 

// sortedKeys is the calculated number of input columns

INPUT_NEURONS = sortedKeys.size() - 1; 
HIDDEN_NEURONS = FILE_SIZE / (2 * (INPUT_NEURONS + OUTPUT_NEURONS));
HIDDEN_NEURONS = HIDDEN_NEURONS &lt;= 0 ? 1 : HIDDEN_NEURONS;
</code></pre>

<p><strong>Model:</strong></p>

<pre><code>int n=0;
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .seed(12345)
    .iterations(1)
    .learningRate(0.001)
    .weightInit(WeightInit.XAVIER)
    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
    .regularization(true).l2(1e-4)
    .updater(new Nesterovs(0.001,0.9))
    .list()
        .layer(n++, new DenseLayer.Builder()
            .nIn(INPUT_NEURONS)
            .nOut(HIDDEN_NEURONS)
            .activation(Activation.RELU)
            .build())
        .layer(n++, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD)
            .nIn(HIDDEN_NEURONS)
            .nOut(OUTPUT_NEURONS)
            .activation(Activation.SOFTMAX)
            .build())
    .pretrain(false).backprop(true).build();            

EarlyStoppingConfiguration esConf = new EarlyStoppingConfiguration.Builder()
    .epochTerminationConditions(
        new MaxEpochsTerminationCondition(10000), 
        new ScoreImprovementEpochTerminationCondition(50))
    .iterationTerminationConditions(new MaxTimeIterationTerminationCondition(5, TimeUnit.MINUTES))
    .scoreCalculator(new DataSetLossCalculator(testSetIterator, true))
    .evaluateEveryNEpochs(1)
    .modelSaver(saver)
    .build();
</code></pre>

<p><strong>Train and Test Code</strong></p>

<pre><code>StatsStorage statsStorage = new InMemoryStatsStorage();
MultiLayerNetwork networkModel = new MultiLayerNetwork(conf);
networkModel.setListeners(new StatsListener(statsStorage), new ScoreIterationListener(10));


IEarlyStoppingTrainer trainer = new EarlyStoppingTrainer(esConf, networkModel, trainSetIterator);
EarlyStoppingResult&lt;MultiLayerNetwork&gt; result = trainer.fit();


// -------------------------- Evaluation trained model and print results --------------------------
System.out.println(""Termination reason: "" + result.getTerminationReason());
System.out.println(""Termination details: "" + result.getTerminationDetails());
System.out.println(""Total epochs: "" + result.getTotalEpochs());
System.out.println(""Best epoch number: "" + result.getBestModelEpoch());
System.out.println(""Score at best epoch: "" + result.getBestModelScore());

MultiLayerNetwork bestNetwork = result.getBestModel();
Evaluation eval1 = new Evaluation(OUTPUT_NEURONS);
testSetIterator.reset();

for (int i = 0; i &lt; testData.numExamples(); i++) {
    DataSet t = testData.get(i);
    INDArray features = t.getFeatureMatrix();
    INDArray labels = t.getLabels();
    INDArray output = bestNetwork.output(features, false);
    eval1.eval(labels, output);
}

M.messageln(eval1.stats());
</code></pre>

<p><strong>Results:</strong></p>

<pre><code>Termination reason: EpochTerminationCondition
Termination details: ScoreImprovementEpochTerminationCondition(maxEpochsWithNoImprovement=50, minImprovement=0.0)
Total epochs: 55
Best epoch number: 4
Score at best epoch: 0.6579822991097982

Examples labeled as 0 classified by model as 0: 397 times
Examples labeled as 0 classified by model as 1: 58 times
Examples labeled as 1 classified by model as 0: 190 times
Examples labeled as 1 classified by model as 1: 55 times


==========================Scores========================================
 # of classes:    2
 Accuracy:        0.6457
 Precision:       0.5815
 Recall:          0.5485
 F1 Score:        0.3073
========================================================================
Pattern1 :      Accuracy: 0.6457142857142857 | Precision: 0.5815229681446081 | Recall: 0.54850863422292 | F1: 0.3072625698324022
</code></pre>

<p>No matter how I tune the learning rate, input &amp; output activation methods, updater, regulation, etc, I still cannot get a satisfactory result.  Grateful if you can give me a hand how I can manipulate the DL4J better.  I am working on Arbiter but with no luck. No sure if I am using the 0.9.1 stable release or not.</p>

<p><strong>Thanks a billion!</strong></p>
","<java><deep-learning><deeplearning4j><dl4j>","2018-10-18 13:20:06","","2","10497764","2018-10-19 07:25:36","2","0","","","10497764","11","0","0"
"43497679","<p>I tried to integrate deeplearning4j with JHipster pom.xml with the following:</p>

<pre><code>&lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
        &lt;version&gt;0.8.0&lt;/version&gt;
       &lt;exclusions&gt;
           &lt;exclusion&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
                &lt;artifactId&gt;logback-core&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
                &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
                &lt;artifactId&gt;logback-access&lt;/artifactId&gt;
            &lt;/exclusion&gt;
    &lt;/exclusions&gt;
&lt;/dependency&gt;
</code></pre>

<p>and I'm getting the following error while I run .\mvnw :</p>

<pre><code>SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/C:/Users/william/.m2/repository      /org/slf4j/slf4j-nop/1.7.6/slf4j-nop-1.7.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/C:/Users/william/.m2/repository/ch/qos/logback/logback-classic/1.1.7/logback-classic-1.1.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.helpers.NOPLoggerFactory]
[WARNING]
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at     sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at   org.springframework.boot.maven.AbstractRunMojo$LaunchRunner.run(AbstractRunMojo.java:478)
    at java.lang.Thread.run(Thread.java:745)
 Caused by: java.lang.IllegalArgumentException: LoggerFactory is not a    Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.helpers.NOPLoggerFactory loaded from file:/C:/Users/william/.m2/repository/org/slf4j/slf4j-api/1.7.21/slf4j-api-1.7.21.jar). If you are using WebLogic you will need to add 'org.slf4j' to prefer-application-packages in WEB-INF/weblogic.xml Object of class [org.slf4j.helpers.NOPLoggerFactory] must be an instance of class  ch.qos.logback.classic.LoggerContext
    at org.springframework.util.Assert.isInstanceOf(Assert.java:346)
    at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLoggerContext(LogbackLoggingSystem.java:221)
    at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLogger(LogbackLoggingSystem.java:213)
    at org.springframework.boot.logging.logback.LogbackLoggingSystem.beforeInitialize(LogbackLoggingSystem.java:98)
    at org.springframework.boot.logging.LoggingApplicationListener.onApplicationStartedEvent(LoggingApplicationListener.java:215)
    at org.springframework.boot.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:197)
    at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:166)
    at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:138)
    at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:121)
    at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:111)
    at org.springframework.boot.context.event.EventPublishingRunListener.started(EventPublishingRunListener.java:60)
    at org.springframework.boot.SpringApplicationRunListeners.started(SpringApplicationRunListeners.java:48)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:303)
    at com.aitrader.AitraderApp.main(AitraderApp.java:64)
    ... 6 more
</code></pre>

<p>as you can see I have added exclusions on deeplearning4j already, but I still getting SLF4J binding conflicts, couldn't finger out what's the problem. </p>
","<maven><spring-boot><jhipster><deeplearning4j>","2017-04-19 13:49:06","","3","6695273","2017-04-20 09:00:57","1","3","1","","6695273","41","0","0"
"52589578","<p>I am using deeplearning4j with JBOSS wildfly. I have placed all the modules as mentioned below in the classpath:</p>

<pre><code>&lt;module name=""org.nd4j.jackson"" /&gt;
&lt;module name=""org.nd4j.nd4j-api"" /&gt;
&lt;module name=""org.nd4j.nd4j-context"" /&gt;
&lt;module name=""org.nd4j.nd4j-common"" /&gt;
&lt;module name=""org.nd4j.nd4j-cuda-92"" /&gt;
&lt;module name=""org.nd4j.nd4j-cuda-92-platform"" /&gt;
&lt;module name=""org.nd4j.nd4j-native"" /&gt;
&lt;module name=""org.nd4j.nd4j-native-api"" /&gt;
&lt;module name=""org.nd4j.nd4j-native-platform"" /&gt;
&lt;module name=""org.bytedeco.javacpp"" /&gt;
&lt;module name=""org.bytedeco.javacpp-presets.cuda"" /&gt;
&lt;module name=""org.bytedeco.javacpp-presets.cuda-platform"" /&gt;
&lt;module name=""org.bytedeco.javacpp-presets.mkl"" /&gt;
&lt;module name=""org.bytedeco.javacpp-presets.openblas"" /&gt;
&lt;module name=""org.bytedeco.javacv"" /&gt;
&lt;module name=""org.datavec.datavec-api"" /&gt;
&lt;module name=""org.deeplearning4j.deeplearning4j-core"" /&gt;
&lt;module name=""org.deeplearning4j.deeplearning4j-nlp"" /&gt;
&lt;module name=""org.deeplearning4j.deeplearning4j-nn"" /&gt;
</code></pre>

<p>I am still getting an exception java.lang.RuntimeException: </p>

<pre><code>org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: Please ensure that you have an nd4j backend on your classpath. Please see: http://nd4j.org/getstarted.html
at org.nd4j.linalg.factory.Nd4j.initContext(Nd4j.java:5449)
while executing the below line of code, basically where I read the model
org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readWord2Vec(WordVectorSerializer.java:787)
</code></pre>

<p>Any idea what could be going on here?</p>
","<deeplearning4j><dl4j><nd4j>","2018-10-01 10:53:48","","0","7585175","2018-10-01 13:18:27","0","3","","","7585175","30","0","0"
"41548883","<p>I am a beginner to Deeplearning4j, and going to to a testing on Cifar-10 images classify. I just copy the Alexnet from DL4j example(AnimalsClassification.java) like:</p>

<pre><code>    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
        .seed(seed)
        .weightInit(WeightInit.DISTRIBUTION)
        .dist(new NormalDistribution(0.0, 0.01))
        .activation(Activation.RELU)
        .updater(Updater.NESTEROVS)
        .iterations(iterations)
        .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer) // normalize to prevent vanishing or exploding gradients
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .learningRate(1e-2)
        .biasLearningRate(1e-2*2)
        .learningRateDecayPolicy(LearningRatePolicy.Step)
        .lrPolicyDecayRate(0.1)
        .lrPolicySteps(100000)
        .regularization(true)
        .l2(5 * 1e-4)
        .momentum(0.9)
        .miniBatch(false)
        .list()
        .layer(0, convInit(""cnn1"", channels, 96, new int[]{11, 11}, new int[]{4, 4}, new int[]{3, 3}, 0))
        .layer(1, new LocalResponseNormalization.Builder().name(""lrn1"").build())
        .layer(2, maxPool(""maxpool1"", new int[]{3,3}))
        .layer(3, conv5x5(""cnn2"", 256, new int[] {1,1}, new int[] {2,2}, nonZeroBias))
        .layer(4, new LocalResponseNormalization.Builder().name(""lrn2"").build())
        .layer(5, maxPool(""maxpool2"", new int[]{3,3}))
        .layer(6,conv3x3(""cnn3"", 384, 0))
        .layer(7,conv3x3(""cnn4"", 384, nonZeroBias))
        .layer(8,conv3x3(""cnn5"", 256, nonZeroBias))
        .layer(9, maxPool(""maxpool3"", new int[]{3,3}))
        .layer(10, fullyConnected(""ffn1"", 4096, nonZeroBias, dropOut, new GaussianDistribution(0, 0.005)))
        .layer(11, fullyConnected(""ffn2"", 4096, nonZeroBias, dropOut, new GaussianDistribution(0, 0.005)))
        .layer(12, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
            .name(""output"")
            .nOut(numLabels)
            .activation(Activation.SOFTMAX)
            .build())
        .backprop(true)
        .pretrain(false)
        .setInputType(InputType.convolutional(height, width, channels))
        .build();
</code></pre>

<p>When I run the code it threw an exception say there are some problems with ""layer-9"" configuration on new int[]{3,3}, it should be greater than 0 and less than pHeight + 2*padH. When change the weight*height from 32 * 32 to 100*100 in java code, it ran properly, but I and not should the result is good. So I am a little bit confused on the layer configuration on alexnet deal with 32*32 images. </p>
","<deep-learning><deeplearning4j><dl4j>","2017-01-09 13:12:29","","0","6714349","2017-01-10 02:20:14","1","0","","","6714349","60","5","0"
"45057634","<p>I'm playing a bit with <a href=""https://deeplearning4j.org/"" rel=""nofollow noreferrer"">DeepLearning4J</a> and I wonder how I can make a classifier return a score instead of a label. Suppose I use the code from the <a href=""https://deeplearning4j.org/tutorials#linear-classifier-tutorial"" rel=""nofollow noreferrer"">linear classifier tutorial</a>, I'd like to make the ANN return the probabilities for a given training example to be labeled 0 or 1. The current configuration looks as follows:</p>

<pre><code>MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
        .seed(123)
        .iterations(1)
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .learningRate(0.01)
        .updater(Updater.NESTEROVS)
        .momentum(0.9)
        .list()
        .layer(0, new DenseLayer.Builder()
                .nIn(2)
                .nOut(20)
                .weightInit(WeightInit.XAVIER)
                .activation(Activation.RELU)
                .build())
        .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD)
                .nIn(20)
                .nOut(2)
                .weightInit(WeightInit.XAVIER)
                .activation(Activation.SOFTMAX)
                .build())
        .pretrain(false)
        .backprop(true)
        .build();
</code></pre>
","<java><machine-learning><neural-network><deeplearning4j>","2017-07-12 12:24:03","","0","3429133","2017-07-12 13:52:31","1","0","","","3429133","1987","503","33"
"43650508","<p>I have a training dataset of 1,00,000+ documents categorised into around 100 categories. I am trying to predict category for a text using <a href=""https://deeplearning4j.org/"" rel=""nofollow noreferrer"">DeepLearning4java</a> library, code based on <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/paragraphvectors/ParagraphVectorsClassifierExample.java"" rel=""nofollow noreferrer"">ParagraphVectorsClassifierExample</a> example. Each document is a single short line of text.</p>

<p>I am splitting available data into training(80%) and test data(20%). With much tuning of parameters, I am getting maximum 20% correct predictions on the test data. I understand lot of things depend on input data itself. However, just wanted to check if the accuracy can be further improved. I see a comment in the example code that says <em>""This example could be improved by using learning cascade for higher accuracy""</em>. Any hint/help/advice to improve prediction accuracy would be highly appreciated.</p>
","<java><deep-learning><document-classification><deeplearning4j><dl4j>","2017-04-27 06:49:05","","0","225667","2017-04-27 09:02:31","0","2","","","225667","7430","1519","2"
"43825674","<p>I'm trying to train a RNN for digital (audio) signal processing using deeplearning4j.
The idea is to have 2 .wav files: one is an audio recording, the second is the same audio recording but processed (for example with a low-pass filter). 
The RNN's input is the 1st (unprocessed) audio recording, the output is the 2nd (processed) audio recording.</p>

<p>I've used the GravesLSTMCharModellingExample from the dl4j examples, and mostly adapted the CharacterIterator class to accept audio data instead of text.</p>

<p>My 1st project to work with audio at all with dl4j is to basically do the same thing as GravesLSTMCharModellingExample but generating audio instead of text, working with 11025Hz 8 bit mono audio, which works (to some quite amusing results). So the basics wrt working with audio in this context seem to work.</p>

<p>So step 2 was to adapt this for audio <em>processing</em> instead of audio <em>generation</em>.</p>

<p>Unfortunately, I'm not having much success.
The best it seems to be able to do is outputting a very noisy version of the input.</p>

<p>As a 'sanity check', I've tested using the same audio file for both the input and the output, which I expected to converge quickly to a model simply copying the input. But it doesn't. Again, after a long time of training, all it seemed to be able to do is produce a noisier version of the input.</p>

<p>The most relevant piece of code I guess is the DataSetIterator.next() method (adapted from the example's CharacterIterator class), which now look like this:</p>

<pre><code>public DataSet next(int num) {
    if (exampleStartOffsets.size() == 0)
        throw new NoSuchElementException();

    int currMinibatchSize = Math.min(num, exampleStartOffsets.size());
    // Allocate space:
    // Note the order here:
    // dimension 0 = number of examples in minibatch
    // dimension 1 = size of each vector (i.e., number of characters)
    // dimension 2 = length of each time series/example
    // Why 'f' order here? See http://deeplearning4j.org/usingrnns.html#data
    // section ""Alternative: Implementing a custom DataSetIterator""
    INDArray input = Nd4j.create(new int[] { currMinibatchSize, columns, exampleLength }, 'f');
    INDArray labels = Nd4j.create(new int[] { currMinibatchSize, columns, exampleLength }, 'f');

    for (int i = 0; i &lt; currMinibatchSize; i++) {
        int startIdx = exampleStartOffsets.removeFirst();
        int endIdx = startIdx + exampleLength;

        for (int j = startIdx, c = 0; j &lt; endIdx; j++, c++) {
            // inputIndices/idealIndices are audio samples converted to indices.
            // With 8-bit audio, this translates to values between 0-255.
            input.putScalar(new int[] { i, inputIndices[j], c }, 1.0);
            labels.putScalar(new int[] { i, idealIndices[j], c }, 1.0);
        }
    }

    return new DataSet(input, labels);
}
</code></pre>

<p>So maybe I'm having a fundamental misunderstanding of what LSTMs are supposed to do.
Is there anything obviously wrong in the posted code that I'm missing?
Is there an obvious reason why training on the same file doesn't necessarily converge quickly to a model that just copies the input? (let alone even trying to train it on signal processing that actually does something?)</p>

<p>I've seen <a href=""https://stackoverflow.com/questions/40843614/using-rnn-to-recover-sine-wave-from-noisy-signal"">Using RNN to recover sine wave from noisy signal</a> which seems to be about a similar problem (but using a different ML framework), but that didn't get an answer.</p>

<p>Any feedback is appreciated!</p>
","<java><machine-learning><audio-processing><deeplearning4j>","2017-05-06 21:44:41","","12","7973401","2017-05-19 18:05:17","1","3","5","","7973401","69","0","0"
"43633092","<p>I'm new to deeplearning4j, i want to make sentence classifier using words vector as input for the classifier. 
I was using python before, where the vector model was generated using gensim, and i want to use that model for this new classifier. 
Is it possible to use gensim's word2vec model in deeplearning4j.word2vec and how i can do that?</p>
","<java><gensim><word2vec><deeplearning4j>","2017-04-26 11:37:02","","4","5182227","2019-02-04 21:31:28","1","3","1","","5182227","35","10","0"
"42985597","<p>I want to train my word2vec model on 5 million sentences dataset and use that in my paragraph2vec. Does following suffice my requirement? I am using deep4j word2vec and paragraph2vec implementations</p>

<p>-Train word2vec by building and calling .fit() method</p>

<p>-build a paragraph2vec model by setting vocabCache() as one that I get by word2vec.getVocab() (of trained model of previous step)</p>

<p>Many thanks in advance,</p>
","<deep-learning><word2vec><deeplearning4j>","2017-03-23 19:43:25","","1","2424347","2017-03-23 20:05:58","0","1","1","","2424347","443","20","2"
"49351662","<p>I want interate a DataSetIterator and add it into a DataSet. Iterate is easy:</p>

<pre><code>while (iterator.hasNext()) {
    DataSet next = iterator.next();
    dataSet.addRow(next, dataSet.numExamples()); // isn't work
}
</code></pre>

<p>if the DataSetIterator batch size is 1, when I do <code>dataSet.addRow(next, 1);</code> this just replace this first element with the next one. If batch size is 2, then raise the exception: <code>Exception in thread ""main"" java.lang.IllegalArgumentException: NDArrayIndex is out of range. Beginning index: 2 must be less than its size: 2</code></p>

<p>I also want know how add a DataSet into another DataSet.</p>
","<java><deeplearning4j><dl4j>","2018-03-18 18:51:36","","0","6237060","2018-03-20 20:03:29","2","0","","","6237060","209","83","0"
"39469715","<p>I am trying to create a neural network using deeplearning4j. I have created a maven project. but when I run the project I get this java error. </p>

<pre><code>Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec (default-cli) on project demo-neural: Command execution failed. Cannot run program ""C:\Program Files\Java\jdk1.8.0_77\bin\java.exe"" (in directory ""C:\Users\dev1\Documents\New folder (3)\NeuralNetwork""): CreateProcess error=206, The filename or extension is too long -&gt; [Help 1]

To see the full stack trace of the errors, re-run Maven with the -e switch.
Re-run Maven using the -X switch to enable full debug logging.

For more information about the errors and possible solutions, please read the following articles:
[Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
</code></pre>

<p>My pom.xml looks like this</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;

    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.cdap&lt;/groupId&gt;
    &lt;artifactId&gt;demo-neural&lt;/artifactId&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;name&gt;A Camel Route&lt;/name&gt;
    &lt;url&gt;http://www.myorganization.org&lt;/url&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.camel&lt;/groupId&gt;
            &lt;artifactId&gt;camel-core&lt;/artifactId&gt;
            &lt;version&gt;2.17.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- logging --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
            &lt;version&gt;1.7.21&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;version&gt;1.7.21&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;version&gt;1.2.17&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
            &lt;version&gt;0.5.0&lt;/version&gt;
            &lt;type&gt;jar&lt;/type&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;dl4j-spark_2.11&lt;/artifactId&gt;
            &lt;version&gt;0.4-rc3.10&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.maven.plugins/maven-compiler-plugin --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.5.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;nd4j-native&lt;/artifactId&gt;
            &lt;version&gt;0.4-rc3.10&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;nd4j-api&lt;/artifactId&gt;
            &lt;version&gt;0.5.0&lt;/version&gt;
            &lt;type&gt;jar&lt;/type&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;canova-nd4j-common&lt;/artifactId&gt;
            &lt;version&gt;0.0.0.16&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;1.6.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt;
            &lt;version&gt;1.6.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
            &lt;version&gt;2.10.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;2.7.2&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;nz.ac.waikato.cms.weka&lt;/groupId&gt;
            &lt;artifactId&gt;weka-stable&lt;/artifactId&gt;
            &lt;version&gt;3.6.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
            &lt;version&gt;0.4.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;nd4j-cuda-7.5-platform&lt;/artifactId&gt;
            &lt;version&gt;0.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.datavec&lt;/groupId&gt;
            &lt;artifactId&gt;datavec-spark_2.10&lt;/artifactId&gt;
            &lt;version&gt;0.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!--&lt;dependency&gt;--&gt;
        &lt;!--&lt;groupId&gt;org.nd4j&lt;/groupId&gt;--&gt;
        &lt;!--&lt;artifactId&gt;nd4j-api&lt;/artifactId&gt;--&gt;
        &lt;!--&lt;version&gt;0.0.3&lt;/version&gt;--&gt;
        &lt;!--&lt;/dependency&gt;--&gt;

        &lt;!-- testing --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.camel&lt;/groupId&gt;
            &lt;artifactId&gt;camel-test&lt;/artifactId&gt;
            &lt;version&gt;2.17.3&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;
            &lt;version&gt;0.0.3.3&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.datavec&lt;/groupId&gt;
            &lt;artifactId&gt;datavec-api&lt;/artifactId&gt;
            &lt;version&gt;0.5.0&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;defaultGoal&gt;install&lt;/defaultGoal&gt;

        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.5.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.7&lt;/source&gt;
                    &lt;target&gt;1.7&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.6&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

            &lt;!-- Allows the example to be run via 'mvn compile exec:java' --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
                &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;1.2.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;mainClass&gt;com.cdap.MainApp&lt;/mainClass&gt;
                    &lt;includePluginDependencies&gt;false&lt;/includePluginDependencies&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;

        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>

<p>Can anyone help me to solve this? I have tried many options and nothing worked. Thanks in advance.</p>
","<java><maven><recurrent-neural-network><deeplearning4j>","2016-09-13 12:08:50","","0","4989899","2017-12-19 09:06:50","3","0","1","","4989899","82","7","0"
"48599775","<p>The code below comes from <a href=""https://deeplearning4j.org"" rel=""nofollow noreferrer"">https://deeplearning4j.org</a>.
I don't quite get the nIn and nOut params. Does the definition below create 2 layers, or 3 with one hidden layer of 1.000 neurons?
And what would happen if the nOut of layer 0 would not match nIn of layer 1? Does this always have to be the same number (in this case 1.000)?</p>

<pre><code>.layer(0, new DenseLayer.Builder()
            .nIn(numRows * numColumns) // Number of input datapoints.
            .nOut(1000) // Number of output datapoints.
            .activation(""relu"") // Activation function.
            .weightInit(WeightInit.XAVIER) // Weight initialization.
            .build())
    .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD)
            .nIn(1000)
            .nOut(outputNum)
            .activation(""softmax"")
            .weightInit(WeightInit.XAVIER)
            .build())
    .pretrain(false).backprop(true)
    .build();
</code></pre>
","<layer><deeplearning4j>","2018-02-03 16:59:37","","0","9309807","2018-02-04 05:32:36","1","0","","","9309807","1","0","0"
"40829209","<p>I try to follow the quick start guide on deeplearning4j, deeplearning4j.org/quickstart. However, when I try to run ""mvn clean install"", it give me the following errors:</p>

<pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.
5.1:compile (default-compile) on project dl4j-examples: Compilation failure: Com
pilation failure:
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/PredictGenderTest.java:[8,55] package
org.apache.lucene.queryparser.flexible.messages does not exist
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/PredictGenderTest.java:[9,37] package
org.canova.api.records.reader does not exist
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/PredictGenderTest.java:[10,42] package
 org.canova.api.records.reader.impl does not exist
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/PredictGenderTest.java:[11,28] package
 org.canova.api.split does not exist
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/PredictGenderTest.java:[12,42] package
 org.deeplearning4j.datasets.canova does not exist
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/GenderRecordReader.java:[19,34] packag
e jdk.internal.util.xml.impl does not exist
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/GenderRecordReader.java:[20,56] packag
e org.apache.lucene.queryparser.flexible.core.util does not exist
[ERROR] /D:/dl4j-examples/dl4j-examples/src/main/java/org/deeplearning4j/example
s/feedforward/classification/DetectGender/GenderRecordReader.java:[21,30] packag
e org.apache.lucene.util does not exist
[ERROR] -&gt; [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e swit
ch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please rea
d the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureExc
eption
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command

[ERROR]   mvn &lt;goals&gt; -rf :dl4j-examples
</code></pre>

<p>I also changed java version tag inside pom.xml from 1.7 to 1.8. I also use window 7 64 bit.</p>
","<maven><deeplearning4j>","2016-11-27 12:58:09","","0","1676537","2016-11-27 14:32:17","1","3","","2016-11-28 19:32:53","1676537","36","7","0"
"52810012","<p>I started using the 1.0.0-beta2 version of deeplearning4j and I am getting the following error when attempting to start:</p>

<p>java.lang.UnsatisfiedLinkError: no jnind4jcpu in java.library.path</p>

<p>In looking around, I came across this issue: <a href=""https://github.com/deeplearning4j/nd4j/issues/1687"" rel=""nofollow noreferrer"">https://github.com/deeplearning4j/nd4j/issues/1687</a></p>

<p>where the solution seemed to be an incomplete set of packages. I have confirmed that I have nd4j-native-platform-1.0.0-beta2.jar, but this jar file contains no libraries as what seemed to be the problem in the issue mentioned above.</p>

<p>I have looked at the maven repository and things are strange there as well:</p>

<p>For <a href=""https://mvnrepository.com/artifact/org.nd4j/nd4j-native-platform"" rel=""nofollow noreferrer"">https://mvnrepository.com/artifact/org.nd4j/nd4j-native-platform</a>, the list of files under ""View All"" seems incomplete for the 1.0.0-beta2 version and the existing jar files under older versons also don't seem to contain any binaries so I am unsure as to where the binaries are supposed to be.</p>

<p>Thanks,</p>

<p>Jason</p>
","<deeplearning4j><java.library.path>","2018-10-15 05:03:07","","0","2178363","2018-10-19 06:49:09","1","0","","","2178363","362","45","2"
"43495662","<p>I am trying to implement a channel for a CNN, this channel is to split a sentence into x number of parts. Each of these parts then gains a sentiment score, and the parts are fed into the CNN. I however don't understand how I can turn these part scores into an INDArray for the CNN.</p>

<p>My current code:</p>

<pre><code>public INDarray getFeatureVectors(List&lt;String&gt; sentences) {
    // nParts is the number of parts to split the sentence into
    // sentences are a list of sentences in the current batch

    // int[] featureShape = new int[]{sentences.size(), nParts}; 
    int[] featureShape = new int[4];
    featureShape[0] = sentences.size();
    featureShape[1] = 1;
    featureShape[2] = nParts;
    featureShape[3] = nParts;
    INDArray features = Nd4j.create(sentences.size());
    for (int i = 0; i &lt; sentences.size(); i++) {

        List&lt;String&gt; tokens = // tokenize sentence

        double[] partScores = // calculate the score for each part
                              // e.g. for nParts = 2, partScores = {-1.0, 1.0}

        INDArray vector = Nd4j.create(partScores, featureShape);
        INDArrayIndex[] indices = new INDArrayIndex[4];
        indices[0] = NDArrayIndex.point(i);
        indices[1] = NDArrayIndex.point(0);
        indices[2] = NDArrayIndex.all();
        indices[3] = NDArrayIndex.all();
        features.put(indices, vector);

    }
    return features;
}
</code></pre>

<p>I have just been experimenting with different feature shape and indices but I don't really have an idea what I'm doing, so any help would be greatly appreciated!</p>

<p>I am basing the code off Deeplearning4js <a href=""https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/iterator/CnnSentenceDataSetIterator.java"" rel=""nofollow noreferrer"">CnnSentenceDataSetIterator</a>, which turns sentences into word embeddings.</p>
","<java><multidimensional-array><deeplearning4j><nd4j>","2017-04-19 12:23:17","","1","1848671","2017-04-19 12:23:17","0","0","","","1848671","195","23","12"
"39249114","<p>I want to write a RNN in Deeplearning4j for stock market predictions but I'm struggling with creating und filling the 3-dimensional <code>INDArrays</code>.
So if I have the following time series:</p>

<blockquote>
  <p>1 2 3 4 5 6 7 8 9 10</p>
</blockquote>

<p>and I want to use 5 values as input and predict the 6th value:</p>

<blockquote>
  <p><strong>Input</strong>: 1 2 3 4 5 <strong>TeachingInput</strong>: 6</p>
  
  <p><strong>Input</strong>: 2 3 4 5 6 <strong>TeachingInput</strong>: 7</p>
</blockquote>

<p>...</p>

<p>I would fill the <code>INDArrays</code> like this:</p>

<pre><code>int numExamples = 5; //1-5, 2-6,..., 5-9
int inputSize = 5; 
int timeSeriesLength = 10;
INDArray features =  Nd4j.create(new int[]{numExamples,inputSize,timeSeriesLength}, 'f');

int outputSize = 1;
INDArray labels =  Nd4j.create(new int[]{numExamples,outputSize,timeSeriesLength}, 'f');
</code></pre>

<p>Is this correct? If so, how do the filled <code>INDArrays</code> look like for the <strong>features</strong>, <strong>labels</strong>, <strong>featuresMask</strong> and <strong>labelsMask</strong>?</p>

<p>Thank you.</p>
","<java><deep-learning><deeplearning4j>","2016-08-31 12:02:08","","2","6778507","2016-09-01 04:15:52","1","1","1","","6778507","13","0","0"
"39029247","<p>i want to run my deeplearning4j program that i created as IntelliJ project from server with command line input and i don't have any clue how to do it, any suggestion?</p>
","<intellij-idea><command-line-interface><deeplearning4j>","2016-08-19 00:00:46","","0","6732756","2016-08-19 17:00:56","1","1","","","6732756","3","0","0"
"51247265","<p>I tested anomaly detection using Deeplearning4j, everything works fine except that, I am not able to preserve the VehicleID while training. What is the best approach in such scenario?</p>

<p>Please look at the following snippet of code, SparkTransformExecutor returns a RDD and InMemorySequence is taking a list when, I am collecting list from RDD indexing is not guaranteed.</p>

<pre><code>  val records:JavaRDD[util.List[util.List[Writable]]] = SparkTransformExecutor
  .executeToSequence(.....)
   val split = records.randomSplit(Array[Double](0.7,0.3))
  val testSequences = split(1)

 //in memory  sequence reader
  val testRR = new InMemorySequenceRecordReader(testSequences.collect().toList)

   val testIter = new RecordReaderMultiDataSetIterator.Builder(batchSize)
           .addSequenceReader(""records"", trainRR)
           .addInput(""records"")
          .build()
</code></pre>
","<deeplearning4j>","2018-07-09 13:53:09","","-1","4288096","2018-08-06 08:29:07","1","1","0","","4288096","438","44","3"
"55796676","<p>I'm currently playing around with Word2Vec in deeplearning4j (using scala).
and I noticed that with my current configuration it is extremely slow when using the GPU backend. I checked after 8 hours at it was still running with only <code>4000 Words/sec</code> and most of the times <code>&lt; 100 Seq/Sec</code>. I killed the process and restarted it with the cpu backend which runs at around <code>150.000 Words/sec</code> and around <code>2000-3000 Seq/Sec</code> and reached the same state in less than 20 minutes.</p>

<p>I would be interested to know if this is a configuration problem or something else. </p>

<p>My dependencies look like this:</p>

<pre><code>//CPU dependency (excluded in GPU run but enabled in CPU)
//libraryDependencies += ""org.nd4j"" % ""nd4j-native-platform"" % ""1.0.0-beta3""

libraryDependencies += ""org.deeplearning4j"" % ""deeplearning4j-core"" % ""1.0.0-beta3""
libraryDependencies += ""org.deeplearning4j"" % ""deeplearning4j-ui_2.11"" % ""1.0.0-beta3""
libraryDependencies += ""org.deeplearning4j"" % ""deeplearning4j-cuda-10.0"" % ""1.0.0-beta3""
libraryDependencies += ""org.nd4j"" % ""nd4j-cuda-10.0-platform"" % ""1.0.0-beta3""
libraryDependencies += ""org.bytedeco.javacpp-presets"" % ""cuda"" % ""10.0-7.4-1.4.4"" classifier ""windows-x86_64-redist""
</code></pre>

<p>My Word2Vec configuration looks like this (for both CPU and GPU)</p>

<pre><code>val w2v = new Word2Vec.Builder()
.minWordFrequency(5)
.iterations(8)
.layerSize(100)
.seed(42)
.windowSize(5)
.iterate(iter)
.batchSize(128)
.tokenizerFactory(tokenizerFactory)
.useUnknown(true)
.build();
</code></pre>

<p>And here is the logging output</p>

<pre><code>[info] Running com.learning.Word2VecBuilder
08:38:36.464 [run-main-0] INFO  org.nd4j.linalg.factory.Nd4jBackend - Loaded [JCublasBackend] backend
08:38:39.715 [run-main-0] INFO  org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 32
08:38:40.929 [run-main-0] INFO  org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 0
08:38:40.935 [run-main-0] INFO  o.n.l.a.o.e.DefaultOpExecutioner - Backend used: [CUDA]; OS: [Windows 10]
08:38:40.935 [run-main-0] INFO  o.n.l.a.o.e.DefaultOpExecutioner - Cores: [4]; Memory: [5,3GB];
08:38:40.936 [run-main-0] INFO  o.n.l.a.o.e.DefaultOpExecutioner - Blas vendor: [CUBLAS]
08:38:40.937 [run-main-0] INFO  o.n.l.j.o.e.CudaExecutioner - Device Name: [GeForce GTX 1060 6GB]; CC: [6.1]; Total/free memory: [6442450944]
08:38:41.318 [run-main-0] DEBUG o.n.j.handler.impl.CudaZeroHandler - Creating bucketID: 5
Maximum number of Memory is: 5726797824 bytes
08:38:41.782 [run-main-0] INFO  o.d.m.s.SequenceVectors - Starting vocabulary building...
08:38:41.782 [run-main-0] DEBUG o.d.m.w.wordstore.VocabConstructor - Target vocab size before building: [0]
08:38:41.937 [run-main-0] DEBUG o.d.m.w.wordstore.VocabConstructor - Trying source iterator: [0]
08:38:41.937 [run-main-0] DEBUG o.d.m.w.wordstore.VocabConstructor - Target vocab size before building: [0]
08:39:02.679 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [100000]; Current vocabulary size: [166411]; Sequences/sec: 4785,38; Words/sec: 50201,03;
08:39:21.238 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [200000]; Current vocabulary size: [472153]; Sequences/sec: 5388,22; Words/sec: 310297,65;
08:39:40.490 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [300000]; Current vocabulary size: [600278]; Sequences/sec: 5194,27; Words/sec: 155047,27;
08:39:58.260 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [400000]; Current vocabulary size: [716479]; Sequences/sec: 5627,46; Words/sec: 199443,61;
08:40:15.360 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [500000]; Current vocabulary size: [869586]; Sequences/sec: 5847,95; Words/sec: 327225,85;
08:40:31.590 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [600000]; Current vocabulary size: [993343]; Sequences/sec: 6161,43; Words/sec: 280355,95;
08:40:50.260 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [700000]; Current vocabulary size: [1122310]; Sequences/sec: 5356,19; Words/sec: 269141,78;
08:41:05.564 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [800000]; Current vocabulary size: [1236145]; Sequences/sec: 6534,24; Words/sec: 338252,61;
08:41:18.590 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [900000]; Current vocabulary size: [1347346]; Sequences/sec: 7676,95; Words/sec: 424641,79;
08:41:31.288 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1000000]; Current vocabulary size: [1477759]; Sequences/sec: 7875,26; Words/sec: 524719,64;
08:41:46.564 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1100000]; Current vocabulary size: [1599449]; Sequences/sec: 6546,22; Words/sec: 451319,13;
08:41:58.174 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1200000]; Current vocabulary size: [1714450]; Sequences/sec: 8613,26; Words/sec: 597122,74;
08:42:14.504 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1300000]; Current vocabulary size: [1806469]; Sequences/sec: 6123,70; Words/sec: 275960,26;
08:42:30.468 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1400000]; Current vocabulary size: [1891943]; Sequences/sec: 6264,09; Words/sec: 277645,01;
08:42:44.215 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1500000]; Current vocabulary size: [1978590]; Sequences/sec: 7274,31; Words/sec: 399430,64;
08:43:00.936 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1600000]; Current vocabulary size: [2061284]; Sequences/sec: 5980,50; Words/sec: 290828,00;
08:43:13.851 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1700000]; Current vocabulary size: [2141019]; Sequences/sec: 7742,93; Words/sec: 356932,17;
08:43:32.731 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1800000]; Current vocabulary size: [2200118]; Sequences/sec: 5296,61; Words/sec: 177499,68;
08:43:55.174 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [1900000]; Current vocabulary size: [2206803]; Sequences/sec: 4455,73; Words/sec: 28245,24;
08:44:16.881 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [2000000]; Current vocabulary size: [2266169]; Sequences/sec: 4606,81; Words/sec: 166374,03;
08:44:33.816 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [2100000]; Current vocabulary size: [2340684]; Sequences/sec: 5904,93; Words/sec: 281809,33;
08:44:49.214 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [2200000]; Current vocabulary size: [2419663]; Sequences/sec: 6494,35; Words/sec: 336157,81;
08:44:58.814 [run-main-0] DEBUG o.d.m.w.wordstore.VocabConstructor - Waiting till all processes stop...
08:44:58.814 [run-main-0] DEBUG o.d.m.w.wordstore.VocabConstructor - Vocab size before truncation: [2470993],  NumWords: [104287573], sequences parsed: [2259529], counter: [104286594]
08:45:00.379 [run-main-0] DEBUG o.d.m.w.wordstore.VocabConstructor - Scavenger: Words before: 2470993; Words after: 448171;
08:45:00.380 [run-main-0] DEBUG o.d.m.w.wordstore.VocabConstructor - Vocab size after truncation: [448171],  NumWords: [101414224], sequences parsed: [2259529], counter: [104286594]
08:45:01.664 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Adding UNK element to vocab...
08:45:03.976 [run-main-0] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [2259529], Current vocabulary size: [448172]; Sequences/sec: [5912,01];
08:45:03.990 [run-main-0] INFO  o.d.m.e.loader.WordVectorSerializer - Projected memory use for model: [341,93 MB]
08:45:04.648 [run-main-0] DEBUG o.n.j.handler.impl.CudaZeroHandler - Creating bucketID: 3
08:45:04.662 [run-main-0] DEBUG o.n.j.handler.impl.CudaZeroHandler - Creating bucketID: 4
08:45:04.697 [run-main-0] DEBUG o.n.j.handler.impl.CudaZeroHandler - Creating bucketID: 0
08:45:04.702 [run-main-0] INFO  o.d.m.e.inmemory.InMemoryLookupTable - Initializing syn1...
08:45:05.877 [run-main-0] INFO  o.d.m.s.SequenceVectors - Building learning algorithms:
08:45:05.877 [run-main-0] INFO  o.d.m.s.SequenceVectors -           building ElementsLearningAlgorithm: [SkipGram]
08:45:05.882 [run-main-0] DEBUG o.n.j.handler.impl.CudaZeroHandler - Creating bucketID: 1
08:45:05.884 [run-main-0] DEBUG o.n.j.handler.impl.CudaZeroHandler - Creating bucketID: 2
08:45:05.900 [run-main-0] INFO  o.d.m.s.SequenceVectors - Starting learning process...
08:49:39.953 [VectorCalculationsThread 2] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [1216051];  Lines vectorized so far: [100000]; Seq/sec: [364,90]; Words/sec: [4437,40]; learningRate: [0.024962528224616736]
08:53:35.063 [VectorCalculationsThread 1] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [2278941];  Lines vectorized so far: [200000]; Seq/sec: [425,33]; Words/sec: [4475,91]; learningRate: [0.02492977581547878]
08:57:14.696 [VectorCalculationsThread 1] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3267747];  Lines vectorized so far: [300000]; Seq/sec: [455,30]; Words/sec: [4483,80]; learningRate: [0.024899306083990255]
...
15:20:17.762 [VectorCalculationsThread 0] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [101800897];  Lines vectorized so far: [3100000]; Seq/sec: [53,89]; Words/sec: [4293,25]; learningRate: [0.021863066154592867]
15:39:39.156 [VectorCalculationsThread 3] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [106697973];  Lines vectorized so far: [3200000]; Seq/sec: [86,10]; Words/sec: [4289,67]; learningRate: [0.02171216087985193]
15:59:06.222 [VectorCalculationsThread 3] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [111588408];  Lines vectorized so far: [3300000]; Seq/sec: [85,68]; Words/sec: [4285,22]; learningRate: [0.021561457378555856]
16:24:31.347 [VectorCalculationsThread 2] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [117830242];  Lines vectorized so far: [3400000]; Seq/sec: [65,57]; Words/sec: [4274,56]; learningRate: [0.02136912956863643]
</code></pre>
","<gpu><deeplearning4j><dl4j><nd4j>","2019-04-22 15:16:21","","0","5239249","2019-04-22 15:16:21","0","1","","","5239249","58","0","0"
"50533401","<p>I'm experimenting with simple time-series forecasting with deeplearning4j 1.0.0-beta using LSTM and RNN:
<a href=""https://gist.github.com/jumpingfella/ae884650f393266c34a6571bdb65bdfe"" rel=""nofollow noreferrer"">https://gist.github.com/jumpingfella/ae884650f393266c34a6571bdb65bdfe</a></p>

<p>The problem is: after input 12-13 prediction doesn't really change. I'm probably missing something obvious. </p>

<p>Output:<br/>
Result on training data: </p>

<pre>
[[[         0,    1.0000,    2.0000,    3.0000,    4.0000,    5.0000,    6.0000,    7.0000,    8.0000,    9.0000,   10.0000,   11.0000,   12.0000,   13.0000,   14.0000,   15.0000,   16.0000,   17.0000,   18.0000,   19.0000,   20.0000,   21.0000,   22.0000,   23.0000,   24.0000]]]
[[[    0.9449,    1.9531,    3.0839,    4.0111,    4.9409,    5.9952,    7.0329,    7.9967,    8.9157,   10.0582,   11.0989,   11.4641,   11.5335,   11.5449,   11.5469,   11.5473,   11.5474,   11.5474,   11.5475,   11.5475,   11.5476,   11.5476,   11.5476,   11.5476,   11.5476]]]
</pre>

<p>Result on test data:</p>

<pre> 
[[[   25.0000,   26.0000,   27.0000,   28.0000,   29.0000,   30.0000,   31.0000,   32.0000,   33.0000,   34.0000,   35.0000,   36.0000,   37.0000,   38.0000,   39.0000,   40.0000,   41.0000,   42.0000,   43.0000,   44.0000,   45.0000,   46.0000,   47.0000,   48.0000,   49.0000]]]
[[[   11.5476,   11.5476,   11.5476,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477,   11.5477]]]
</pre>
","<java><time-series><lstm><rnn><deeplearning4j>","2018-05-25 16:25:27","","0","3393131","2018-05-25 16:48:29","0","1","","","3393131","26","17","0"
"41172457","<p>I'm using the Doc2Vec algorithm with Deeplearning4j and it works fine when I run it on my Windows 10 PC, however when I try to run it on a Linux box, i get the following error:</p>

<pre><code>java.lang.NoClassDefFoundError: Could not initialize class org.nd4j.linalg.factory.Nd4j
at org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable$Builder.&lt;init&gt;(InMemoryLookupTable.java:581) ~[run.jar:?]
at org.deeplearning4j.models.sequencevectors.SequenceVectors$Builder.presetTables(SequenceVectors.java:801) ~[run.jar:?]
at org.deeplearning4j.models.paragraphvectors.ParagraphVectors$Builder.build(ParagraphVectors.java:663) ~[run.jar:?]
</code></pre>

<p>I've tried this on a couple of Linux machines, both of which were running Xubuntu and had sudo permissions</p>

<p>Here is the code for creating my ParagraphVectors:
    InputStream is = new ByteArrayInputStream(baos.toByteArray());</p>

<pre><code>  LabelAwareSentenceIterator iter;
  iter = new LabelAwareListSentenceIterator(is, DELIM);
  iter.setPreProcessor(new SentencePreProcessor() {
    @Override
    public String preProcess(String sentence) {
      return new InputHomogenization(sentence).transform();
    }
  });

  TokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();
  vec = new ParagraphVectors.Builder().minWordFrequency(minWordFrequency).batchSize(batchSize)
      .iterations(iterations).layerSize(layerSize).stopWords(stopWords).windowSize(windowSize)
      .learningRate(learningRate).tokenizerFactory(tokenizerFactory).iterate(iter).build();
  vec.fit();
</code></pre>

<p>And here is my pom.xml (versions are all 0.7.1, but I had been using 0.4-rc3.9 and got the same error) :</p>

<pre><code>&lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-ui-model&lt;/artifactId&gt;
        &lt;version&gt;${dl4j.version}&lt;/version&gt;
        &lt;exclusions&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;log4j&lt;/groupId&gt;
                &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;/exclusion&gt;
        &lt;/exclusions&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
        &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;
        &lt;version&gt;${dl4j.version}&lt;/version&gt;
        &lt;exclusions&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;log4j&lt;/groupId&gt;
                &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;/exclusion&gt;
        &lt;/exclusions&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
        &lt;artifactId&gt;nd4j-native&lt;/artifactId&gt;
        &lt;version&gt;${nd4j.version}&lt;/version&gt;
        &lt;exclusions&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;/exclusion&gt;
            &lt;exclusion&gt;
                &lt;groupId&gt;log4j&lt;/groupId&gt;
                &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;/exclusion&gt;
        &lt;/exclusions&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.datavec/datavec-api --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.datavec&lt;/groupId&gt;
        &lt;artifactId&gt;datavec-api&lt;/artifactId&gt;
        &lt;version&gt;${nd4j.version}&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>
","<linux><ubuntu><deep-learning><deeplearning4j><nd4j>","2016-12-15 20:03:28","","1","7303799","2016-12-15 20:47:25","1","0","","","7303799","8","0","0"
"46769705","<p>Is there any way to load doc2vec model saved using gensim into deeplearning4j's ParagraphVectors?</p>

<p>My gensim model is valid - I am able to load it using gensim with no problems.</p>

<p>When I call WordVectorSerializer.readParagraphVectors on my model from Java it throws exception:</p>

<pre><code>Exception in thread ""main"" java.util.zip.ZipException: error in opening zip file
    at java.util.zip.ZipFile.open(Native Method)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:163)
    at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readWord2Vec(WordVectorSerializer.java:889)
    at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readParagraphVectors(WordVectorSerializer.java:825)
    at pl.org.opi.main.main(main.java:17)
</code></pre>

<p>Upon debugging the code, I noticed that deeplearning4j expects a zip file with multiple txt files and a single json file inside of it. Is there a way to convert the gensim model to the zip expected by deeplearning4j or is there a dedicated method for this in dl4j API (couldn't find any using examples and javadoc)?</p>
","<java><python><gensim><deeplearning4j><doc2vec>","2017-10-16 11:53:39","","1","7271874","2017-10-16 11:53:39","0","1","","","7271874","43","2","0"
"49070509","<p>I'm attempting to run a neural network built using dl4j using the GPU. The code works fine when using the native platform dependency, but when I switch to using CUDA, execution fails with an exception:</p>

<pre><code>Caused by: java.lang.UnsatisfiedLinkError: ...\cuda-8.0-6.0-1.3-windows-x86_64.jar\org\bytedeco\javacpp\windows-x86_64\jnicuda.dll: Can't find dependent libraries
</code></pre>

<p>Here is what I have in my build.gradle:</p>

<pre><code>//    compile group: 'org.nd4j', name: 'nd4j-native-platform', version: '0.9.1'
compile group: 'org.nd4j', name: 'nd4j-cuda-8.0-platform', version: '0.9.1'
</code></pre>

<p>Here are the dependencies showin in IDEA:</p>

<p><a href=""https://i.stack.imgur.com/2Hp0f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2Hp0f.png"" alt=""Dependencies""></a></p>

<p>I was able to install the CUDA Toolkit and build the examples just fine. Running deviceQuery shows that my CUDA Driver / Runtime version is 9.1, is this a problem since the nd4j-cuda-8.0-platform references 8.0? Not sure, since the version is 0.9.1, which matches my CUDA version and there does not seem to be a 9.0 platform available.</p>

<p>Is there something additional that I'm missing? If so, how do I troubleshoot/resolve this?</p>
","<cuda><deeplearning4j><dl4j>","2018-03-02 13:52:31","","1","1289242","2018-03-02 16:38:25","1","4","1","","1289242","93","1","0"
"49582972","<p>I am trying to build paragraph vectors and perform some inferences on them with the DeepLearning4J framework in Java. When I build my paragraph vectors into a ZIP folder, I am able to get similarities by using line numbers like so:</p>

<pre><code>SentenceIterator sentenceIterator = new BasicLineIterator(new File(inputFilePath));
AbstractCache&lt;VocabWord&gt; abstractCache = new AbstractCache&lt;VocabWord&gt;();
TokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();
tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());

LabelsSource labelsSource = new LabelsSource(""LINE_"");

ParagraphVectors paragraphVectors = new ParagraphVectors.Builder()
        .minWordFrequency(1)
        .iterations(5)
        .epochs(1)
        .layerSize(100)
        .learningRate(0.025)
        .labelsSource(labelsSource)
        .windowSize(5)
        .iterate(sentenceIterator)
        .trainWordVectors(false)
        .vocabCache(abstractCache)
        .tokenizerFactory(tokenizerFactory)
        .sampling(0)
         .build();
paragraphVectors.fit();

double similarity1 = paragraphVectors.similarity(""LINE_9835"", ""LINE_100"");
System.out.println(""Similarity: "" + similarity1);

WordVectorSerializer.writeParagraphVectors(paragraphVectors, outputParagraphVectorsFilePath);
</code></pre>

<p>The variable <code>inputFilePath</code> refers to the text document that contains some information. The variable <code>outputParagraphVectorsFilePath</code> refers to the location on the disk where the vectors are to be stored. This function works and the similarities are accurate. The problem occurs below:</p>

<pre><code>TokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();
tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());

ParagraphVectors paragraphVectors = WordVectorSerializer.readParagraphVectors(new File(inputFilePath));
paragraphVectors.setTokenizerFactory(tokenizerFactory);
paragraphVectors.getConfiguration().setIterations(1);

INDArray inferredVectorA = paragraphVectors.inferVector(""This is my world ."");
INDArray inferredVectorA2 = paragraphVectors.inferVector(""This is my world ."");
INDArray inferredVectorB = paragraphVectors.inferVector(""This is my way ."");


System.out.println(""Cosine similarity A/B:"" + Transforms.cosineSim(inferredVectorA, inferredVectorB));
System.out.println(""Cosine similarity A/B2:"" + Transforms.cosineSim(inferredVectorA, inferredVectorA2));
</code></pre>

<p>The <code>inputFilePath</code> variable refers to the location on disk where the ZIP folder is that contains the vectors. When I run this function, I get the following:</p>

<blockquote>
  <p><code>Cosine similarity A/B:1.0</code><br><code>Cosine similarity A/B2:1.0</code></p>
</blockquote>

<p>Even if I change the vectors around and compare them to other vectors, I get the same 1.0. Am I doing something wrong? Any help would be greatly appreciated.</p>
","<java><nlp><deeplearning4j>","2018-03-31 00:11:56","","1","5662596","2018-04-02 06:34:33","1","0","","","5662596","1233","1286","17"
"51278138","<p>I have a model to train on a large data set that does not fit into RAM. So, basically my plan is to slice the data set creating a <code>DataSet</code> instance with input vectors and associated labels for every chunk. E.g. if I have 1M input vectors/labels I'd split them into 10 chunks each having 100K records.<br>
Then I'd put a chunk into 2 <code>INDArray</code> objects (for inputs and labels), create a <code>DataSet</code> and call <code>model.fit()</code> with that data set, repeating this procedure for every chunk and repeating the whole process until say the model's score reaches some value.
My questions are:<br>
1. Do I understand the process correctly?<br>
2. Can the <code>INDArray</code> instances be reused? Would it be right to allocate them once and then just fill them up with data set chunks over and over again?</p>
","<deeplearning4j><dl4j>","2018-07-11 05:55:48","","0","1221374","2018-07-11 08:07:06","1","0","","","1221374","116","8","0"
"52403395","<p>I trained a model using deeplearning4j on a first part of my data and saved it.</p>

<pre><code>ModelSerializer.writeModel(model, locationToSave, true);
</code></pre>

<p>Now, I want to train this saved model on the next part of data.</p>

<p>I loaded the model:</p>

<pre><code>MultiLayerNetwork model = ModelSerializer.restoreMultiLayerNetwork(""location"");
</code></pre>

<p>and then I used to train it on the new data</p>

<pre><code>model.fit(trainingDataIt);
</code></pre>

<p>but the model seems not fitting because I don't see the stat of each iteration</p>

<pre><code>11:56:56.161 [ADSI prefetch thread] DEBUG o.n.l.memory.abstracts.Nd4jWorkspace - Steps: 4
11:59:30.072 [main] DEBUG o.d.d.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
11:59:30.123 [ADSI prefetch thread] DEBUG o.n.l.memory.abstracts.Nd4jWorkspace - Steps: 4
12:01:39.760 [main] DEBUG o.d.d.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
12:01:39.793 [ADSI prefetch thread] DEBUG o.n.l.memory.abstracts.Nd4jWorkspace - Steps: 4
12:03:46.496 [main] DEBUG o.d.d.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
12:03:46.551 [ADSI prefetch thread] DEBUG o.n.l.memory.abstracts.Nd4jWorkspace - Steps: 4
</code></pre>

<p>is it normal, or there is something wrong with my approach ?</p>
","<java><deep-learning><deeplearning4j>","2018-09-19 10:09:44","","-1","10155402","2019-01-04 17:26:29","1","1","","","10155402","1","0","0"
"50639683","<p>Unexpected exception when load own dataset. The execution of the code from <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/transferlearning/vgg16/dataHelpers/FeaturizedPreSave.java"" rel=""nofollow noreferrer"">FeaturizedPreSave</a> with just a minor change in the number of samples to be taken into consideration for the neural network produce an exception of Unsupported Operation.</p>

<pre><code>numClasses = 96 
</code></pre>

<p>instead of the initial 5. And own <a href=""https://vk.com/doc108308742_467339969"" rel=""nofollow noreferrer"">DataSet</a>.</p>

<p>Error: </p>

<blockquote>
  <p>Reader output: 96 output classes, but array.size (1) is 58
  (must be equal to 1 or numClasses = 96).</p>
</blockquote>

<p><strong>Version Information:</strong></p>

<ul>
<li>Deeplearning4j 1.0.0-alpha</li>
<li>CUDA 8</li>
<li>Java 8</li>
</ul>

<p>Trace:</p>

<ul>
<li><p><a href=""https://github.com/Piastres/example/blob/master/error_trace"" rel=""nofollow noreferrer"">error_trace</a></p></li>
<li><p><a href=""https://github.com/Piastres/example/blob/master/log_trace"" rel=""nofollow noreferrer"">log_trace</a></p></li>
</ul>
","<java><dataset><deeplearning4j><dl4j>","2018-06-01 08:58:23","","0","9879859","2018-06-03 03:56:14","2","4","","","9879859","1","0","0"
"39104184","<p>I want to run the LSTM code from <a href=""https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/character"" rel=""noreferrer"">deeplearning4j examples</a> in my own sbt project using scala. My setup is Ubuntu 14.04, sbt 0.13, Oracle Java 8, nd4j version 0.5.0, scala 2.11.8. My approach can be found in my git <a href=""https://github.com/arzt/deeplearning-example"" rel=""noreferrer"">repo</a>. Feel free to clone it.
On runtime I get the following <a href=""https://github.com/arzt/deeplearning-example/blob/master/stack-trace.txt"" rel=""noreferrer"">warnings and errors</a>. How can I fix this?</p>
","<scala><sbt><deeplearning4j><nd4j>","2016-08-23 14:38:18","","5","885808","2016-10-29 05:15:56","4","4","1","","885808","171","2","0"
"38620687","<p>When using spark-submit, I need to send the dependencies with --packages. Which package should I use?</p>

<p>I tried making an uber jar which contain the dependencies, but I get the following error:</p>

<pre><code>java.lang.IllegalArgumentException: Please specify an existing file
</code></pre>

<p>The error is the result of the following code:</p>

<pre><code>String path = ""hdfs:///user/data.txt"";
SentenceIterator iter = new LineSentenceIterator(new File(path));
</code></pre>
","<java><apache-spark><word2vec><deeplearning4j><dl4j>","2016-07-27 18:33:15","","0","1161187","2016-07-28 01:27:10","1","0","","","1161187","122","19","0"
"40789213","<p>While debugging regression <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/feedforward/regression/RegressionSum.java"" rel=""nofollow noreferrer"">sample</a> of deeplearning4j I've noticed that it doesn't have normalization of data inputs and outputs. So first of all question, why it doesn't have normalization? And second question, is there somewhere in network architecture normalization mechanism?</p>

<p>As prof of non-normalized input is the following screenshot which was taken right before execution of line</p>

<pre><code>return new ListDataSetIterator(listDs,batchSize);
</code></pre>

<p><a href=""https://i.stack.imgur.com/0oOS5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0oOS5.png"" alt=""enter image description here""></a></p>
","<java><neural-network><deeplearning4j>","2016-11-24 14:43:46","","1","677824","2016-11-25 05:50:53","1","2","","","677824","3462","1186","15"
"40745247","<p>I'm trying to load Google news corpus using this code:</p>

<pre><code>           File gModel = new File(""/word2vec/GoogleNews-vectors-negative300.bin.gz"");
Word2Vec vec = WordVectorSerializer.loadGoogleModel(gModel, true);
</code></pre>

<p>but it causes this error:</p>

<pre><code>    Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
</code></pre>

<p>I tried to change VM options parameters like it explained in official documenatation of deeplearning4j with:</p>

<pre><code>         -Xms1024m -Xmx10g -XX:MaxPermSize=2g 
</code></pre>
","<java><netbeans><out-of-memory><deeplearning4j>","2016-11-22 15:04:20","","1","7115108","2016-11-22 15:04:20","0","6","1","","7115108","6","0","0"
"50022300","<p>I am trying to generate uber-jar using <code>sbt</code> compile and <code>sbt</code> package commands for running my application on our remote server with spark installed as standalone mode there. I used deeplearning4j framework for building LSTM neural network and tend to perform training model through spark. Nevertheless, I got into an issue when running the spark-submit command:</p>

<pre><code>spark-submit --class ""lstm.SparkLSTM"" --master local[*] 
stock_prediction_scala_2.11-0.1.jar --packages 
org.deeplearning4j:deeplearning4j-core:0.9.1 ""/home/hadoop/ScalaWorkspace/Stock_Prediction_Scala/target/lstm_train/prices-split-adjusted.csv"" ""WLTW""
</code></pre>

<p>The problem is that seemly spark-submit did not take effect in my circumstance. It has been terminated right after entering spark-submit without throwing any errors. I have not seen the progress of training in the output.</p>

<pre><code>[hadoop@abc lstm_train]$ spark-submit --class ""lstm.SparkLSTM"" --master local[*] stock_prediction_scala_2.11-0.1.jar --packages org.deeplearning4j:deeplearning4j-core:0.9.1 ""/home/hadoop/ScalaWorkspace/Stock_Prediction_Scala/target/lstm_train/prices-split-adjusted.csv"" ""WLTW""
2018-04-25 17:06:50 WARN  Utils:66 - Your hostname, gaion34 resolves to a loopback address: 127.0.0.1; using 192.168.0.173 instead (on interface eno1)
2018-04-25 17:06:50 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2018-04-25 17:06:51 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-04-25 17:06:51 INFO  ShutdownHookManager:54 - Shutdown hook called
2018-04-25 17:06:51 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-c4aee15e-d23b-4c03-95a7-12d9d39f714a
</code></pre>

<p>my main class:
<a href=""https://gist.github.com/rickyhai11/627d0da8bc93615785382b249618f43b"" rel=""nofollow noreferrer"">https://gist.github.com/rickyhai11/627d0da8bc93615785382b249618f43b</a></p>

<p>How to see generated logs by spark-submit command? I have tried to use --verbose, however, it did not help. </p>

<p>Any one has experienced this issue before, please advise me . thanks</p>
","<scala><apache-spark><lstm><spark-submit><deeplearning4j>","2018-04-25 12:24:34","","0","4969113","2018-04-25 20:45:44","1","1","","","4969113","1","0","0"
"48244635","<p>I'm trying to build the latest DeelLearning4j 0.9.2 using the build-dl4j-stack executable on CentOS workstation. ND4j and Dataved both build with no problem. The DeepLearning4j throws an error at deeplearning4j-cuda module.</p>

<p>[ERROR] Failed to execute goal on project deeplearning4j-cuda-9.0: Could not resolve dependencies for project org.deeplearning4j:deeplearning4j-cuda-9.0:jar:0.9.2-SNAPSHOT: Could not find artifact org.nd4j:nd4j-cuda-9.0:jar:linux-x86_64:0.9.2-SNAPSHOT in sonatype-nexus-snapshots (<a href=""https://oss.sonatype.org/content/repositories/snapshots"" rel=""nofollow noreferrer"">https://oss.sonatype.org/content/repositories/snapshots</a>) -> [Help 1]</p>

<p>Does anyone knows how to resolve this?</p>

<p>I have CentOS 7 kernel 3.10, nvidia driver 384.81, and Cuda 8.0 installed, and I was able to build DL4j 0.9.2 about 2 months ago with no problem on that system. 
Is the nd4j-cuda-9.0 jar the nd4j module adapted for cuda 9.0 that I don't have, and I need to upgrade my CUDA to 9.0?</p>

<p>thank a bunch</p>
","<maven><nexus><deeplearning4j>","2018-01-13 21:53:21","","0","4747688","2018-01-14 01:47:38","1","0","","","4747688","1","0","0"
"49391057","<p>During last few days I have started working with deeplearning4j library and I have encountered a kind of a problem. </p>

<p>My testing and input data consist of 25 binary values. Training set contains 40 rows. Network has 4 output values. My goal is to train the network to have as little error as possible. </p>

<p>I have tried different configurations (also the ones that were presented in deeplearning4j examples) but still I can not configure my network to have accuracy satisfactory level. What is more classification is really odd - for instance output values of network are like [0.31, 0.12, 0.24, 0.33].</p>

<p>To my mind proper values should be like [0, 0, 0, 1] etc. </p>

<p>My Neural network configuration:</p>

<pre><code>private static final int SEED = 123;
private static final int ITERATIONS = 1;
private static final int NUMBER_OF_INPUT_NODES = 25; 
private static final int NUMBER_OF_OUTPUT_NODES = 4; 
private static final int EPOCHS = 10;

public static MultiLayerNetwork getNeuralNetwork() {
    StatsStorage storage = configureUI();
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().seed(SEED).iterations(ITERATIONS).learningRate(1e-1)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .weightInit(WeightInit.RELU).updater(Updater.ADADELTA).list()
            .layer(0, new DenseLayer.Builder().nIn(NUMBER_OF_INPUT_NODES).nOut(60)
                    .activation(Activation.RELU).build())
            .layer(1, new DenseLayer.Builder().nIn(60).nOut(50)
                    .activation(Activation.RELU).build())
            .layer(2, new DenseLayer.Builder().nIn(50).nOut(50)
                    .activation(Activation.RELU).build())
            .layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(50).nOut(NUMBER_OF_OUTPUT_NODES)
                    .activation(Activation.SOFTMAX).build()).backprop(true).build();

    MultiLayerNetwork network = new MultiLayerNetwork(conf);
    network.init();
    network.setListeners(new StatsListener(storage), new ScoreIterationListener(1));
    DataSetIterator iterator = new ListDataSetIterator(createTrainingSet());
    for (int i = 0; i &lt; EPOCHS; i++) {
        network.fit(iterator);
    }
    return network;
}
</code></pre>

<p>I will be really grateful for any help.
Regards,</p>
","<java><neural-network><deeplearning4j>","2018-03-20 17:51:35","","1","6871584","2018-06-29 06:56:29","1","1","","","6871584","21","3","0"
"51367636","<p>This question comes up a lot on the support channel. People think downloading JAR files would simplify things. </p>
","<deeplearning4j><nd4j>","2018-07-16 17:58:32","","0","2736155","2018-07-19 05:49:06","2","0","","","2736155","391","115","1"
"51344016","<p>I'm planning on using this <a href=""http://csc.lsu.edu/~saikat/n-mnist/"" rel=""nofollow noreferrer"">modified version of MNIST</a> for benchmarking research, but they are currently in .mat format. So, I've read on StackOverflow that <code>MatlabRecordReader</code> actually isn't that robust, and that it's far smarter to change the data into CSV format.
I've downloaded Matlab and changed the .mat file to a .csv file that has 60000 (for the test data) lines, the first 784 values of each line being the pixel values of the image itself and the last 10 values being the label (though I believe I can easily condense the label into one value at the end of the first 784 values).</p>

<p>Now that I have this data, I'm not exactly sure how I should pass it through an <code>Iterator</code> properly for my Conv Neural Network. <a href=""https://deeplearning4j.org/csv-deep-learning"" rel=""nofollow noreferrer"">I've looked up the documentation</a>, but this isn't exactly what I need, and looking up the examples in the the <a href=""https://deeplearning4j.org/doc/org/deeplearning4j/datasets/datavec/RecordReaderDataSetIterator.html"" rel=""nofollow noreferrer"">docs</a> for the <code>RecordReaderDatasetIterator</code> was also a near-miss because it treats lines of the CSV files as either a 1 dimensional vector (as apposed to a matrix) or formats the data for linear regression. </p>

<p>I hope this has been clear enough. Could someone please assist me?</p>

<hr>
","<csv><machine-learning><conv-neural-network><deeplearning4j><dl4j>","2018-07-14 22:58:14","","0","5731044","2018-07-15 09:57:58","1","0","","","5731044","117","13","0"
"44857570","<pre><code>    File gModel = new File(""D:/FluidAI/WordVec/pathToSaveModel.txt"");
    Word2Vec vecs = WordVectorSerializer.readWord2VecModel(gModel);//(""D:/FluidAI/WordVec/pathToSaveModel.txt"");
    vecs.setTokenizerFactory(t);
    vecs.setSentenceIter(iter);
    //vecs.set
    vecs.fit();

    WordVectorSerializer.writeWordVectors(vecs, ""pathToWriteto.txt""); 
     WordVectorSerializer.writeWord2VecModel(vecs, ""pathToSaveModel.txt"");
     Collection&lt;String&gt; st=vecs.wordsNearest(""enclave"", 10);
     System.out.println(st);


    double cosSim = vecs.similarity(""extn"", ""dausa"");
    System.out.println(cosSim);
</code></pre>

<p><strong>in the vecs.similarity throws null pointer exception after traning</strong></p>
","<java><deeplearning4j><dl4j>","2017-07-01 05:11:33","","0","7875196","2017-07-01 05:11:33","0","3","","","7875196","15","52","0"
"55867334","<p>I have tested and trained the neural network for regression. When I run the network with new data I just get the same nuber for each entry. I adapted this from a classification system and that worked.</p>

<p>the code is:</p>

<pre><code>private void writeINDArray(INDArray output, PrintWriter writer, Iterator&lt;String&gt; identifierIterator) {
    int rows = output.rows();
    int coluumns = output.columns();

    for (int i = 0; i &lt; rows; i++) {
        INDArray row = output.getRow(i);
        StringJoiner stringJoiner = new StringJoiner(""\t"");

        for (int j = 0; j &lt; coluumns; j++) {
            stringJoiner.add(Float.toString(row.getFloat(j)));
        }

        if (identifierIterator.hasNext()) {
            stringJoiner.add(identifierIterator.next());
        }
        else {
            throw new RuntimeException(""identifier list is empty!"");
        }
        writer.println(stringJoiner.toString());
        log.info(stringJoiner);
    }
}

@Override
public void run(File neuralNetworkZipFile, File fingerPrintFile, List&lt;String&gt; identifiers) {
    log.info(String.format(""running %s on %s"", neuralNetworkZipFile.getAbsolutePath(), fingerPrintFile.getAbsolutePath()));

    Iterator&lt;String&gt; identifierIterator = identifiers.iterator();

    runResultFile = new File(""run_results_"" + Utility.timeDate() + "".txt"");

    try (RecordReader recordReader = new CSVRecordReader(0, ','); PrintWriter writer = new PrintWriter(runResultFile)) {
        recordReader.initialize(new FileSplit(fingerPrintFile));

        DataSetIterator iterator = neuralNetworkSupporter.getDataSetIterator(recordReader);
        MultiLayerNetwork model = ModelSerializer.restoreMultiLayerNetwork(neuralNetworkZipFile);

        while (iterator.hasNext()) {
            DataSet fingerPrint = iterator.next();
            INDArray output = model.output(fingerPrint.getFeatures(), false);

            writeINDArray(output, writer, identifierIterator);
        }
    }
    catch (IOException | InterruptedException e) {
        e.printStackTrace();
    }
}
</code></pre>

<p>Any suggestions what I am doing wrong? I have read the JavaDoc for the MultiLayerNetwork and the INDArray but nothing appears to cause this issue. I did have some issue loading the data in without data and had to do an ugly hack. To get it working. </p>

<pre><code>private void outputBitSet(MolecularProperties molecularProperties, PrintWriter writer) {
    StringBuilder builder = new StringBuilder();
    BitSet fingerprintBitSet = molecularProperties.bitSet;

    if (useStructuralFingerprint) {
        for (int i = 0; i &lt; fingerprintBitSet.size(); i++) {
            double bit = fingerprintBitSet.get(i) ? VALUE2 : VALUE1;

            appendComma(builder);
            builder.append(bit);
        }
    }
    if (useMolecularProperties) {
        addProperties(builder, molecularProperties);
    }

    if (! action.equals(Action.RUN)) {
        if (isRegression) {
            log.debug(String.format(""%8.6f"", molecularProperties.regressionValue) + "" "" + molecularProperties.id);

            appendComma(builder);
            builder.append(String.format(""%8.6f"", molecularProperties.regressionValue));
        }
        else {
            appendComma(builder);
            builder.append(molecularProperties.classification);
        }
    }
    else { // TODO This is need to fix an issue with CSVRecordReader expecting there to be one or more regression values. 
        if (isRegression) {
            appendComma(builder);
            builder.append(String.format(""%8.6f"", VALUE1));
        }
        else {
            appendComma(builder);
            builder.append(CLASS1);
        }
    }
    writer.println(builder.toString());
}

private void outputBitSet(List&lt;MolecularProperties&gt; molecularPropertiesList, PrintWriter writer) {
    if (!action.equals(Action.RUN)) {
        Collections.shuffle(molecularPropertiesList);
    }
    molecularPropertiesList.forEach(m -&gt; outputBitSet(m, writer));
}
</code></pre>

<p>I would really appreciate any suggestions?</p>

<p>Bart</p>
","<deeplearning4j>","2019-04-26 12:09:15","","0","11415403","2019-04-26 12:09:15","0","0","1","","11415403","1","0","0"
"49451160","<p>I successfully followed deeplearning4j.org tutorial on Word2Vec, so I am able to load already trained model or train a new one based on some raw text (more specifically, I am using <code>GoogleNews-vectors-negative300</code> and <code>Emoji2Vec</code> pre-trained model). </p>

<p>However, I would like to combine these two above models for the following reason: Having a sentence (for example, a comment from Instagram or Twitter, which consists of emoji), I want to identify the emoji in the sentence and then map it to the word it is related to. In order to do that, I was planning to iterate over all the words in the sentence and calculate the closeness (how near the emoji and the word are located in the vector space).</p>

<p>I <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecUptrainingExample.java"" rel=""nofollow noreferrer"">found the code</a> how to uptrain the already existing model. However, it is mentioned that new words are not added in this case and only weights for the existing words will be updated based on a new text corpus. </p>

<p>I would appreciate any help or ideas on the problem I have. Thanks in advance!</p>
","<java><nlp><emoji><word2vec><deeplearning4j>","2018-03-23 13:49:15","","0","3279338","2018-03-24 18:15:50","1","2","","","3279338","123","57","0"
"52828788","<p>I am trying to use DL4J for deep learning and have provided the training data with the labels. I am then trying to send a test data by assigning a dummy label. Without providing a dummy label, it gives runtime error. I dont understand why we need to assign label to test data. <br>
Additionally, I want to know what is the accuracy of the prediction made. From what I saw in the dl4j docs, there is something known as a confusion matrix which is generated. I understand that this just gives us an idea of how well the training data has trained the system. Is there a way to get the accuracy of prediction on test data? Since we are giving a dummy label for the test data, I feel that the confusion matrix is also not generated correctly.</p>
","<deep-learning><deeplearning4j><dl4j>","2018-10-16 05:59:46","","1","997988","2018-10-18 16:15:49","2","0","","","997988","46","7","0"
"43485657","<p>Help me please! I'm working on a project using <code>deeplearning4j</code>. The MNIST example works very well  but I get an error with my dataset.
My data set has two outputs.</p>

<pre><code>int height = 45;
int width = 800;
int channels = 1;
int rngseed = 123;
Random randNumGen = new Random(rngseed);
int batchSize = 128;
int outputNum = 2;
int numEpochs = 15;
File trainData = new File(""C:/Users/JHP/Desktop/learningData/training"");
File testData = new File(""C:/Users/JHP/Desktop/learningData/testing"");
FileSplit train = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, randNumGen);
FileSplit test = new FileSplit(testData, NativeImageLoader.ALLOWED_FORMATS, randNumGen);
ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator();

ImageRecordReader recordReader = new ImageRecordReader(height, width, channels, labelMaker);
ImageRecordReader recordReader2 = new ImageRecordReader(height, width, channels, labelMaker);
recordReader.initialize(train);
recordReader2.initialize(test);

DataSetIterator dataIter = new RecordReaderDataSetIterator(recordReader, batchSize, 1, outputNum);
DataSetIterator testIter = new RecordReaderDataSetIterator(recordReader2, batchSize, 1, outputNum);

DataNormalization scaler = new ImagePreProcessingScaler(0, 1);
scaler.fit(dataIter);
dataIter.setPreProcessor(scaler);

System.out.println(""Build model...."");
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
        .seed(rngseed)
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .iterations(1)
        .learningRate(0.006)
        .updater(Updater.NESTEROVS).momentum(0.9)
        .regularization(true).l2(1e-4)
        .list()
        .layer(0,   new DenseLayer.Builder()
                .nIn(height * width)
                .nOut(1000)
                .activation(Activation.RELU)
                .weightInit(WeightInit.XAVIER)
                .build()
                )
        .layer(1, newOutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD)
                .nIn(1000)
                .nOut(outputNum)
                .activation(Activation.SOFTMAX)
                .weightInit(WeightInit.XAVIER)
                .build()
                )
        .pretrain(false).backprop(true)
        .build();

MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();
model.setListeners(new ScoreIterationListener(1));

System.out.println(""Train model...."");
for (int i = 0; i &lt; numEpochs; i++) {
    try {
        model.fit(dataIter);
    } catch (Exception e) {
        System.out.println(e);
    }
}
</code></pre>

<p>error is </p>

<blockquote>
  <p>org.deeplearning4j.exception.DL4JInvalidInputException: Input that is
  not a matrix; expected matrix (rank 2), got rank 4 array with shape
  [128, 1, 45, 800]</p>
</blockquote>
","<deep-learning><mnist><deeplearning4j>","2017-04-19 03:11:30","","0","7887249","2017-04-20 04:19:40","1","2","","","7887249","3","0","0"
"39513337","<p>I am trying to load a CSV data set with canova/datavec, and can not find the ""idiomatic"" way of doing it. I struggle a bit since I feel that there is an evolution of the framework, which makes it difficult for me to determine what is relevant and what is not. </p>

<pre><code>object S extends App{
  val recordReader:RecordReader = new CSVRecordReader(0, "","")
  recordReader.initialize(new FileSplit(new File(""./src/main/resources/CSVdataSet.csv"")))
  val iter:DataSetIterator = new RecordReaderDataSetIterator(recordReader, 100)
  while(iter.hasNext){
    println(iter.next())
  }
}
</code></pre>

<p>I have a csv file that starts with a header description, and thus my output is an exception</p>

<pre><code>(java.lang.NumberFormatException: For input string: ""iid"":)
</code></pre>

<p>I started looking into the schema builder, since I get an exception because of schema/the header. So I was thinking to add a schema like this;</p>

<pre><code>val schema = new Schema.Builder()
    .addColumnInteger(""iid"")
    .build()   
</code></pre>

<p>From my point of view, the noob-view, the BasicDataVec-examples are not completely clear because they link it to spark etc. From the IrisAnalysisExample (<a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/datavec-examples/src/main/java/org/datavec/transform/analysis/IrisAnalysis.java"" rel=""nofollow"">https://github.com/deeplearning4j/dl4j-examples/blob/master/datavec-examples/src/main/java/org/datavec/transform/analysis/IrisAnalysis.java</a>). 
I assume that the file content is first read into JavaRDD (potentially a Stream) and then treated afterwards. The schema is not used except for the DataAnalysis. </p>

<p>So, could someone help with making me understand how I parse (as a stream or iterator, a CSV-file with a header description as the first line? </p>

<p>I understand from their book (Deep learning:A practitioners Approach) that  spark is needed for data transformation (which a schema is used for). I thus rewrote my code to;</p>

<pre><code>object S extends App{
  val schema: Schema = new Schema.Builder()
    .addColumnInteger(""iid"")
    .build
  val recordReader = new CSVRecordReader(0, "","")
  val f = new File(""./src/main/resources/CSVdataSet.csv"")
  recordReader.initialize(new FileSplit(f))
  val sparkConf:SparkConf = new SparkConf()
  sparkConf.setMaster(""local[*]"");
  sparkConf.setAppName(""DataVec Example"");
  val sc:JavaSparkContext = new JavaSparkContext(sparkConf)
  val lines = sc.textFile(f.getAbsolutePath);
  val examples = lines.map(new StringToWritablesFunction(new CSVRecordReader()))
  val process = new TransformProcess.Builder(schema).build()
  val executor = new SparkTransformExecutor()
  val processed = executor.execute(examples, process)
  println(processed.first())
}
</code></pre>

<p>I thought now that the schema would dictate that I only would have the iid-column, but the output is:</p>

<p>[iid, id, gender, idg, .....]</p>
","<scala><deeplearning4j>","2016-09-15 14:11:46","","2","1546268","2016-09-18 14:02:17","1","0","","","1546268","946","298","7"
"40332679","<p>I'm trying to build a neural network using deep learning4j framework, I get the following error : </p>

<pre><code>java.lang.IllegalStateException: Unexpected state occurred for AsyncDataSetIterator: runnable died or no data available"" this exception
</code></pre>

<p>Here is my code</p>

<pre><code>package com.neuralnetwork;

import com.sliit.preprocessing.NormalizeDataset;
import com.sliit.ruleengine.RuleEngine;
import org.apache.commons.io.FilenameUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.datavec.api.records.reader.SequenceRecordReader;
import org.datavec.api.records.reader.impl.csv.CSVSequenceRecordReader;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.layers.GravesLSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.util.ModelSerializer;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.api.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;
import weka.core.Instances;
import weka.core.converters.CSVLoader;
import weka.core.converters.CSVSaver;
import weka.filters.Filter;
import weka.filters.supervised.instance.StratifiedRemoveFolds;

import java.io.*;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

/**
 * Deep Belif Neural Network to detect frauds.
 */
public class FraudDetectorDeepBeilefNN {

    private static final Log log = LogFactory.getLog(FraudDetectorDeepBeilefNN.class);
    public int outputNum = 4;
    private int iterations = 5;
    private int seed = 1234;
    private MultiLayerNetwork model = null;
    public int HIDDEN_LAYER_COUNT = 8;
    public int numHiddenNodes = 21;
    public int inputs = 41;
    private String uploadDirectory = ""D:/Data"";
    private ArrayList &lt; Map &lt; String, Double &gt;&gt; roc;

    public FraudDetectorDeepBeilefNN() {    
    }

    public void buildModel() {    
        System.out.println(""Build model...."");
        iterations = outputNum + 1;
        NeuralNetConfiguration.Builder builder = new NeuralNetConfiguration.Builder();
        builder.iterations(iterations);
        builder.learningRate(0.001);
        // builder.momentum(0.01);
        builder.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);
        builder.seed(seed);
        builder.biasInit(1);
        builder.regularization(true).l2(1e-5);
        builder.updater(Updater.RMSPROP);
        builder.weightInit(WeightInit.XAVIER);

        NeuralNetConfiguration.ListBuilder list = builder.list();

        for (int i = 0; i &lt; HIDDEN_LAYER_COUNT; i++) {

            GravesLSTM.Builder hiddenLayerBuilder = new GravesLSTM.Builder();
            hiddenLayerBuilder.nIn(i == 0 ? inputs : numHiddenNodes);
            hiddenLayerBuilder.nOut(numHiddenNodes);
            hiddenLayerBuilder.activation(""tanh"");
            list.layer(i, hiddenLayerBuilder.build());
        }

        RnnOutputLayer.Builder outputLayer = new RnnOutputLayer.Builder(LossFunction.MCXENT);
        outputLayer.activation(""softmax"");
        outputLayer.nIn(numHiddenNodes);
        outputLayer.nOut(outputNum);
        list.layer(HIDDEN_LAYER_COUNT, outputLayer.build());
        list.pretrain(false);
        list.backprop(true);
        MultiLayerConfiguration configuration = list.build();
        model = new MultiLayerNetwork(configuration);
        model.init();
        //model.setListeners(new ScoreIterationListener(1));    
    }

    public String trainModel(String modelName, String filePath, int outputs, int inputsTot) throws NeuralException {

        try {

            System.out.println(""Neural Network Training start"");
            loadSaveNN(modelName, false);
            if (model == null) {

                buildModel();
            }
            System.out.println(""modal"" + model);
            System.out.println(""file path "" + filePath);
            File fileGeneral = new File(filePath);
            CSVLoader loader = new CSVLoader();
            loader.setSource(fileGeneral);
            Instances instances = loader.getDataSet();
            instances.setClassIndex(instances.numAttributes() - 1);
            StratifiedRemoveFolds stratified = new StratifiedRemoveFolds();
            String[] options = new String[6];
            options[0] = ""-N"";
            options[1] = Integer.toString(5);
            options[2] = ""-F"";
            options[3] = Integer.toString(1);
            options[4] = ""-S"";
            options[5] = Integer.toString(1);
            stratified.setOptions(options);
            stratified.setInputFormat(instances);
            stratified.setInvertSelection(false);
            Instances testInstances = Filter.useFilter(instances, stratified);
            stratified.setInvertSelection(true);
            Instances trainInstances = Filter.useFilter(instances, stratified);
            String directory = fileGeneral.getParent();
            CSVSaver saver = new CSVSaver();
            File trainFile = new File(directory + ""/"" + ""normtrainadded.csv"");
            File testFile = new File(directory + ""/"" + ""normtestadded.csv"");
            if (trainFile.exists()) {

                trainFile.delete();
            }
            trainFile.createNewFile();
            if (testFile.exists()) {

                testFile.delete();
            }
            testFile.createNewFile();
            saver.setFile(trainFile);
            saver.setInstances(trainInstances);
            saver.writeBatch();
            saver = new CSVSaver();
            saver.setFile(testFile);
            saver.setInstances(testInstances);
            saver.writeBatch();
            SequenceRecordReader recordReader = new CSVSequenceRecordReader(0, "","");
            recordReader.initialize(new org.datavec.api.split.FileSplit(trainFile));
            SequenceRecordReader testReader = new CSVSequenceRecordReader(0, "","");
            testReader.initialize(new org.datavec.api.split.FileSplit(testFile));
            DataSetIterator iterator = new org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator(recordReader, 2, outputs, inputsTot, false);
            DataSetIterator testIterator = new org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator(testReader, 2, outputs, inputsTot, false);
            roc = new ArrayList &lt; Map &lt; String, Double &gt;&gt; ();
            String statMsg = """";
            Evaluation evaluation;
            for (int i = 0; i &lt; 100; i++) {
                if (i % 2 == 0) {

                    model.fit(iterator);
                    evaluation = model.evaluate(testIterator);
                } else {

                    model.fit(testIterator);
                    evaluation = model.evaluate(iterator);
                }
                Map &lt; String, Double &gt; map = new HashMap &lt; String, Double &gt; ();
                Map &lt; Integer, Integer &gt; falsePositives = evaluation.falsePositives();
                Map &lt; Integer, Integer &gt; trueNegatives = evaluation.trueNegatives();
                Map &lt; Integer, Integer &gt; truePositives = evaluation.truePositives();
                Map &lt; Integer, Integer &gt; falseNegatives = evaluation.falseNegatives();
                double fpr = falsePositives.get(1) / (falsePositives.get(1) + trueNegatives.get(1));
                double tpr = truePositives.get(1) / (truePositives.get(1) + falseNegatives.get(1));
                map.put(""FPR"", fpr);
                map.put(""TPR"", tpr);
                roc.add(map);
                statMsg = evaluation.stats();
                iterator.reset();
                testIterator.reset();
            }
            loadSaveNN(modelName, true);
            System.out.println(""ROC "" + roc);
            return statMsg;

        } catch (Exception e) {
            e.printStackTrace();
            System.out.println(""Error ocuured while building neural netowrk :"" + e.getMessage());
            throw new NeuralException(e.getLocalizedMessage(), e);
        }
    }

    public boolean generateModel(String modelName) {

        boolean status = false;
        try {    
            loadSaveNN(modelName, true);
            status = true;
        } catch (Exception e) {

            System.out.println(""Error occurred:"" + e.getLocalizedMessage());
        }
        return status;
    }

    private void loadSaveNN(String name, boolean save) {    

        File directory = new File(uploadDirectory);
        File[] allNN = directory.listFiles();
        boolean status = false;
        try {

            if (model == null &amp;&amp; save) {

                buildModel();
            }
            if (allNN != null &amp;&amp; allNN.length &gt; 0) {
                for (File NN: allNN) {

                    String fnme = FilenameUtils.removeExtension(NN.getName());
                    if (name.equals(fnme)) {

                        status = true;
                        if (save) {

                            ModelSerializer.writeModel(model, NN, true);
                            System.out.println(""Model Saved With Weights Successfully"");

                        } else {

                            model = ModelSerializer.restoreMultiLayerNetwork(NN);
                        }
                        break;
                    }
                }
            }
            if (!status &amp;&amp; save) {

                //File tempFIle = File.createTempFile(name,"".zip"",directory);
                File tempFile = new File(directory.getAbsolutePath() + ""/"" + name + "".zip"");
                if (!tempFile.exists()) {

                    tempFile.createNewFile();
                }
                ModelSerializer.writeModel(model, tempFile, true);
            }
        } catch (IOException e) {
            System.out.println(""Error occurred:"" + e.getMessage());
        }
    }

    public String testModel(String modelName, String[] rawData, Map &lt; Integer, String &gt; map, int inputs, int outputs, String ruleModelSavePath) throws Exception {

        String status = """";
        String fpath = uploadDirectory;
        FileWriter fwriter = new FileWriter(uploadDirectory + ""original/insertdata.csv"", true);
        fwriter.write(""\n"");
        fwriter.write(rawData[0]);
        fwriter.close();
        if (model == null) {
            loadSaveNN(modelName, false);
        }
        NormalizeDataset norm = new NormalizeDataset(uploadDirectory + ""original/insertdata.csv"");
        norm.updateStringValues(map);
        norm.whiteningData();
        norm.normalizeDataset();
        BufferedReader bufferedReader = new BufferedReader(new FileReader(new File(uploadDirectory + ""originalnorminsertdata.csv"")));
        String output = """";
        String prevOutput = """";
        while ((output = bufferedReader.readLine()) != null) {

            prevOutput = output;
        }
        bufferedReader.close();
        File readFile = new File(uploadDirectory + ""normtest.csv"");
        if (readFile.exists()) {

            readFile.delete();
        }
        readFile.createNewFile();
        PrintWriter writer = new PrintWriter(readFile);
        writer.println(prevOutput);
        writer.flush();
        writer.close();
        SequenceRecordReader recordReader = new CSVSequenceRecordReader(0, "","");
        recordReader.initialize(new org.datavec.api.split.FileSplit(new File(uploadDirectory + ""normtest.csv"")));
        DataSetIterator iterator = new org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator(recordReader, 2, outputs, inputs, false);
        INDArray outputArr = null;
        while (iterator.hasNext()) {

            DataSet ds = iterator.next();
            INDArray provided = ds.getFeatureMatrix();
            outputArr = model.rnnTimeStep(provided);
        }
        //INDArray finalOutput = Nd4j.argMax(outputArr,1);
        double result = Double.parseDouble(Nd4j.argMax(outputArr, 1).toString());
        if (result == 0.0) {

            status = ""Normal Transaction"";
        } else {

            status = ""Fraud Transaction, "";
            bufferedReader = new BufferedReader(new FileReader(new File(uploadDirectory + ""original/insertdata.csv"")));
            String heading = """";
            heading = bufferedReader.readLine();
            bufferedReader.close();
            File ruleFile = new File(uploadDirectory + ""normrules.csv"");
            if (ruleFile.exists()) {

                ruleFile.delete();
            }
            ruleFile.createNewFile();
            PrintWriter writeNew = new PrintWriter(ruleFile);
            writeNew.println(heading);
            writeNew.println(rawData[0]);
            writeNew.flush();
            writeNew.close();
            RuleEngine engine = new RuleEngine(fpath + ""original/insertdata.csv"");
            engine.geneateModel(ruleModelSavePath, false);
            String finalStatus = status + ""Attack Type:"" + engine.predictionResult(uploadDirectory + ""normrules.csv"");
            status = finalStatus;
        }
        return status;
    }

    public void setUploadDirectory(String uploadDirectory) {    
        this.uploadDirectory = uploadDirectory;    
    }

    public static void main(String[] args) {

        FraudDetectorDeepBeilefNN neural_network = new FraudDetectorDeepBeilefNN();
        System.out.println(""start======================="");
        try {
            neural_network.inputs = 4;
            neural_network.numHiddenNodes = 3;
            neural_network.HIDDEN_LAYER_COUNT = 2;
            neural_network.outputNum = 2;
            neural_network.buildModel();
            String output = neural_network.trainModel(""nn"", ""D:/Data/a.csv"", 2, 4);
            System.out.println(output);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    public ArrayList &lt; Map &lt; String, Double &gt;&gt; getRoc() {
        return roc;
    }
}
</code></pre>

<p>Here is the 1st few lines of my dataset : </p>

<pre><code>length,caplen,hlen,version,output
60,60,6,0,normal
243,243,8,0,normal
60,60,6,0,neptune
60,60,6,0,neptune
</code></pre>
","<java><exception><neural-network><deep-learning><deeplearning4j>","2016-10-30 19:21:17","","2","4029417","2016-11-02 09:50:02","0","3","","","4029417","26","0","0"
"41136856","<p>I get the following RuntimeException:</p>

<pre><code>java.lang.RuntimeException: org.nd4j.linalg.factory.Nd4jBackend$NoAvailableBackendException: Please ensure that you have an nd4j backend on your classpath. Please see: http://nd4j.org/getstarted.html
</code></pre>

<p>it seems to me that i am missing an nd4j backend i here is my gradel file dependencies:</p>

<pre><code>dependencies {
   compile group: 'org.nd4j', name: 'nd4j-context', version: '0.7.1'
   compile('org.deeplearning4j:deeplearning4j-nn:0.7.1')
}
</code></pre>

<p>tryed using nd4j-native and bunch of other but didn't help.</p>

<p>Have no idea what more to do.</p>
","<java><android><runtimeexception><deeplearning4j><nd4j>","2016-12-14 07:15:51","","1","7290785","2016-12-20 01:16:58","1","1","","","7290785","6","0","0"
"37133444","<p>I am trying to run the example of deeplearning4j LenetMnistExample. Error of ""Frame is not available"" occurrs in these lines</p>

<pre><code>DataSetIterator mnistTrain = new MnistDataSetIterator(batchSize,true,12345);
DataSetIterator mnistTrain = new MnistDataSetIterator(batchSize,true,12345);
</code></pre>

<p>Can anyone help how to resolve this?</p>
","<java><deeplearning4j>","2016-05-10 08:47:12","","0","5358156","2016-05-18 06:04:05","1","0","","","5358156","65","6","0"
"44858023","<p>I am trying to build a complex neural network using Computation Graph implementation in Deeplearning4J. I need to have multiple outputs so that's why I can't go with the generic MultiLayerConfiguration.
However, my problem is that in this case I do not know how to do the evaluation of my model and I would like at least to know the accuracy.
Has anybody worked with Comp Graphs in dl4j?</p>
","<neural-network><evaluation><deeplearning4j><dl4j>","2017-07-01 06:28:42","","0","4650403","2017-07-01 12:17:08","1","0","","","4650403","1","0","0"
"44346756","<p>I have a CSV dataset with both numerical and nominal attributes. I defined <strong>Schema</strong> for dataset that list all possible values for nominal attributes. After that I created <strong>TransformProcess</strong> to convert nominal values to numeric values using  <strong>CategoricalToOneHotTransform</strong>. How can I use this <strong>TransformProcess</strong> on <strong>RecordReaderDataSetIterator</strong> to prepare for my neural network ?</p>

<pre><code>        Schema schema = new Schema.Builder()
        .addColumnInteger(""age"")
        .addColumnCategorical(""workclass"", ""Private"", ""Self-emp-not-inc"", ""Self-emp-inc"", ""Federal-gov"", ""Local-gov"", ""State-gov"", ""Without-pay"", ""Never-worked"")
        .addColumnInteger(""fnlwgt"")
        .addColumnCategorical(""education"", ""Bachelors"", ""Some-college"", ""11th"", ""HS-grad"", ""Prof-school"", ""Assoc-acdm"", ""Assoc-voc"", ""9th"", ""7th-8th"", ""12th"", ""Masters"", ""1st-4th"", ""10th"", ""Doctorate"", ""5th-6th"", ""Preschool"")
        .addColumnInteger(""education-num"")
        .addColumnCategorical(""marital-status"", ""Married-civ-spouse"", ""Divorced"", ""Never-married"", ""Separated"", ""Widowed"", ""Married-spouse-absent"", ""Married-AF-spouse"")
        .addColumnCategorical(""occupation"", ""Tech-support"", ""Craft-repair"", ""Other-service"", ""Sales"", ""Exec-managerial"", ""Prof-specialty"", ""Handlers-cleaners"", ""Machine-op-inspct"", ""Adm-clerical"", ""Farming-fishing"", ""Transport-moving"", ""Priv-house-serv"", ""Protective-serv"", ""Armed-Forces"")
        .addColumnCategorical(""relationship"", ""Wife"", ""Own-child"", ""Husband"", ""Not-in-family"", ""Other-relative"", ""Unmarried"")
        .addColumnCategorical(""race"", ""White"", ""Asian-Pac-Islander"", ""Amer-Indian-Eskimo"", ""Other"", ""Black"")
        .addColumnCategorical(""sex"", ""Female"", ""Male"")
        .addColumnInteger(""capital-gain"")
        .addColumnInteger(""capital-loss"")
        .addColumnInteger(""hours-per-week"")
        .addColumnCategorical(""native-country"", ""United-States"", ""Cambodia"", ""England"", ""Puerto-Rico"", ""Canada"", ""Germany"", ""Outlying-US(Guam-USVI-etc)"", ""India"", ""Japan"", ""Greece"", ""South"", ""China"", ""Cuba"", ""Iran"", ""Honduras"", ""Philippines"", ""Italy"", ""Poland"", ""Jamaica"", ""Vietnam"", ""Mexico"", ""Portugal"", ""Ireland"", ""France"", ""Dominican-Republic"", ""Laos"", ""Ecuador"", ""Taiwan"", ""Haiti"", ""Columbia"", ""Hungary"", ""Guatemala"", ""Nicaragua"", ""Scotland"", ""Thailand"", ""Yugoslavia"", ""El-Salvador"", ""Trinadad&amp;Tobago"", ""Peru"", ""Hong"", ""Holand-Netherlands"")
        .addColumnCategorical(""class"", ""&gt;50K"", ""&lt;=50K"")
        .build();

    TransformProcess tp = new TransformProcess.Builder(schema)
        .transform(new CategoricalToOneHotTransform(""workclass""))
        .transform(new CategoricalToOneHotTransform(""education""))
        .transform(new CategoricalToOneHotTransform(""marital-status""))
        .transform(new CategoricalToOneHotTransform(""occupation""))
        .transform(new CategoricalToOneHotTransform(""relationship""))
        .transform(new CategoricalToOneHotTransform(""race""))
        .transform(new CategoricalToOneHotTransform(""sex""))
        .transform(new CategoricalToOneHotTransform(""native-country""))
        .transform(new CategoricalToIntegerTransform(""class""))
        .build();

    Schema outputSchema = tp.getFinalSchema();

    int numLinesToSkip = 0;
    String delimiter = "","";
    CSVRecordReader recordReader = new CSVRecordReader(numLinesToSkip, delimiter);
    recordReader.initialize(new FileSplit(Paths.get(""..\\adult.data"").toFile()));


    int labelIndex = outputSchema.getColumnNames().size() - 1;
    int numClasses = 2;
    int batchSize = 2000;

    RecordReaderDataSetIterator iterator = new RecordReaderDataSetIterator(recordReader, batchSize, labelIndex, numClasses);

    DataSet allData = iterator.next();
    allData.shuffle();
    SplitTestAndTrain testAndTrain = allData.splitTestAndTrain(0.65);
</code></pre>
","<java><csv><deeplearning4j>","2017-06-03 17:29:42","","2","1288977","2017-06-04 02:24:21","1","0","","","1288977","525","87","4"
"55080746","<p>I use DL4J and I attach the UI.
How can I display on UI the cost error of the <strong>validation set</strong>?
What have I to configure? </p>

<p>I have to check if there is overfitting but the UI display only the cost error of the <strong>training set</strong>. See the image attached below
<a href=""https://i.stack.imgur.com/RdPPa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RdPPa.png"" alt=""enter image description here""></a></p>

<p>DL4J version: 1.0.0-beta3</p>
","<performance><neural-network><deeplearning4j><dl4j>","2019-03-09 18:39:01","","0","5242553","2019-03-09 18:45:37","0","0","","","5242553","398","8","1"
"55659943","<p>I have been struggling with loading my keras neural network model for my Android application with deeplearning4j for a while now. I have searched for solutions (as much as there are), but every solution brings up new errors and I just could not get this thing to work.</p>

<p>Anyways, I have trained a <strong>NON</strong> sequential model with keras in Python and saved it like this:</p>

<pre><code>model.save('model.h5')
</code></pre>

<p>Now I am trying to import this model with deeplearning4j in Android Studio. I have tried many possible variants, but this is where I am now:</p>

<pre><code>String modelPath = new ClassPathResource(""res/raw/model.h5"").getFile().getPath();
ComputationGraph model = KerasModelImport.importKerasModelAndWeights(modelPath)
</code></pre>

<p>This however triggers a following error:</p>

<pre><code>java.lang.NoClassDefFoundError: Failed resolution of: Lorg/bytedeco/javacpp/hdf5;
</code></pre>

<p>As I understand, gradle is unable to resolve a dependency  <code>hdf5</code> from <code>org.bytedeco</code>, which I agree on as I have excluded <code>hdf5-platform</code> in my gradle build, but <code>hdf5</code> should not even be supported by Android as far as I know (?).</p>

<p>I have also tried to include <code>hdf5-platform</code> and run the same code, but doing so triggers another error:</p>

<pre><code>java.lang.UnsatisfiedLinkError: Platform ""android-arm64"" not supported by class org.bytedeco.javacpp.hdf5
</code></pre>

<p>I am rather new into gradle concepts and I do not know Android in depth, but seems that the problem is with my gradle dependencies. There is also limited amount of information about <code>deeplearning4j</code> and I could not figure out an alternative solution either.</p>

<p>I will also include my gradle dependencies I have from <a href=""https://deeplearning4j.org/docs/latest/deeplearning4j-android"" rel=""nofollow noreferrer"">this tutorial.</a></p>

<pre><code>implementation (group: 'org.deeplearning4j', name: 'deeplearning4j-core', version: '1.0.0-beta3') {
    exclude group: 'org.bytedeco.javacpp-presets', module: 'opencv-platform'
    exclude group: 'org.bytedeco.javacpp-presets', module: 'leptonica-platform'
    exclude group: 'org.bytedeco.javacpp-presets', module: 'hdf5-platform'
    exclude group: 'org.nd4j', module: 'nd4j-base64'
}
implementation group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta3'
implementation group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta3', classifier: ""android-arm""
implementation group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta3', classifier: ""android-arm64""
implementation group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta3', classifier: ""android-x86""
implementation group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta3', classifier: ""android-x86_64""
implementation group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.3-1.4.3'
implementation group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.3-1.4.3', classifier: ""android-arm""
implementation group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.3-1.4.3', classifier: ""android-arm64""
implementation group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.3-1.4.3', classifier: ""android-x86""
implementation group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.3-1.4.3', classifier: ""android-x86_64""
implementation group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.3-1.4.3'
implementation group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.3-1.4.3', classifier: ""android-arm""
implementation group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.3-1.4.3', classifier: ""android-arm64""
implementation group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.3-1.4.3', classifier: ""android-x86""
implementation group: 'org.bytedeco.javacpp-presets', name: 'opencv', version: '3.4.3-1.4.3', classifier: ""android-x86_64""
implementation group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.3'
implementation group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.3', classifier: ""android-arm""
implementation group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.3', classifier: ""android-arm64""
implementation group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.3', classifier: ""android-x86""
implementation group: 'org.bytedeco.javacpp-presets', name: 'leptonica', version: '1.76.0-1.4.3', classifier: ""android-x86_64""
</code></pre>

<p><strong>(How) should I change my dependencies to get this model importing to work?</strong></p>

<p><strong>Or should I change the way of importing my model somehow?</strong></p>
","<android><gradle><android-gradle><build.gradle><deeplearning4j>","2019-04-12 21:58:41","","1","5524775","2019-04-13 03:11:33","1","1","","","5524775","114","26","0"
"47184886","<p>i am working on neural network (java, deeplearning4j), that will classify the words. For example i will have 3 groups: sport, music, science. I check topics, tutorials and examples. The problem is: what is the best way to do it?</p>

<p>That is what i tried:
1) text file like this:
       football, 0
       bass, 1
       aircraft, 2 ....
   Than i create a CSVRecordReader ... but it crashed because it is a text.
2) Create Word2Vec with words, but i don't know how to label it to get suitable groups.</p>

<p>Could you please advise? Thank you in advance</p>
","<java><word2vec><deeplearning4j>","2017-11-08 16:30:14","","1","6195023","2017-11-08 16:30:14","0","0","1","","6195023","6","0","0"
"41596911","<p>I'm trying to execute a fat jar builded with sbt-assembly using spark and deepLearning4J, unfortunately during execution i'm confronted to the <strong>Exception in thread ""main"" java.lang.NoClassDefFoundError:</strong> error for many jars.
I tried to add jars using the --jars option in spark-submit but when i add a jar, i'm falling on the same error for another class from another dependency.</p>

<p>If i understand well, the FatJar generated by sbt-assembly should prevent this kind of problem because it include all needed jar.</p>

<p>My scala files are into <strong>myproject/src/main/scala/xxx/spark/yyy/</strong></p>

<p>Maybe it's due to the merging strategy ?</p>

<p>I join my build.sbt file if it can help !</p>

<p>Thank you in advance.</p>

<pre><code>name := ""myproject""

version := ""1.0""

scalaVersion := ""2.10.4""

val sparkVersion = ""1.6.2""

mainClass in assembly := Some(""xxx.spark.yyy.Main"")

resolvers += Resolver.sojava.lang.NoClassDefFoundErrornatypeRepo(""releases"")

resolvers += ""Spark Packages Repo"" at ""https://dl.bintray.com/spark-packages/maven""

resolvers += ""Akka Snapshot Repository"" at ""http://repo.akka.io/snapshots/""

resolvers += ""Artifactory"" at ""http://artifacts.kameleoon.net:8081/artifactory/sbt/""

resolvers += ""Sbt plugins"" at ""https://dl.bintray.com/sbt/sbt-plugin-releases""

resolvers += ""Sonatype Releases"" at ""https://oss.sonatype.org/content/repositories/releases/""

resolvers += Resolver.url(""artifactory"", url(""http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases""))(Resolver.ivyStylePatterns)


libraryDependencies ++= Seq(
""org.apache.spark"" %% ""spark-core"" % sparkVersion % ""provided"",
""org.apache.spark"" %% ""spark-sql"" % sparkVersion % ""provided"",
""com.datastax.spark"" %% ""spark-cassandra-connector"" % ""1.6.0"",
""org.apache.spark""  %% ""spark-mllib""  % sparkVersion % ""provided"",
""org.hibernate"" % ""hibernate-core"" % ""4.3.11.Final"",
""org.hibernate"" % ""hibernate-entitymanager"" % ""4.3.11.Final"",
compilerPlugin(""org.scalamacros"" % ""paradise"" % ""2.1.0"" cross CrossVersion.full),
""org.json"" % ""json"" % ""20160810"",
""org.joda"" % ""joda-convert"" % ""1.2"",
""jfree"" % ""jfreechart"" % ""1.0.13"",
""commons-io"" % ""commons-io"" % ""2.4"",
""com.google.guava"" % ""guava"" % ""20.0"",
""jfree"" % ""jfreechart"" % ""1.0.13"",
""org.bytedeco"" % ""javacv"" % ""1.2"",
""org.datavec"" % ""datavec-data-codec"" % ""0.7.2"",
""org.datavec"" % ""datavec-spark_2.10"" % ""0.7.2"",
""org.datavec"" % ""datavec-api"" % ""0.7.2"",
""org.deeplearning4j"" % ""deeplearning4j-core"" % ""0.7.2"",
""org.deeplearning4j"" % ""deeplearning4j-nn"" % ""0.7.2"",
""org.deeplearning4j"" % ""dl4j-spark_2.10"" % ""0.7.2"",
""org.jblas"" % ""jblas"" % ""1.2.4""
)


assemblyMergeStrategy in assembly := {
    case PathList(""org"", ""joda"", ""time"", ""base"", ""BaseDateTime.class"") =&gt; MergeStrategy.first
    case PathList(""com"", ""esotericsoftware"", ""minlog"", ""Log.class"") =&gt; MergeStrategy.first
    case PathList(""org"", ""apache"", xs @ _*) =&gt; MergeStrategy.last
    case PathList(""com"", ""google"", xs @ _*) =&gt; MergeStrategy.last
    case PathList(""META-INF"", xs @ _*) =&gt; MergeStrategy.rename
    case ""about.html"" =&gt; MergeStrategy.rename
    case x =&gt; val oldStrategy = (assemblyMergeStrategy in assembly).value
    oldStrategy(x)
}
</code></pre>
","<apache-spark><sbt><runtime-error><sbt-assembly><deeplearning4j>","2017-01-11 17:19:34","","4","3608476","2017-01-11 17:20:36","0","2","","","3608476","414","200","0"
"41435319","<p>As part of a larger piece of code, I am using this</p>

<pre><code>INDArray imageArray = Nd4j.create(rgbValues);
</code></pre>

<p>to create my input array for getting an output from my already-trained neural network. rgbValues is a single-dimensional array of approximately 10,000 floats. This one line of code takes about 3 seconds to run, and I'm not sure why. Any help with this is appreciated.</p>
","<java><arrays><deeplearning4j><nd4j>","2017-01-03 01:40:05","","0","2465177","2017-01-03 04:25:03","1","0","","","2465177","97","5","0"
"43263692","<p>I need to run a simple Java based deeplearning4j example in hadoop cluster and I found one <a href=""https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/nlp/word2vec/Word2VecRawTextExample.java"" rel=""nofollow noreferrer"">here</a>. My need to specify the input from command line (which should be a path on HDFS) and output should go to HDFS, for later view</p>

<p>However, in the example there is no mention, it is hard coding the input from local file system and output goes to local file system.</p>

<p>Can anyone help me here?</p>
","<hadoop><apache-spark><deeplearning4j>","2017-04-06 19:00:45","","0","3438473","2017-04-06 23:00:05","1","0","","","3438473","2244","819","9"
"50548892","<p>I need to make changes to an existing deeplearning4j (DL4J) model that has already been trained. The network consists of an input layer, one Graves LSTM and one RNN Output Layer. </p>

<p>My question is: Is it possible to add one or more untrained neurons to the LSTM layer without having to rebuild the model from a new config (which would I assume require retraining it)? I'd like to do things like, add one or more neurons to an existing layer, or add an entire layer (untrained) to a trained model.</p>

<p>Are these possible? I couldn't find any references to this, but I've seen folks doing it in other languages/frameworks so I wonder if I can also do it in DL4J.</p>

<p>BTW I'm aware this is an unusual thing to be doing. Please ignore the fact it will mess up the trained network, I just need to know if I can do it and how to go about it. :)</p>

<p>Any pointers will help!</p>

<p>Thanks!</p>

<p>Eduardo</p>
","<java><neural-network><deep-learning><topology><deeplearning4j>","2018-05-27 03:59:55","","1","489088","2018-05-27 06:45:12","1","0","","","489088","1697","63","2"
"47839617","<p>Consider the following code, which uses the <a href=""https://nd4j.org/"" rel=""nofollow noreferrer"">ND4J library</a> to create a simpler version of <a href=""https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/datasets/samples_generator.py#L639"" rel=""nofollow noreferrer"">the ""moons"" test data set</a>:</p>

<pre><code>val n = 100
val n1: Int = n/2
val n2: Int = n-n1
val outerX = Nd4j.getExecutioner.execAndReturn(new Cos(Nd4j.linspace(0, Math.PI, n1)))
val outerY = Nd4j.getExecutioner.execAndReturn(new Sin(Nd4j.linspace(0, Math.PI, n1)))
val innerX = Nd4j.getExecutioner.execAndReturn(new Cos(Nd4j.linspace(0, Math.PI, n2))).mul(-1).add(1)
val innerY = Nd4j.getExecutioner.execAndReturn(new Sin(Nd4j.linspace(0, Math.PI, n2))).mul(-1).add(1)
val X: INDArray = Nd4j.vstack(
  Nd4j.concat(1, outerX, innerX), // 1 x n
  Nd4j.concat(1, outerY, innerY)  // 1 x n
) // 2 x n
val y: INDArray = Nd4j.hstack(
  Nd4j.zeros(n1), // 1 x n1
  Nd4j.ones(n2)   // 1 x n2
) // 1 x n
println(s""# y shape: ${y.shape().toList}"")                        // 1x100
println(s""# y data length: ${y.data().length()}"")                 // 100
println(s""# X shape: ${X.shape().toList}"")                        // 2x100
println(s""# X row 0 shape: ${X.getRow(0).shape().toList}"")        // 1x100
println(s""# X row 1 shape: ${X.getRow(1).shape().toList}"")        // 1x100
println(s""# X row 0 data length: ${X.getRow(0).data().length()}"") // 200    &lt;- !
println(s""# X row 1 data length: ${X.getRow(1).data().length()}"") // 100
</code></pre>

<p>On the second to last line, <code>X.getRow(0).data().length()</code> is, surprisingly, 200 not 100. On inspection this is because the structure returned by <code>data()</code> contains the entire matrix, i.e. both rows, concatenated.</p>

<p>How do I get just the actual first row of the X matrix into a Java (or Scala) <code>List</code>? I could take just the first 100 items of the 200-element ""first row"", but that doesn't seem very elegant.</p>
","<java><scala><deeplearning4j><nd4j>","2017-12-15 20:43:07","","0","3921643","2019-01-01 15:04:04","2","0","","","3921643","55","5","0"
"36214342","<p>I am new no ML and I hava strated using Deeplearning4j library. And I literaly got lost in the source code. How can i read training set with multiple labels, but not just 1? For example I wan't to teach lstm to classify texts in 4 classes. How can i read trainig dataset for that?
Thanks</p>

<p><strong>Edit:</strong>
This is what my iterator's code looks like now. I hava got POJO class for vacancy, which contains only list of skill's ids and vacancy text. In each file for each train/test set 2 lines: one with ids (comma is the separator) and text. All set contains 4 skills, so net's outputs equals 5. I have trained word2vec model, so my iterator also uses that.</p>

<p>I use original code example for <a href=""https://github.com/deeplearning4j/dl4j-0.4-examples/tree/master/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment"" rel=""nofollow"">sentimenal analysis</a></p>

<p>My iterator:</p>

<pre><code>package SkillsMiner;    
import SkillsMiner.Entities.VacancyLightEntity;
import SkillsMiner.Utils.Reader;
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.FilenameUtils;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.deeplearning4j.text.tokenization.tokenizer.preprocessor.CommonPreprocessor;
import org.deeplearning4j.text.tokenization.tokenizerfactory.DefaultTokenizerFactory;
import org.deeplearning4j.text.tokenization.tokenizerfactory.TokenizerFactory;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.DataSetPreProcessor;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.INDArrayIndex;
import org.nd4j.linalg.indexing.NDArrayIndex;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.NoSuchElementException;

/** This is a DataSetIterator that is specialized for the IMDB review dataset used in the Word2VecSentimentRNN example
 * It takes either the train or test set data from this data set, plus a WordVectors object (typically the Google News
 * 300 pretrained vectors from https://code.google.com/p/word2vec/) and generates training data sets.&lt;br&gt;
 * Inputs/features: variable-length time series, where each word (with unknown words removed) is represented by
 * its Word2Vec vector representation.&lt;br&gt;
 * Labels/target: a single class (negative or positive), predicted at the final time step (word) of each review
 *
 * @author Alex Black
 */
public class SentimentExampleIterator implements DataSetIterator {
    private final WordVectors wordVectors;
    private final int batchSize;
    private final int vectorSize;
    private final int truncateLength;

    private int cursor = 0;
    private final File[] filePathes;
    private final TokenizerFactory tokenizerFactory;
    private int labelsCount = 4;

    /**
     * @param dataDirectory the directory of the IMDB review data set
     * @param wordVectors WordVectors object
     * @param batchSize Size of each minibatch for training
     * @param truncateLength If reviews exceed
     * @param train If true: return the training data. If false: return the testing data.
     */
    public SentimentExampleIterator(String dataDirectory, WordVectors wordVectors, int batchSize, int truncateLength, boolean train) throws IOException {
        this.batchSize = batchSize;
        this.vectorSize = wordVectors.lookupTable().layerSize();

        File p = new File(FilenameUtils.concat(dataDirectory, ""learning/"" + (train ? ""train"" : ""test"")) + ""/"");
        filePathes = p.listFiles();

        this.wordVectors = wordVectors;
        this.truncateLength = truncateLength;

        tokenizerFactory = new DefaultTokenizerFactory();
        tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());
    }


    @Override
    public DataSet next(int num) {
        if (cursor &gt;= filePathes.length) throw new NoSuchElementException();
        try{
            return nextDataSet(num);
        }catch(IOException e){
            throw new RuntimeException(e);
        }
    }

    private DataSet nextDataSet(int num) throws IOException {
        List&lt;VacancyLightEntity&gt; vacancies = new ArrayList&lt;&gt;(num);
        boolean[] positive = new boolean[num];
        for( int i=0; i&lt;num &amp;&amp; cursor&lt;totalExamples(); i++ ){
            String path = filePathes[cursor].getAbsolutePath();
            vacancies.add(Reader.readVacancyFromFile(path));
            cursor++;
        }

        //Second: tokenize vacancies and filter out unknown words
        List&lt;List&lt;String&gt;&gt; allTokens = new ArrayList&lt;&gt;(vacancies.size());
        int maxLength = 0;
        for(VacancyLightEntity v : vacancies){
            List&lt;String&gt; tokens = tokenizerFactory.create(v.getText()).getTokens();
            List&lt;String&gt; tokensFiltered = new ArrayList&lt;&gt;();
            for(String t : tokens ){
                if(wordVectors.hasWord(t)) tokensFiltered.add(t);
            }
            allTokens.add(tokensFiltered);
            maxLength = Math.max(maxLength,tokensFiltered.size());
        }
        //If longest review exceeds 'truncateLength': only take the first 'truncateLength' words
        if(maxLength &gt; truncateLength) maxLength = truncateLength;

        //Create data for training
        //Here: we have vacancies.size() examples of varying lengths
        INDArray features = Nd4j.create(vacancies.size(), vectorSize, maxLength);
        INDArray labels = Nd4j.create(vacancies.size(), labelsCount, maxLength);    //Two labels: positive or negative
        //Because we are dealing with vacancies of different lengths and only one output at the final time step: use padding arrays
        //Mask arrays contain 1 if data is present at that time step for that example, or 0 if data is just padding
        INDArray featuresMask = Nd4j.zeros(vacancies.size(), maxLength);
        INDArray labelsMask = Nd4j.zeros(vacancies.size(), maxLength);

        int[] temp = new int[2];
        for( int i=0; i&lt;vacancies.size(); i++ ){
            List&lt;String&gt; tokens = allTokens.get(i);
            temp[0] = i;
            //Get word vectors for each word in review, and put them in the training data
            for( int j=0; j&lt;tokens.size() &amp;&amp; j&lt;maxLength; j++ ){
                String token = tokens.get(j);
                INDArray vector = wordVectors.getWordVectorMatrix(token);
                features.put(new INDArrayIndex[]{NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.point(j)}, vector);

                temp[1] = j;
                featuresMask.putScalar(temp, 1.0);  //Word is present (not padding) for this example + time step -&gt; 1.0 in features mask
            }

            int idx = (positive[i] ? 0 : 1);
            int lastIdx = Math.min(tokens.size(),maxLength);
            labels.putScalar(new int[]{i,idx,lastIdx-1},1.0);   //Set label: [0,1] for negative, [1,0] for positive
            labelsMask.putScalar(new int[]{i,lastIdx-1},1.0);   //Specify that an output exists at the final time step for this example
        }

        return new DataSet(features,labels,featuresMask,labelsMask);
    }

    @Override
    public int totalExamples() {
        return filePathes.length;
    }

    @Override
    public int inputColumns() {
        return vectorSize;
    }

    @Override
    public int totalOutcomes() {
        return 2;
    }

    @Override
    public void reset() {
        cursor = 0;
    }

    @Override
    public int batch() {
        return batchSize;
    }

    @Override
    public int cursor() {
        return cursor;
    }

    @Override
    public int numExamples() {
        return totalExamples();
    }

    @Override
    public void setPreProcessor(DataSetPreProcessor preProcessor) {
        throw new UnsupportedOperationException();
    }

    @Override
    public List&lt;String&gt; getLabels() {
        return Arrays.asList(""positive"",""negative"");
    }

    @Override
    public boolean hasNext() {
        return cursor &lt; numExamples();
    }

    @Override
    public DataSet next() {
        return next(batchSize);
    }

    @Override
    public void remove() {

    }
}
</code></pre>
","<java><machine-learning><deep-learning><deeplearning4j>","2016-03-25 05:06:55","","2","4205978","2016-03-25 05:24:16","0","4","","","4205978","84","8","0"
"36174493","<p>I am trying to add sentiment analysis program written in <code>deeplearning4j</code> to Spark pipeline. Then I have to override the method <code>org.apache.spark.ml.PredictionModel. predict()</code> to predict using the <code>RNN</code> model I created. As I understood, the arguments to this method is a feature row and the label for the row is predicted. But, in sentiment analysis program, the features should be a 2D array because each row contains a list of list of vectors as <code>[[0.0011181544391649161,0.0025584171060651644,0.01754946247376411,-0.006530340570481004,0.003487414946750136,0.004426218948032432,0.00404,0.002611281607120172,0.006444432718879956,-0.012260229877306768,0.002399729592556043]]</code></p>

<p>But I get a 1D array as<code>[0.003356837383,0.0074654373,...]</code> as the input of predict method. Can you explain why this happens and can you explain the what are the arguments to the predict method?</p>
","<java><apache-spark><deep-learning><prediction><deeplearning4j>","2016-03-23 09:39:07","","0","5950143","2016-07-25 12:07:42","1","2","","","5950143","131","40","0"
"50703420","<p>In some papy i I saw this sentenceno “Recent CNN based object detectors，matter one-stage methods like YOLO [1,2，3], and RetinaNet or two-stage detectors like Faster R-CNN , R-FCN  and FPN  are usually trying to directly finetune from ImageNet pre-trained models designed for image classification.”
What i question is that can i remove the finefune step and just train the network from scratch？In fact i try it on yolo3 model.I trained the yolo3 model from scratch with only one image and let it overfitting.But the result was poor.So if i want to design a object detectors,do i realy need to train the backbone on the ImageNet dataset？Thank you!</p>
","<object-detection><deeplearning4j>","2018-06-05 15:10:07","","0","9648551","2018-06-05 15:10:07","0","0","1","","9648551","1","0","0"
"38184920","<p>I want to implement a Recursive neural tensor network(<strong>RNTN</strong>) in java.</p>

<p>I've used Deeplearning4j for word2vec pipeline to vectorize a corpus of words.</p>

<p>for NLP pipeline I've used Opennlp.( for tokenizing, POStaging and parsing)</p>

<p>Now, I figured out that I need an RNTN for my purpose and I didn't find much support, any references would be helpful. Many libraries are written in R or python or even in Scala and the NLP pipeline most of the people used is stanfordnlp. But I want to do this with Opennlp and java.</p>

<p>After that, I would like to combine the word vectors with neural net and then do the task I want to do something like sentiment analysis.</p>

<p>How can I proceed? Any input will be helpful.</p>

<p>Thanks.</p>
","<java><neural-network><opennlp><word2vec><deeplearning4j>","2016-07-04 12:35:29","","11","6495588","2017-06-11 21:33:01","1","2","2","","6495588","796","292","77"
"38784437","<p>I'm new to Spark and I'm currently trying to build a neural network using the deeplearning4j api. The training works just fine, but I'm encountering problems at evaluation. I get the following error message</p>

<pre><code>18:16:16,206 ERROR ~ Exception in task 0.0 in stage 14.0 (TID 19)
java.lang.IllegalStateException: Network did not have same number of parameters as the broadcasted set parameters at org.deeplearning4j.spark.impl.multilayer.evaluation.EvaluateFlatMapFunction.call(EvaluateFlatMapFunction.java:75)
</code></pre>

<p>I can't seem to find the reason for this problem, and information on spark and deeplearning4j is sparse. I essentially took this structure from this example <a href=""https://github.com/deeplearning4j/dl4j-spark-cdh5-examples/blob/2de0324076fb422e2bdb926a095adb97c6d0e0ca/src/main/java/org/deeplearning4j/examples/mlp/IrisLocal.java"" rel=""nofollow"">https://github.com/deeplearning4j/dl4j-spark-cdh5-examples/blob/2de0324076fb422e2bdb926a095adb97c6d0e0ca/src/main/java/org/deeplearning4j/examples/mlp/IrisLocal.java</a>.</p>

<p>This is my code</p>

<pre><code>public class DeepBeliefNetwork {

private JavaRDD&lt;DataSet&gt; trainSet;
private JavaRDD&lt;DataSet&gt; testSet;

private int inputSize;
private int numLab;
private int batchSize;
private int iterations;
private int seed;
private int listenerFreq;
MultiLayerConfiguration conf;
MultiLayerNetwork model;
SparkDl4jMultiLayer sparkmodel;
JavaSparkContext sc;

MLLibUtil mllibUtil = new MLLibUtil();

public DeepBeliefNetwork(JavaSparkContext sc, JavaRDD&lt;DataSet&gt; trainSet, JavaRDD&lt;DataSet&gt; testSet, int numLab,
        int batchSize, int iterations, int seed, int listenerFreq) {

    this.trainSet = trainSet;
    this.testSet = testSet;
    this.numLab = numLab;
    this.batchSize = batchSize;
    this.iterations = iterations;
    this.seed = seed;
    this.listenerFreq = listenerFreq;
    this.inputSize = testSet.first().numInputs();
    this.sc = sc;



}

public void build() {
    System.out.println(""input Size: "" + inputSize);
    System.out.println(trainSet.first().toString());
    System.out.println(testSet.first().toString());

    conf = new NeuralNetConfiguration.Builder().seed(seed)
            .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)
            .gradientNormalizationThreshold(1.0).iterations(iterations).momentum(0.5)
            .momentumAfter(Collections.singletonMap(3, 0.9))
            .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).list(4)
            .layer(0,
                    new RBM.Builder().nIn(inputSize).nOut(500).weightInit(WeightInit.XAVIER)
                            .lossFunction(LossFunction.RMSE_XENT).visibleUnit(RBM.VisibleUnit.BINARY)
                            .hiddenUnit(RBM.HiddenUnit.BINARY).build())
            .layer(1,
                    new RBM.Builder().nIn(500).nOut(250).weightInit(WeightInit.XAVIER)
                            .lossFunction(LossFunction.RMSE_XENT).visibleUnit(RBM.VisibleUnit.BINARY)
                            .hiddenUnit(RBM.HiddenUnit.BINARY).build())
            .layer(2,
                    new RBM.Builder().nIn(250).nOut(200).weightInit(WeightInit.XAVIER)
                            .lossFunction(LossFunction.RMSE_XENT).visibleUnit(RBM.VisibleUnit.BINARY)
                            .hiddenUnit(RBM.HiddenUnit.BINARY).build())
            .layer(3, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD).activation(""softmax"").nIn(200)
                    .nOut(numLab).build())
            .pretrain(true).backprop(false).build();

}

public void trainModel() {

    model = new MultiLayerNetwork(conf);
    model.init();
    model.setListeners(Collections.singletonList((IterationListener) new ScoreIterationListener(listenerFreq)));

    // Create Spark multi layer network from configuration

    sparkmodel = new SparkDl4jMultiLayer(sc.sc(), model);
    sparkmodel.fitDataSet(trainSet);

//Evaluation
    Evaluation evaluation = sparkmodel.evaluate(testSet);
    System.out.println(evaluation.stats());
</code></pre>

<p>Does anyone have advice about how to handle my JavaRDD? I believe that the problem lies in there.</p>

<p>Thanks a lot!</p>

<p><strong>EDIT1</strong></p>

<p>I'm using deeplearning4j version <em>0.4-rc.10</em>, and spark <em>1.5.0</em> Here's the stack trace</p>

<pre><code>11:03:53,088 ERROR ~ Exception in task 0.0 in stage 16.0 (TID 21 java.lang.IllegalStateException: Network did not have same number of parameters as the broadcasted set parameter
at org.deeplearning4j.spark.impl.multilayer.evaluation.EvaluateFlatMapFunction.call(EvaluateFlatMapFunction.java:75)
at org.deeplearning4j.spark.impl.multilayer.evaluation.EvaluateFlatMapFunction.call(EvaluateFlatMapFunction.java:41)
at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156)
at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
at org.apache.spark.scheduler.Task.run(Task.scala:88)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1933)
at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
at org.apache.spark.rdd.RDD.reduce(RDD.scala:985)
at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:375)
at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:47)
at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.evaluate(SparkDl4jMultiLayer.java:629)
at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.evaluate(SparkDl4jMultiLayer.java:607)
at org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.evaluate(SparkDl4jMultiLayer.java:597)
at deep.deepbeliefclassifier.DeepBeliefNetwork.trainModel(DeepBeliefNetwork.java:117)
at deep.deepbeliefclassifier.DataInput.main(DataInput.java:105)
</code></pre>
","<java><apache-spark><deeplearning4j>","2016-08-05 08:16:35","","0","6681455","2016-08-05 09:56:37","1","3","","","6681455","1","0","0"
"37385239","<p>I have been learning about Word2Vec(Deeplearning4j) but i could find not anything about it supporting Chinese. From various sources I got to know that it can work for chinese also by using some plugin.</p>

<p>So please tell me any plugin for chinese, also how it should be implemented with word2vec.</p>

<p>And if Deeplearning4j Word2Vec is good or not for english and chinese language(both) support. If not please suggest some better choice with it's link.</p>

<p>Language : Java</p>
","<java><word2vec><chinese-locale><deeplearning4j>","2016-05-23 07:40:14","","1","5088066","2016-11-28 09:32:51","3","4","","","5088066","19","0","0"
"48262500","<p>I'm trying to write a simple <code>Classifier</code> example in Scala and looking at the <code>dl4j</code> examples a <code>NeuralNetConfiguration</code> instance is created (using a builder) and then passed as an argument to a <code>MultiLayerNetwork</code> class constructor. However, looking at the source code, <code>MultiLayerNetwork</code> does not have any constructor taking a <code>NeuralNetConfiguration</code> as an example. It looks like these examples are not up to date. Does anyone know how to do this in the last <code>dl4j</code> version?</p>

<p>I've manage to find a work around but I'm not sure this is the right way to do it:</p>

<pre><code>val conf = new NeuralNetConfiguration.Builder()
   .seed(seed) 
   .iterations(iterations) 
   .learningRate(1e-6f) 
   .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT) 
   .l1(1e-1).regularization(true).l2(2e-4)
   .useDropConnect(true)
   .layer(hiddenLayer)
   .layer(outputLayer)
   .build()
val confs: util.List[NeuralNetConfiguration] = new util.LinkedList();
confs.add(conf)
val builder = new MultiLayerConfiguration.Builder()
builder.setConfs(confs)
val model: MultiLayerNetwork = new MultiLayerNetwork(builder.build())
</code></pre>
","<scala><deeplearning4j><dl4j>","2018-01-15 11:54:54","","0","7709729","2018-01-16 17:22:34","2","0","","","7709729","8","0","0"
"46635269","<p>I tried to use the dutchembeddings in Word2Vec format with dl4j. But an exception is thrown when loadStaticModel is called: ""Unable to guess input file format""</p>

<pre><code>WordVectorSerializer.loadStaticModel(new File(WORD_VECTORS_PATH)
</code></pre>

<p><a href=""https://github.com/clips/dutchembeddings"" rel=""nofollow noreferrer"">https://github.com/clips/dutchembeddings</a> (I downloaded the wikipedia 160 tar.gz)</p>

<p>How can I get the dutchembeddings in Word2Vec format working with dl4j?</p>

<p><strong>Stacktrace</strong></p>

<pre><code>Loading word vectors and creating DataSetIterators
o.d.m.e.l.WordVectorSerializer - Trying DL4j format...
o.d.m.e.l.WordVectorSerializer - Trying CSVReader...
o.d.m.e.l.WordVectorSerializer - Trying BinaryReader...
Exception in thread ""main"" java.lang.RuntimeException: Unable to guess input file format
    at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.loadStaticModel(WordVectorSerializer.java:2646)
    at org.deeplearning4j.examples.convolution.sentenceclassification.CnnDutchSentenceClassification.main(CnnDutchSentenceClassification.java:122)

Process finished with exit code 1
</code></pre>
","<word2vec><deeplearning4j><dl4j>","2017-10-08 19:36:04","","0","5089416","2017-10-09 19:13:54","0","2","","","5089416","31","0","0"
"50062222","<p>I want to use DL4J [<a href=""https://deeplearning4j.org/]"" rel=""nofollow noreferrer"">https://deeplearning4j.org/]</a> and tried the instructions on the setup guide [<a href=""https://deeplearning4j.org/gettingstarted]"" rel=""nofollow noreferrer"">https://deeplearning4j.org/gettingstarted]</a>. I am facing problems while building using Maven (build failure). Do I need to really use Maven to build everything. I just want to write some basic programs and run the examples which comes with DL4J. I am unable to find a list of jar files which i can import into my project and compile the examples. Any help is appreciated.</p>
","<deeplearning4j><dl4j>","2018-04-27 12:18:04","","1","1468768","2018-07-12 14:58:10","2","1","","","1468768","76","7","0"
"40265735","<p>I am learning deep learning, and use deeplearning4J tuts.
But when i run example code, some issue there :</p>

<p><img src=""https://i.stack.imgur.com/STDF7.png"" alt=""enter image description here""></p>

<pre><code>Caused by: java.lang.UnsatisfiedLinkError: no jnind4j in java.library.path
    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
    at java.lang.Runtime.loadLibrary0(Runtime.java:870)
    at java.lang.System.loadLibrary(System.java:1122)
    at org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:727)
    at org.bytedeco.javacpp.Loader.load(Loader.java:502)
    at org.nd4j.nativeblas.NativeOps.&lt;clinit&gt;(NativeOps.java:37)
    ... 18 more
</code></pre>

<p>i have tried some solutions on web but it doesnot work.
So, if you know how to fix it, help me.</p>
","<java><intellij-idea><deeplearning4j>","2016-10-26 15:01:06","","1","6172474","2018-06-20 16:30:23","3","0","","","6172474","121","28","0"
"50339032","<p>In many (or most) Deeplearning4j example I have seen, when building a configuration method calls seem to be added to method calls . . .</p>

<p>For example:</p>

<pre><code> MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(rngSeed) //include a random seed for reproducibility
            // use stochastic gradient descent as an optimization algorithm
            .updater(new Nesterovs(0.006, 0.9))
            .l2(1e-4)
            .list()
            .layer(0, new DenseLayer.Builder() //create the first, input layer with xavier initialization
                    .nIn(numRows * numColumns)
                    .nOut(1000)
                    .activation(Activation.RELU)
                    .weightInit(WeightInit.XAVIER)
                    .build())
            .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) //create hidden layer
                    .nIn(1000)
                    .nOut(outputNum)
                    .activation(Activation.SOFTMAX)
                    .weightInit(WeightInit.XAVIER)
                    .build())
            .pretrain(false).backprop(true) //use backpropagation to adjust weights
</code></pre>

<p>.build();</p>

<p>Occassionally I see examples where thh conf variable is created in one statement, and then each of the other operations are created in separate statements.  <strong>Is there any benefit in doing it the first way?</strong>  It does tend to obfuscate when a particular method call returns a different type of object.  Also, it would seem that the second approach would be more amenable to working with Jshell.</p>
","<deeplearning4j>","2018-05-14 21:08:21","","0","3113481","2018-05-15 00:27:49","1","1","","","3113481","1","0","0"
"48699251","<p>The CSV with text columns (sentence features in column) to convert it to svmlight or libsvm format (numerical format) by the vectorization like bag of words, etc? </p>
","<svm><libsvm><word2vec><deeplearning4j><svmlight>","2018-02-09 05:27:59","","0","744240","2018-05-25 17:55:51","1","1","","","744240","67","2","0"
"55882266","<p>I m trying to build model for classification of 5 human activities based on data from triaxial accelerometer with LSTM neural network. I configured my model based on deeplearnin4j examples, but I think training is not working properly because the Score after iterations is not decreasing after any number of epochs. </p>

<p>You can see logs from training in Console in following block:</p>

<pre><code>Scanning for projects...

-------------------------&lt; com.mycompany:LSTM &gt;-------------------------
Building LSTM 1.0-SNAPSHOT
--------------------------------[ jar ]---------------------------------

--- exec-maven-plugin:1.2.1:exec (default-cli) @ LSTM ---
13:10:01.334 [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [JCublasBackend] backend
13:10:04.058 [main] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 32
13:10:05.568 [main] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 0
13:10:05.572 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CUDA]; OS: [Windows 10]
13:10:05.572 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [12]; Memory: [3,5GB];
13:10:05.572 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [CUBLAS]
13:10:05.572 [main] INFO org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner - Device Name: [GeForce GTX 1050 Ti]; CC: [6.1]; Total/free memory: [4294967296]
13:10:05.711 [main] DEBUG org.nd4j.jita.handler.impl.CudaZeroHandler - Creating bucketID: 0
13:10:05.720 [main] DEBUG org.nd4j.jita.handler.impl.CudaZeroHandler - Creating bucketID: 2
13:10:05.725 [main] DEBUG org.nd4j.jita.handler.impl.CudaZeroHandler - Creating bucketID: 1
13:10:05.725 [main] DEBUG org.nd4j.jita.handler.impl.CudaZeroHandler - Creating bucketID: 5
13:10:06.160 [main] DEBUG org.nd4j.jita.handler.impl.CudaZeroHandler - Creating bucketID: 4
13:10:06.163 [main] DEBUG org.nd4j.jita.handler.impl.CudaZeroHandler - Creating bucketID: 3
13:10:27.140 [main] INFO org.deeplearning4j.nn.multilayer.MultiLayerNetwork - Starting MultiLayerNetwork with WorkspaceModes set to [training: ENABLED; inference: ENABLED], cacheMode set to [NONE]
13:10:28.832 [main] DEBUG org.deeplearning4j.nn.layers.recurrent.LSTM - CudnnLSTMHelper successfully initialized
13:10:28.855 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:10:32.604 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 0 is 1.5328358650207519
13:10:49.904 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 20 is 1.4521272659301758
13:11:07.154 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 40 is 1.0928638458251954
13:11:22.941 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 60 is 1.0838352203369142
13:11:36.821 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 80 is 1.5488048553466798
13:11:48.534 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 100 is 0.9116960525512695
13:12:08.894 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 120 is 1.2259522438049317
13:12:15.793 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:12:15.926 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:12:37.306 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:12:37.327 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

========================Evaluation Metrics========================
 # of classes:    5
 Accuracy:        0,4583
 Precision:       0,5973    (2 classes excluded from average)
 Recall:          0,3800
 F1 Score:        0,6127    (2 classes excluded from average)
Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)

Warning: 2 classes were never predicted by the model and were excluded from average precision
Classes excluded from average precision: [0, 1]

=========================Confusion Matrix=========================
  0  1  2  3  4
----------------
  0  0 15  4  0 | 0 = 0
  0  0 13  6  0 | 1 = 1
  0  0 50 53  0 | 2 = 2
  0  0 49 53  1 | 3 = 3
  0  0  2  0 18 | 4 = 4

Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
13:12:37.327 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:12:53.130 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 140 is 2.0276023864746096
13:13:10.387 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 160 is 1.5625686645507812
13:13:25.929 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 180 is 1.5873197555541991
13:13:40.327 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 200 is 0.9586276054382324
13:13:55.432 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 220 is 0.7576540946960449
13:14:11.708 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 240 is 0.875974464416504
13:14:26.234 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:14:26.241 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:14:47.523 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:14:47.531 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

========================Evaluation Metrics========================
 # of classes:    5
 Accuracy:        0,4280
 Precision:       0,5372    (2 classes excluded from average)
 Recall:          0,3806
 F1 Score:        0,5673    (2 classes excluded from average)
Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)

Warning: 2 classes were never predicted by the model and were excluded from average precision
Classes excluded from average precision: [0, 1]

=========================Confusion Matrix=========================
  0  1  2  3  4
----------------
  0  0 14  5  0 | 0 = 0
  0  0  6 13  0 | 1 = 1
  0  0 18 85  0 | 2 = 2
  0  0 26 75  2 | 3 = 3
  0  0  0  0 20 | 4 = 4

Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
13:14:47.531 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:14:58.324 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 260 is 0.8601213455200195
13:15:16.516 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 280 is 0.9153075218200684
13:15:32.987 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 300 is 1.5319385528564453
13:15:47.603 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 320 is 0.8646750450134277
13:16:02.376 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 340 is 1.4645065307617187
13:16:17.640 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 360 is 0.8540505409240723
13:16:36.874 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 380 is 1.950326681137085
13:16:36.874 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:16:36.882 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:16:57.005 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:16:57.012 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

========================Evaluation Metrics========================
 # of classes:    5
 Accuracy:        0,4205
 Precision:       0,4696    (2 classes excluded from average)
 Recall:          0,3767
 F1 Score:        0,5170    (2 classes excluded from average)
Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)

Warning: 2 classes were never predicted by the model and were excluded from average precision
Classes excluded from average precision: [0, 1]

=========================Confusion Matrix=========================
  0  1  2  3  4
----------------
  0  0  9  7  3 | 0 = 0
  0  0  6 12  1 | 1 = 1
  0  0 12 91  0 | 2 = 2
  0  0 21 79  3 | 3 = 3
  0  0  0  0 20 | 4 = 4

Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
13:16:57.012 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:17:18.203 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 400 is 1.320208740234375
13:17:35.460 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 420 is 1.6308589935302735
13:17:52.665 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 440 is 1.1166643142700194
13:18:04.471 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 460 is 0.9357609748840332
13:18:19.177 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 480 is 1.330645751953125
13:18:38.274 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 500 is 0.7987275123596191
13:18:47.534 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:18:47.541 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:19:06.557 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:19:06.563 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

========================Evaluation Metrics========================
 # of classes:    5
 Accuracy:        0,4356
 Precision:       0,4292    (1 class excluded from average)
 Recall:          0,3931
 F1 Score:        0,4009    (1 class excluded from average)
Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)

Warning: 1 class was never predicted by the model and was excluded from average precision
Classes excluded from average precision: [1]

=========================Confusion Matrix=========================
  0  1  2  3  4
----------------
  1  0  6  7  5 | 0 = 0
  1  0  5 12  1 | 1 = 1
  0  0  9 94  0 | 2 = 2
  1  0 14 85  3 | 3 = 3
  0  0  0  0 20 | 4 = 4

Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
13:19:06.564 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:19:21.384 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 520 is 1.4552995681762695
13:19:38.909 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 540 is 0.7883223533630371
13:19:54.437 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 560 is 0.7487533569335938
13:20:09.842 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 580 is 0.8777185440063476
13:20:24.370 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 600 is 1.3696569442749023
13:20:40.463 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 620 is 0.7666335105895996
13:20:55.188 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:20:55.195 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:21:13.798 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:21:13.805 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

========================Evaluation Metrics========================
 # of classes:    5
 Accuracy:        0,4545
 Precision:       0,5491    (1 class excluded from average)
 Recall:          0,4285
 F1 Score:        0,4959    (1 class excluded from average)
Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)

Warning: 1 class was never predicted by the model and was excluded from average precision
Classes excluded from average precision: [1]

=========================Confusion Matrix=========================
  0  1  2  3  4
----------------
  4  0  8  7  0 | 0 = 0
  1  0  5 12  1 | 1 = 1
  0  0 11 92  0 | 2 = 2
  2  0 15 85  1 | 3 = 3
  0  0  0  0 20 | 4 = 4

Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
13:21:13.806 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:21:23.939 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 640 is 1.6179420471191406
13:21:41.866 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 660 is 1.214307975769043
13:21:56.973 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 680 is 0.922851276397705
13:22:12.932 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 700 is 0.8825350761413574
13:22:26.289 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 720 is 0.8634878158569336
13:22:42.122 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 740 is 1.2621063232421874
13:23:00.384 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 760 is 0.772999382019043
13:23:01.654 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:23:01.661 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:23:20.198 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:23:20.205 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

========================Evaluation Metrics========================
 # of classes:    5
 Accuracy:        0,4924
 Precision:       0,5578    (1 class excluded from average)
 Recall:          0,4651
 F1 Score:        0,5434    (1 class excluded from average)
Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)

Warning: 1 class was never predicted by the model and was excluded from average precision
Classes excluded from average precision: [1]

=========================Confusion Matrix=========================
  0  1  2  3  4
----------------
  6  0  3  8  2 | 0 = 0
  0  0  7 11  1 | 1 = 1
  0  0 31 72  0 | 2 = 2
  5  0 21 73  4 | 3 = 3
  0  0  0  0 20 | 4 = 4

Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
13:23:20.206 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:23:38.008 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 780 is 1.0697845458984374
13:23:55.367 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 800 is 0.7442119121551514
13:24:14.028 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 820 is 1.108755111694336
13:24:25.903 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 840 is 0.6914281845092773
13:24:39.838 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 860 is 0.6886839866638184
13:24:57.245 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 880 is 0.883371639251709
13:25:08.683 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:25:08.690 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:25:27.483 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:25:27.490 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

========================Evaluation Metrics========================
 # of classes:    5
 Accuracy:        0,4318
 Precision:       0,5647    (1 class excluded from average)
 Recall:          0,4169
 F1 Score:        0,5048    (1 class excluded from average)
Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)

Warning: 1 class was never predicted by the model and was excluded from average precision
Classes excluded from average precision: [1]

=========================Confusion Matrix=========================
  0  1  2  3  4
----------------
  4  0 13  2  0 | 0 = 0
  0  0 10  9  0 | 1 = 1
  0  0 23 80  0 | 2 = 2
  2  0 31 67  3 | 3 = 3
  0  0  0  0 20 | 4 = 4

Confusion matrix format: Actual (rowClass) predicted as (columnClass) N times
==================================================================
13:25:27.491 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:25:42.416 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 900 is 1.1810382843017577
13:25:59.881 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 920 is 0.8167665481567383
13:26:16.202 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 940 is 1.4128643035888673
13:26:31.338 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 960 is 0.7820789813995361
13:26:44.886 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 980 is 0.8911567687988281
13:27:01.673 [main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 1000 is 0.9900901794433594
13:27:16.626 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:27:16.633 [ADSI prefetch thread] DEBUG org.nd4j.linalg.memory.abstracts.Nd4jWorkspace - Steps: 5
13:27:35.392 [main] DEBUG org.deeplearning4j.datasets.iterator.AsyncDataSetIterator - Manually destroying ADSI workspace
13:27:35.399 [main] INFO com.mycompany.lstm.UCISequenceClassificationExample - 

</code></pre>

<p>On input I have 2 files for each sample: </p>

<ul>
<li>in feature file I have acceleration values for 3 axes separated (3 columns) and in each row there are 3 values (x,y,z axis),  number of rows in each sample feature file is different for each sample. For example: </li>
</ul>

<pre><code>-1.4757843000000002 0.9027405000000001  8.998032
-1.4912566999999999 0.98605347  8.887344
-1.5733795  0.95510864  8.881393
-1.6447906000000003 0.93130493  8.804031
-1.6876373  0.9931946   8.838547
-1.7602386000000003 0.9515380999999999  8.852829
-1.7102509  1.0074768   8.79332
-1.7804718000000002 0.9920044   8.881393
-1.7590485  1.0110474   8.854019000000001
-1.7126312  1.0503235   8.889725
-1.7221526999999999 1.1217346000000001  8.948044
-1.6543120999999998 1.1276855   8.927811
-1.6590729  1.0872191999999998  9.105148
-1.6626433999999997 1.1622008999999998  9.218216 
</code></pre>

<ul>
<li>in label file I have only one int value in first row specifying the expected output activity</li>
</ul>

<p>Below I will put my entire code of LSTM.</p>

<pre><code>       SequenceRecordReader trainFeatures = new CSVSequenceRecordReader(0, csvSplitBy);

        SequenceRecordReader trainLabels = new CSVSequenceRecordReader();

        try {

            trainFeatures.initialize(new NumberedFileInputSplit(featuresDirTrain.getAbsolutePath() + ""/%d.csv"", 0, trainCount-1));

            trainLabels.initialize(new NumberedFileInputSplit(labelsDirTrain.getAbsolutePath() + ""/%d.csv"", 0, trainCount-1));

        } catch (IOException ex) {

            java.util.logging.Logger.getLogger(ExternalDatasetVersion.class.getName()).log(Level.SEVERE, null, ex);

        } catch (InterruptedException ex) {

            java.util.logging.Logger.getLogger(ExternalDatasetVersion.class.getName()).log(Level.SEVERE, null, ex);

        }



        int miniBatchSize = 10;

        int numLabelClasses = 5;    

        DataSetIterator trainData = new SequenceRecordReaderDataSetIterator(trainFeatures, trainLabels, miniBatchSize, numLabelClasses,

            false, SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END);



        //Normalize the training data

        DataNormalization normalizer = new NormalizerStandardize();

        normalizer.fit(trainData);              //Collect training data statistics

        trainData.reset();



        //Use previously collected statistics to normalize on-the-fly. Each DataSet returned by 'trainData' iterator will be normalized

        trainData.setPreProcessor(normalizer);





        // ----- Load the test data -----

        //Same process as for the training data.

        SequenceRecordReader testFeatures = new CSVSequenceRecordReader(0, csvSplitBy);

        SequenceRecordReader testLabels = new CSVSequenceRecordReader();

        try {

            testFeatures.initialize(new NumberedFileInputSplit(featuresDirTest.getAbsolutePath() + ""/%d.csv"", 0, testCount-1));        

            testLabels.initialize(new NumberedFileInputSplit(labelsDirTest.getAbsolutePath() + ""/%d.csv"", 0, testCount-1));

        } catch (IOException ex) {

            java.util.logging.Logger.getLogger(ExternalDatasetVersion.class.getName()).log(Level.SEVERE, null, ex);

        } catch (InterruptedException ex) {

            java.util.logging.Logger.getLogger(ExternalDatasetVersion.class.getName()).log(Level.SEVERE, null, ex);

        } 

        DataSetIterator testData = new SequenceRecordReaderDataSetIterator(testFeatures, testLabels, miniBatchSize, numLabelClasses,

            false, SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END);



        testData.setPreProcessor(normalizer);   


        // ----- Configure the network -----

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()

                .seed(123)    //Random number generator seed for improved repeatability. Optional.

                .weightInit(WeightInit.XAVIER)

                .updater(new Nesterovs(0.005))

                .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)  //Not always required, but helps with this data set

                .gradientNormalizationThreshold(0.5)

                .list()

                .layer(0, new LSTM.Builder().activation(Activation.TANH).nIn(3).nOut(10).build())

                .layer(1, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT)

                        .activation(Activation.SOFTMAX).nIn(10).nOut(numLabelClasses).build())

                .build();



        MultiLayerNetwork net = new MultiLayerNetwork(conf);

        net.init();



        net.setListeners(new ScoreIterationListener(20));   //Print the score (loss function value) every 20 iterations





        // ----- Train the network, evaluating the test set performance at each epoch -----

        int nEpochs = 200;

        String str = ""Test set evaluation at epoch %d: Accuracy = %.2f, F1 = %.2f"";

        for (int i = 0; i &lt; nEpochs; i++) {

            net.fit(trainData);



            //Evaluate on the test set:

            //Evaluation evaluation = net.evaluate(testData);            

            //log.info(String.format(str, i, evaluation.accuracy(), evaluation.f1()));



            testData.reset();

            trainData.reset();

        }

        Evaluation evaluation = net.evaluate(testData); 

        log.info(evaluation.stats());



        log.info(""----- Example Complete -----"");
</code></pre>

<p>Can anyone help me with this please ?</p>
","<lstm><deeplearning4j><multiclass-classification>","2019-04-27 15:51:04","","0","11331002","2019-04-27 16:24:46","0","0","","","11331002","1","0","0"
"55855210","<p>I'm working in an environment consisting of: Windows 10, Eclipse,
cuda.version =10.0, and bytedecoPresets.version = 10.0-7.3-1.4.3</p>

<p>Using a LocalFileModelSaver to save a MultiLayerNetwork is very slow when the network is trained using a ParallelWrapper and multiple GPUs.  When using an EarlyStoppingTrainer with a single GPU the amount of time to save the model is as expected.  Do I have something configured incorrectly or is this a bug in DL4J?</p>

<p><b>I used the debugger to step through the DL4J code when it saved the model.  In the case that multiple GPUs and the parallel wrapper were used the following stack trace is generated:</b></p>

<p>Thread [main] (Suspended (breakpoint at line 69 in cudaEvent_t))<br>
    cudaEvent_t.synchronize() line: 69<br>
    GridFlowController(SynchronousFlowController).waitTillFinished(AllocationPoint) line: 134<br>
    GridFlowController.waitTillFinished(AllocationPoint) line: 63<br>
    GridFlowController.synchronizeToHost(AllocationPoint) line: 47<br>
    CudaZeroHandler.synchronizeThreadDevice(Long, Integer, AllocationPoint) line: 1304<br>
    AtomicAllocator.synchronizeHostData(DataBuffer) line: 370<br>
    CudaFloatDataBuffer(BaseCudaDataBuffer).getFloat(long) line: 1131<br>
    CudaFloatDataBuffer(BaseDataBuffer).write(DataOutputStream) line: 1562<br>
    CudaFloatDataBuffer(BaseCudaDataBuffer).write(DataOutputStream) line: 801<br>
    Nd4j.write(INDArray, DataOutputStream) line: 2464<br>
    ModelSerializer.writeModel(Model, OutputStream, boolean, DataNormalization) line: 156<br>
    ModelSerializer.writeModel(Model, OutputStream, boolean) line: 119<br>
    LenetNetworkTrainer.trainNetwork(boolean) line: 251 
    ImageClassificationTrainer.run() line: 88<br>
    ImageClassificationTrainer.main(String[]) line: 217</p>

<p><b>It appears that cudaEvent_t.synchronize() is called every time a float value is retrieved when writing the CudaFloatDataBuffer (I believe this to be the source of the problem).</b></p>

<p><b>The equivalent stack trace when using a single GPU and an EarlyStoppingTrainer is:</b></p>

<p>Thread [main] (Suspended)<br>
GridFlowController(SynchronousFlowController).synchronizeToHost(AllocationPoint) line: 97<br>
    GridFlowController.synchronizeToHost(AllocationPoint) line: 50<br>
    CudaZeroHandler.synchronizeThreadDevice(Long, Integer, AllocationPoint) line: 1304<br>
    AtomicAllocator.synchronizeHostData(DataBuffer) line: 370<br>
    CudaFloatDataBuffer(BaseCudaDataBuffer).getFloat(long) line: 1131<br>
    CudaFloatDataBuffer(BaseDataBuffer).write(DataOutputStream) line: 1562<br>
    CudaFloatDataBuffer(BaseCudaDataBuffer).write(DataOutputStream) line: 801<br>
    Nd4j.write(INDArray, DataOutputStream) line: 2464<br>
    ModelSerializer.writeModel(Model, OutputStream, boolean, DataNormalization) line: 156<br>
    ModelSerializer.writeModel(Model, OutputStream, boolean) line: 119<br>
    ModelSerializer.writeModel(Model, String, boolean) line: 106<br>
    LocalFileModelSaver.save(MultiLayerNetwork, String) line: 99<br>
    LocalFileModelSaver.saveBestModel(MultiLayerNetwork, double) line: 77<br>
    LocalFileModelSaver.saveBestModel(Model, double) line: 42<br>
    EarlyStoppingTrainer(BaseEarlyStoppingTrainer).fit() line: 223<br>
    LenetNetworkTrainer.trainNetwork(boolean) line: 228 
    ImageClassificationTrainer.run() line: 88<br>
    ImageClassificationTrainer.main(String[]) line: 217</p>

<p><b>cudaEvent_t.synchronize() is not being called for every float being written.</b></p>

<pre><code>        network = new MultiLayerNetwork(getConfiguration(
            trainingIterator.getLabels().size(), this.builder.getRandomSeed(),
            this.builder.getImageWidth(), this.builder.getImageHeight(),
            this.builder.getImageChannels(), this.builder.getEpochs(),
            iterators.getLeft().getRight(), this.builder.getBatchSize(),
            this.builder.getLearningRateInitialValue(),
            this.builder.getLearningRateDecayExponent()));
        network.init();

        ParallelWrapper wrapper = new ParallelWrapper.Builder&lt;&gt;(network)
            .prefetchBuffer(8).workers(2).averagingFrequency(3)
            .reportScoreAfterAveraging(true).build();
        LocalFileModelSaver localFileModelSaver = new LocalFileModelSaver(
            this.builder.getTrainingPath().getAbsolutePath());
       DataSetLossCalculator dataSetLossCalculator = new dataSetLossCalculator(testingIterator, true);

        for (int i = 0; i &lt; this.builder.getEpochs(); i++) {
          wrapper.fit(trainingIterator);
        }

        //
        // This takes about 25 minutes to save the model
        //
        localFileModelSaver.saveLatestModel(network, 0.0);
</code></pre>

<p>I expect the model to be saved to disk in about 15 to 30 seconds.  Am I missing something?</p>
","<deeplearning4j><dl4j>","2019-04-25 18:03:02","","0","11412067","2019-04-25 18:03:02","0","0","","","11412067","1","0","0"
"46168965","<p>I am trying to build an application on spark using Deeplearning4j library. I have a cluster where i am going to run my jar(built using intelliJ) using spark-submit command. Here's my code</p>

<pre><code>package Com.Spark.Examples

import scala.collection.mutable.ListBuffer
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.canova.api.records.reader.RecordReader
import org.canova.api.records.reader.impl.CSVRecordReader
import org.deeplearning4j.nn.api.OptimizationAlgorithm
import org.deeplearning4j.nn.conf.MultiLayerConfiguration
import org.deeplearning4j.nn.conf.NeuralNetConfiguration
import org.deeplearning4j.nn.conf.layers.DenseLayer
import org.deeplearning4j.nn.conf.layers.OutputLayer
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import org.deeplearning4j.nn.weights.WeightInit
import org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer
import org.nd4j.linalg.lossfunctions.LossFunctions

object FeedForwardNetworkWithSpark {
  def main(args:Array[String]): Unit ={
    val recordReader:RecordReader = new CSVRecordReader(0,"","")
    val conf = new SparkConf()
      .setAppName(""FeedForwardNetwork-Iris"")
    val sc = new SparkContext(conf)
    val numInputs:Int = 4
    val outputNum = 3
    val iterations =1
    val multiLayerConfig:MultiLayerConfiguration = new NeuralNetConfiguration.Builder()
      .seed(12345)
      .iterations(iterations)
      .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
      .learningRate(1e-1)
      .l1(0.01).regularization(true).l2(1e-3)
      .list(3)
      .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3).activation(""tanh"").weightInit(WeightInit.XAVIER).build())
      .layer(1, new DenseLayer.Builder().nIn(3).nOut(2).activation(""tanh"").weightInit(WeightInit.XAVIER).build())
      .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER)
        .activation(""softmax"")
        .nIn(2).nOut(outputNum).build())
      .backprop(true).pretrain(false)
      .build
    val network:MultiLayerNetwork = new MultiLayerNetwork(multiLayerConfig)
    network.init
    network.setUpdater(null)
    val sparkNetwork:SparkDl4jMultiLayer = new
        SparkDl4jMultiLayer(sc,network)
    val nEpochs:Int = 6
    val listBuffer = new ListBuffer[Array[Float]]()
    (0 until nEpochs).foreach{i =&gt; val net:MultiLayerNetwork = sparkNetwork.fit(""/user/iris.txt"",4,recordReader)
      listBuffer +=(net.params.data.asFloat().clone())
      }
    println(""Parameters vs. iteration Output: "")
    (0 until listBuffer.size).foreach{i =&gt;
      println(i+""\t""+listBuffer(i).mkString)}
  }
}
</code></pre>

<p>Here is my build.sbt file</p>

<pre><code>name := ""HWApp""

version := ""0.1""

scalaVersion := ""2.12.3""

libraryDependencies += ""org.apache.spark"" % ""spark-core_2.10"" % ""1.6.0"" % ""provided""
libraryDependencies += ""org.apache.spark"" % ""spark-mllib_2.10"" % ""1.6.0"" % ""provided""
libraryDependencies += ""org.deeplearning4j"" % ""deeplearning4j-nlp"" % ""0.4-rc3.8""
libraryDependencies += ""org.deeplearning4j"" % ""dl4j-spark"" % ""0.4-rc3.8""
libraryDependencies += ""org.deeplearning4j"" % ""deeplearning4j-core"" % ""0.4-rc3.8""
libraryDependencies += ""org.nd4j"" % ""nd4j-x86"" % ""0.4-rc3.8"" % ""test""
libraryDependencies += ""org.nd4j"" % ""nd4j-api"" % ""0.4-rc3.8""
libraryDependencies += ""org.nd4j"" % ""nd4j-jcublas-7.0"" % ""0.4-rc3.8""
libraryDependencies += ""org.nd4j"" % ""canova-api"" % ""0.0.0.14""
</code></pre>

<p>when i see my code in intelliJ, it does not show any error but when i execute the application on cluster: i got something like this:</p>

<p><a href=""https://i.stack.imgur.com/AiJmE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AiJmE.png"" alt=""Error""></a></p>

<p>I don't know what it wants from me. Even a little help will be appreciated. Thanks.</p>
","<apache-spark><deeplearning4j><nd4j>","2017-09-12 06:01:03","","0","7187962","2017-09-12 07:05:22","1","0","","","7187962","5","0","0"
"55548396","<p>I need to upgrade my dl4j environment to at least alpha-1.0.0 to get support for CUDA 9 (needed for RTX 2080).</p>

<p>After switching dependencies to alpha-1.0.0 my network doesn´t really learn anymore.</p>

<p>I upgraded my code following the upgrade guide on the dl4j release notes.</p>

<p>Behaviour doesn´t change when using any of the higher versions.
The new code does still produce the desired results when going back to 0.9.1</p>

<p>I do not use pretraining so behaviour change of fit() should not be the problem here?!</p>

<p>Training with 0.9.1:
imgur.com/51645Lr.png</p>

<p>Training with 1.0.0-alpha:
imgur.com/Nkirn7i.png</p>

<pre class=""lang-java prettyprint-override""><code>//Network setup
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .trainingWorkspaceMode(WorkspaceMode.SEPARATE)
                .inferenceWorkspaceMode(WorkspaceMode.SEPARATE)
                .seed(seed)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .weightInit(WeightInit.XAVIER)
                .updater(new RmsProp.Builder().rmsDecay(0.95).learningRate(learningRate).build())
                .l2(1e-4)
                .list()
                .layer(0, new LSTM.Builder()
                        .name(""LSTM 1"")
                        .nIn(nIn)
                        .nOut(lstmLayer1Size)
                        .activation(Activation.TANH)
                        .gateActivationFunction(Activation.HARDSIGMOID)
                        .dropOut(dropoutRatio)
                        .build())
                .layer(1, new LSTM.Builder()
                        .name(""LSTM 2"")
                        .nIn(lstmLayer1Size)
                        .nOut(lstmLayer2Size)
                        .activation(Activation.TANH)
                        .gateActivationFunction(Activation.HARDSIGMOID)
                        .dropOut(dropoutRatio)
                        .build())
                .layer(2, new DenseLayer.Builder()
                        .name(""Dense"")
                        .nIn(lstmLayer2Size)
                        .nOut(denseLayerSize)
                        .activation(Activation.RELU)
                        .build())
                .layer(3, new RnnOutputLayer.Builder()
                        .nIn(denseLayerSize)
                        .nOut(nOut)
                        .activation(Activation.IDENTITY)
                        .lossFunction(LossFunctions.LossFunction.MSE)
                        .build())
                .backpropType(BackpropType.TruncatedBPTT)
                .tBPTTForwardLength(truncatedBPTTLength)
                .tBPTTBackwardLength(truncatedBPTTLength)
                .build();

        MultiLayerNetwork net = new MultiLayerNetwork(conf);




//training loop

        log.info(""Training..."");
        for (int i = 0; i &lt; epochs; i++) {
            while (iterator.hasNext()){ net.fit(iterator.next()); 
            }
            iterator.reset(); // reset iterator
            net.rnnClearPreviousState(); // clear previous state
        }


</code></pre>
","<deep-learning><deeplearning4j><dl4j>","2019-04-06 10:59:32","","0","5410450","2019-04-06 10:59:32","0","0","","","5410450","1","0","0"
"41637150","<p>Good afternoon to everybody.</p>

<p>I'm quite new to the Deepleaning4j library and there are a couple of stuff that are still unclear to me.
The concept of ""epoch"" is not new, thus, it is clear that it represents a full-cycle on the training set.
My first doubt is related to the concept of ""iteration"". What is an iteration over the training set? Does it correspond to the analysis of a mini-batch number of training instances or to something else?</p>

<p>In my code, I set "".iterations(1)""; however, when I run my code I see a lot of:</p>

<p>... ScoreIterationListener - Score at iteration XX is yy.yyyyyy""</p>

<p>So, if I set "".iterations(1)"", why do I continue to see values of XX greater than 1?
Are there, maybe, some differences between the idea of ""iteration"" as a network configuration parameter and what ""iteration"" means for the ScoreIterationListener class?</p>

<p>Thanks everybody for any answer or link to useful information.</p>

<p>Best,
Mauro.</p>
","<java><deeplearning4j>","2017-01-13 14:45:01","","4","2389528","2017-04-21 23:39:52","1","0","1","","2389528","26","0","0"
"40312884","<p><strong>Why a Graves-LSTM layer Cell has 11 weigths, and what is their purpose</strong>?</p>

<p>Given below example can generate weigth list:</p>

<pre><code>MultiLayerNetwork model = new MultiLayerNetwork(new NeuralNetConfiguration.Builder()
        .list()
        .layer(0, new GravesLSTM.Builder()
                .nIn(1)
                .nOut(1)
                .activation(""sigmoid"")
                .weightInit(WeightInit.ZERO)
                .build()
                )
        .build());
model.init();

System.out.println(""Weigths: "" + model.paramTable());
</code></pre>

<p><strong>out:</strong></p>

<blockquote>
  <p>Weigths:</p>
  
  <p>{0_W=[0.00, 0.00, 0.00, 0.00], 0_RW=[0.00, 0.00, 0.00, 0.00,
  0.00, 0.00, 0.00], 0_b=[0.00, 1.00, 0.00, 0.00]}</p>
  
  <p><strong>( 11 weigth + 4 bias )</strong></p>
</blockquote>

<p>in contrast here is the output <strong>using DenseLayer</strong> instead of GravesLSTM:</p>

<blockquote>
  <p>Weigths: {0_W=0.00, 0_b=0.00}</p>
  
  <p><strong>( 1 weight + 1 bias, this is clear. )</strong></p>
</blockquote>
","<neural-network><lstm><deeplearning4j>","2016-10-28 20:37:25","","1","1386911","2016-10-28 21:00:37","0","2","","","1386911","2381","565","22"
"47391798","<p>I'm attempting to run the deeplearning4j <a href=""https://deeplearning4j.org/quickstart"" rel=""nofollow noreferrer"">quickstart</a> classification problem.</p>

<p>I'm using Windows 8 64 bit, Oracle JDK 8, IntelliJ 2017.2.5, and Maven 3.3.9.</p>

<p>I got past the Canova issue that plagued me earlier.  Now I have a new problem at runtime.</p>

<p>Here's the pom.xml that I constructed for myself.  I did not take @newOne advice to use the example pom.xml, because I want to make sure I can create a stripped down one from scratch without depending on their modules.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0""
         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;deeplearning4j.example&lt;/groupId&gt;
    &lt;artifactId&gt;deeplearning4j&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

    &lt;properties&gt;
        &lt;maven.test.skip&gt;false&lt;/maven.test.skip&gt;
        &lt;timestamp&gt;${maven.build.timestamp}&lt;/timestamp&gt;
        &lt;maven.build.timestamp.format&gt;MM-dd-yyyy HH:mm&lt;/maven.build.timestamp.format&gt;
        &lt;timestamp&gt;${maven.build.timestamp}&lt;/timestamp&gt;
        &lt;maven.build.timestamp.format&gt;MM-dd-yyyy HH:mm&lt;/maven.build.timestamp.format&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
        &lt;deeplearning4j.version&gt;0.9.1&lt;/deeplearning4j.version&gt;
        &lt;nd4j.version&gt;0.9.1&lt;/nd4j.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-core --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
            &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-nlp --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;
            &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-ui-components --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-ui-components&lt;/artifactId&gt;
            &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-ui-model --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-ui-model&lt;/artifactId&gt;
            &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-nearestneighbors-model --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-nearestneighbors-model&lt;/artifactId&gt;
            &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.nd4j/nd4j-api --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
            &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
            &lt;version&gt;${nd4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;version&gt;1.2.17&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/deeplearning4j-nn --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;deeplearning4j-nn&lt;/artifactId&gt;
            &lt;version&gt;${deeplearning4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/jfree/jfreechart --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;jfree&lt;/groupId&gt;
            &lt;artifactId&gt;jfreechart&lt;/artifactId&gt;
            &lt;version&gt;1.0.13&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.datavec/datavec-api --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.datavec&lt;/groupId&gt;
            &lt;artifactId&gt;datavec-api&lt;/artifactId&gt;
            &lt;version&gt;0.9.1&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.deeplearning4j/dl4j-test-resources --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
            &lt;artifactId&gt;dl4j-test-resources&lt;/artifactId&gt;
            &lt;version&gt;0.4.0&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;junit&lt;/groupId&gt;
            &lt;artifactId&gt;junit&lt;/artifactId&gt;
            &lt;version&gt;4.12&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.mockito&lt;/groupId&gt;
            &lt;artifactId&gt;mockito-all&lt;/artifactId&gt;
            &lt;version&gt;1.10.19&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.0.1&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;attach-sources&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;jar&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                    &lt;execution&gt;
                        &lt;id&gt;attach-javadocs&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;jar&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.5.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-site-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.4&lt;/version&gt;
                &lt;dependencies&gt;
                    &lt;dependency&gt;
                        &lt;groupId&gt;org.apache.maven.wagon&lt;/groupId&gt;
                        &lt;artifactId&gt;wagon-http-lightweight&lt;/artifactId&gt;
                        &lt;version&gt;1.0&lt;/version&gt;
                    &lt;/dependency&gt;
                &lt;/dependencies&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
                &lt;artifactId&gt;buildnumber-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;1.4&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;generate-resources&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;create-metadata&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
                &lt;configuration&gt;
                    &lt;attach&gt;true&lt;/attach&gt;
                    &lt;!--make it available for jar/war classpath resource --&gt;
                    &lt;addOutputDirectoryToResources&gt;true&lt;/addOutputDirectoryToResources&gt;
                    &lt;revisionOnScmFailure&gt;na&lt;/revisionOnScmFailure&gt;
                    &lt;doCheck&gt;false&lt;/doCheck&gt;
                    &lt;doUpdate&gt;false&lt;/doUpdate&gt;
                    &lt;revisionOnScmFailure&gt;true&lt;/revisionOnScmFailure&gt;
                    &lt;format&gt;{0,date,yyyy-MM-dd_HH-mm}_{1}&lt;/format&gt;
                    &lt;items&gt;
                        &lt;item&gt;timestamp&lt;/item&gt;
                        &lt;item&gt;${user.name}&lt;/item&gt;
                    &lt;/items&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;
</code></pre>

<p>IntelliJ has no red in my Maven window.  I can build and package the code successfully without error messages.</p>

<p>When I run the classification problem I get this message at runtime:</p>

<pre><code>SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Exception in thread ""ADSI prefetch thread"" java.lang.RuntimeException: org.nd4j.linalg.exception.ND4JIllegalStateException: Invalid shape: Requested INDArray shape [50, 0] contains dimension size values &lt; 1 (all dimensions must be 1 or more)
    at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:442)
Caused by: org.nd4j.linalg.exception.ND4JIllegalStateException: Invalid shape: Requested INDArray shape [50, 0] contains dimension size values &lt; 1 (all dimensions must be 1 or more)
    at org.nd4j.linalg.factory.Nd4j.checkShapeValues(Nd4j.java:5022)
    at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:5012)
    at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4965)
    at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4093)
    at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertWritables(RecordReaderMultiDataSetIterator.java:377)
    at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertFeaturesOrLabels(RecordReaderMultiDataSetIterator.java:271)
    at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.nextMultiDataSet(RecordReaderMultiDataSetIterator.java:234)
    at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next(RecordReaderMultiDataSetIterator.java:177)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:306)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:393)
    at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:51)
    at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:423)
</code></pre>

<p>What should I adjust in the input to get past this matrix with zero columns?  </p>
","<java><maven><intellij-idea><deeplearning4j><nd4j>","2017-11-20 12:14:52","","0","37213","2017-11-20 16:31:40","1","3","","","37213","273389","7665","6499"
"46995912","<p>I'm trying to use DL4J's K-Means implementation. I set it up as follows:</p>

<pre><code>int CLUSTERS = 5;
int MAX_ITERATIONS = 300;
String DISTANCE_METRIC = ""cosinesimilarity"";
KMeansClustering KMEANS = KMeansClustering.setup(CLUSTERS, MAX_ITERATIONS, DISTANCE_METRIC);
</code></pre>

<p>My data points are vectors of size 300 (doubles), and my test set is comprised of ~ 100 data points each time (give or take). I'm running it on my CPU (4 cores) in a single threaded fashion.</p>

<p>Evaluation takes a very long time (a few seconds per example).</p>

<p>I took a peek inside the algorithm's implementation and it looks like its concurrency level is very high - a lot of threads are being created (one per data point, to be exact) and executed in parallel.
Perhaps this is an overkill?
Is there any way I can control it through configuration? Other ways to speed it up? If not, is there any other fast java-based solution for executing k-means?</p>
","<java><multithreading><k-means><deeplearning4j><nd4j>","2017-10-29 00:58:02","","2","223365","2018-09-23 20:19:55","1","2","","","223365","627","71","2"
"48845162","<p>The base problem is trying to use a custom data model to create a DataSetIterator to be used in a <a href=""https://deeplearning4j.org/index.html"" rel=""nofollow noreferrer"">deeplearning4j</a> network.</p>

<p>The data model I am trying to work with is a java class that holds a bunch of doubles, created from quotes on a specific stock, such as timestamp, open, close, high, low, volume, technical indicator 1, technical indicator 2, etc.
I query an internet source, <a href=""https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&amp;symbol=MSFT&amp;interval=1min&amp;apikey=demo"" rel=""nofollow noreferrer"">example</a>, (also several other indicators from the same site) which provide json strings that I convert into my data model for easier access and to store in an sqlite database.</p>

<p>Now I have a List of these data models that I would like to use to train an LSTM network, each double being a feature. Per the Deeplearning4j documentation and several examples, the way to use training data is to use the ETL processes described <a href=""https://deeplearning4j.org/etl-userguide"" rel=""nofollow noreferrer"">here</a> to create a DataSetIterator which is then used by the network.</p>

<p>I don't see a clean way to convert my data model using any of the provided RecordReaders without first converting them to some other format, such as a CSV or other file. I would like to avoid this because it would use up a lot of resources. It seems like there would be a better way to do this simple case. Is there a better approach that I am just missing?</p>
","<java><deep-learning><lstm><deeplearning4j>","2018-02-17 19:36:48","","1","2187716","2018-03-04 23:56:24","2","0","","","2187716","12","0","0"